block_bbox,block_content,block_embedding,block_id,block_summary,block_type,document_title,image_caption,image_footer,image_path,num_blocks,num_pages,page_height,page_number,page_width,pdf_path,section_title
"[110, 97, 502, 137]",GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing,"[ 0.01164246  0.05502319 -0.00830841 ...  0.01348114  0.02281189
 -0.03375244]",0,,text,,,,,8,19,792.0,1,612.0,src/resources/pdf/GenArtist.pdf,
"[164, 177, 450, 191]",Zhenyu Wang1 ∗ Aoxue Li2 Zhenguo Li2 Xihui Liu3 †,"[ 0.05880737  0.05941772 -0.01980591 ...  0.02204895  0.00650024
 -0.02293396]",1,,text,,,,,8,19,792.0,1,612.0,src/resources/pdf/GenArtist.pdf,
"[136, 200, 477, 235]","1 Tsinghua University 2 Noah’s Ark Lab, Huawei 3 The University of Hong Kong wangzy20@mails.tsinghua.edu.cn, lax@pku.edu.cn, Li.Zhenguo@huawei.com, xihuiliu@eee.hku.hk","[ 0.05212402  0.03149414 -0.02038574 ... -0.01992798  0.02426147
 -0.04446411]",2,,text,,,,,8,19,792.0,1,612.0,src/resources/pdf/GenArtist.pdf,
"[283, 263, 328, 276]",Abstract,"[-0.01850891  0.02378845 -0.01013184 ...  0.0120697  -0.02732849
  0.00043058]",3,,text,,,,,8,19,792.0,1,612.0,src/resources/pdf/GenArtist.pdf,
"[143, 287, 469, 474]","Despite the success achieved by existing image generation and editing methods, cur- rent models still struggle with complex problems including intricate text prompts, and the absence of verification and self-correction mechanisms makes the generated images unreliable. Meanwhile, a single model tends to specialize in particular tasks and possess the corresponding capabilities, making it inadequate for fulfilling all user requirements. We propose GenArtist, a unified image generation and editing system, coordinated by a multimodal large language model (MLLM) agent. We integrate a comprehensive range of existing models into the tool library and utilize the agent for tool selection and execution. For a complex problem, the MLLM agent decomposes it into simpler sub-problems and constructs a tree structure to systematically plan the procedure of generation, editing, and self-correction with step-by-step verification. By automatically generating missing position-related in- puts and incorporating position information, the appropriate tool can be effectively employed to address each sub-problem. Experiments demonstrate that GenArtist can perform various generation and editing tasks, achieving state-of-the-art perfor- mance and surpassing existing models such as SDXL and DALL-E 3, as can be seen in Fig. 1. Project page is https://zhenyuw16.github.io/GenArtist_page/.","[ 0.02171326  0.03771973 -0.04141235 ...  0.02412415  0.0222168
 -0.01118469]",4,,text,,,,,8,19,792.0,1,612.0,src/resources/pdf/GenArtist.pdf,
"[108, 492, 190, 506]",1 Introduction,"[ 0.02896118  0.05752563 -0.01829529 ...  0.02319336  0.03010559
  0.01599121]",5,,text,,,,,8,19,792.0,1,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 516, 505, 615]","With the recent advancements in diffusion models [17, 10], image generation and editing methods have rapidly progressed. Current improvements in image generation and editing can be broadly categorized into two tendencies. The first [40, 41, 37, 6, 1] involves training from scratch using more advanced model architectures [41, 36] and larger-scale datasets, thereby scaling up existing models to achieve a more general generation or editing capability. These methods can usually enhance the overall controllability and quality of image generation. The second is primarily about finetuning or additionally designing pre-trained large-scale image generation models on specific datasets to extend their capability [42, 23, 4] or enhance their performance on certain tasks [25, 18]. These methods are usually task-specific and can demonstrate advantageous results on some particular tasks.","[ 0.01208496 -0.02294922 -0.00861359 ... -0.01673889  0.03469849
 -0.02111816]",6,,text,,,,,8,19,792.0,1,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 621, 505, 686]","Despite this, current image generation or editing methods are still imperfect and confront some urgent challenges on the way to building a human-desired system: 1) The demand for image generation and editing is highly diverse and variable, like various requirements for objects and backgrounds, numerous demands about various operations in text prompts or instructions. Meanwhile, different models often possess different strengths and focus. General models may be weaker than some finetuned models in certain aspects, but they can exhibit better performance in out-of-distribution data.","[-0.01235199 -0.02893066 -0.00741577 ... -0.01361084 -0.01042175
 -0.03433228]",7,,text,,,,,8,19,792.0,1,612.0,src/resources/pdf/GenArtist.pdf,
"[111, 80, 499, 690]",,"[ 0.01502991 -0.01902771 -0.03274536 ... -0.0058403   0.00153351
 -0.02261353]",0,,image,,"generation and editing. For text-to-image generation, it obtains greater accuracy compared to existing models like SDXL and DALL-E 3. For image editing, it also excels in complex editing tasks.",,src/resources/pdf/GenArtist/auto/images/55d3ef2b69406e18e7304d95e6871266f3476e6f45a3ba2b20be0cb9a91ba2e9.jpg,2,19,792.0,2,612.0,src/resources/pdf/GenArtist.pdf,
"[276, 72, 359, 79]",text-to-image generation,"[-0.02871704  0.06134033  0.01218414 ... -0.00306892  0.01530457
 -0.03396606]",1,,text,,,,,2,19,792.0,2,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 72, 505, 193]","Therefore, it is nearly impossible for a well-trained model to meet all human requirements, and the use of only a single model is often sub-optimal. 2) Models still struggle with complex problems, such as lengthy and intricate sentences in text-to-image tasks or complicated instructions with multiple steps in editing tasks. Scaling up or finetuning models can alleviate this issue. However, since texts are highly variable, flexible, and can be easy to combine, there are always complex problems that a trained model cannot effectively handle. 3) Although meticulously designed, models still inevitably encounter some failure cases. Generated images sometimes fail to accurately correspond to the content of user prompts. Existing models lack the ability to autonomously assess the correctness of generated images, not to mention self-correcting them, making generated images unreliable. What we truly desire, therefore, should be a unified image generation and editing system, which can satisfy nearly all human requirements while producing reliable image results.","[ 0.00264931  0.00508118 -0.02078247 ... -0.01444244 -0.01059723
 -0.01902771]",0,,text,,,,,10,19,792.0,3,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 198, 505, 286]","In this paper, we propose a unified image generation and editing system called GenArtist to ad- dress the above challenges. Our fundamental idea is to utilize a multimodal large language model (MLLM) as an AI agent, which acts as an ""artist"" and ""draws"" images according to user instructions. Specifically, in response to user instructions, the agent will analyze the user requirements, decompose complex problems, and conduct planning comprehensively to formulate the specific solutions. Then, it executes image generation or editing operations by invoking external tools to meet the user demands. After images are obtained, it finally performs verification and correction on the generated results to further ensure the accuracy of the generated images. The core mechanisms of the agent are:","[ 0.03543091  0.04122925 -0.03912354 ...  0.0006938   0.03311157
 -0.00437927]",1,,text,,,,,10,19,792.0,3,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 291, 505, 346]","Decomposition of intricate text prompts. The MLLM agent first decomposes the complex problems into several simple sub-problems. For complicated text prompts in generation tasks, it extracts single-object concepts and necessary background elements. For complex instructions in editing tasks, it breaks down intricate operations into several simple single editing actions. The decomposition of complex problems significantly improves the reliability of model execution.","[ 0.0435791   0.01919556  0.00413895 ... -0.02915955 -0.00510025
 -0.01391602]",2,,text,,,,,10,19,792.0,3,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 351, 505, 416]","Planning tree with step-by-step verification. After decomposition, we construct a tree structure to plan the execution of sub-tasks. Each operation is a node in the tree, with subsequent operations as its child nodes, and different tools for the same action are its sibling nodes. Each node is followed by verification to ensure that its operation can be executed correctly. Then, both generation, editing, and self-correction mechanisms can be incorporated. Through this planning tree, the proceeding of the system can be considered as a traversal process and the whole system can be coordinated.","[ 0.0519104   0.01480865  0.00540924 ... -0.0496521   0.03448486
 -0.01046753]",3,,text,,,,,10,19,792.0,3,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 422, 505, 477]","Position-aware tool execution. Most of object-level tools require position-related inputs, like the position of the object to be manipulated. These necessary inputs may not be provided by the user. Existing MLLMs are also position-insensitive, and cannot provide accurate positional guidance. We thus introduce a set of auxiliary tools to automatically complete these position-related inputs, and incorporate position information for the MLLM agent through detection models for tool execution.",[0.04983521 0.01250458 0.0178833  ... 0.019104   0.05667114 0.00525665],4,,text,,,,,10,19,792.0,3,612.0,src/resources/pdf/GenArtist.pdf,
"[108, 482, 326, 493]",Our main contributions can be summarized as follows:,"[ 0.05023193  0.00179958 -0.0506897  ... -0.00543976  0.03372192
 -0.02055359]",5,,text,,,,,10,19,792.0,3,612.0,src/resources/pdf/GenArtist.pdf,
"[108, 498, 506, 597]","• We propose GenArtist, a unified image generation and editing system. The MLLM agent serves as the ""brain"" to coordinate and manage the entire process. To the best of our knowledge, this is the first unified system that encompasses the vast majority of existing generation and editing tasks. • Through viewing the operations as nodes and constructing the planning tree, our MLLM agent can schedule for generation and editing tasks, and automatically verify and self-correct generated images. This significantly enhances the controllability of user instructions over images. • By incorporating position information into the integrated tool library and employing auxiliary tools for providing missing position-related inputs, the agent performs tool selection and invokes the most suitable tool, providing a unified interface for various tasks in generation and editing.","[ 0.03997803  0.01065063 -0.03152466 ...  0.00754929  0.04550171
 -0.02043152]",6,,text,,,,,10,19,792.0,3,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 602, 505, 657]","Extensive experiments demonstrate the effectiveness of our GenArtist. It achieves more than improvement compared to DALL-E 3 [1] on T2I-CompBench [18], a comprehensive benchmark for open-world compositional T2I generation, and also obtains the state-of-the-art performance on the image editing benchmark MagicBrush [61]. As can be seen in the visualized examples in Fig. 1, GenArtist well serves as a unified image generation and editing system.","[ 0.0114212   0.03701782 -0.01112366 ...  0.02784729  0.02879333
 -0.03060913]",7,,text,,,,,10,19,792.0,3,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 673, 197, 687]",2 Related Work,"[ 0.01348114  0.00482178 -0.00327682 ...  0.00814056  0.05316162
  0.00292969]",8,,text,,,,,10,19,792.0,3,612.0,src/resources/pdf/GenArtist.pdf,
"[106, 699, 504, 722]","Image generation and editing. With the development of diffusion models [10, 17], both image generation and editing have achieved remarkable success. Many general text-to-image generation [41,","[-0.00682449  0.04107666 -0.03089905 ...  0.00559998  0.0269165
 -0.01835632]",9,,text,,,,,10,19,792.0,3,612.0,src/resources/pdf/GenArtist.pdf,
"[115, 72, 495, 211]",,"[ 0.02575684 -0.01757812  0.00399017 ... -0.00095797  0.03077698
 -0.0368042 ]",0,,image,,"Figure 2: The overview of our GenArtist. The MLLM agent is responsible for decomposing problems and planning using a tree structure, then invoking tools to address the issues. Employing the agent as the ""brain"" effectively realizes a unified generation and editing system.",,src/resources/pdf/GenArtist/auto/images/7559bc4365e0c752e1aff138f010435317fc821be78d160d8d5e5045de7b2b1c.jpg,7,19,792.0,4,612.0,src/resources/pdf/GenArtist.pdf,
"[106, 273, 505, 372]","43, 37, 6] and editing methods [2, 61, 45, 15] have been proposed and achieved high-quality generated images. Based on these general models, many methods conduct finetuning or design additional modules for some specialized tasks, like customized image generation [42, 21, 23, 30], image generation with text rendering [5, 4], exemplar-based image editing [56, 8], image generation that focuses on persons [53]. Meanwhile, some methods aim to improve the controllability of texts over images. For example, ControlNet [62] controls Stable Diffusion with various conditioning inputs like Canny edges, [50] adopts sketch images for conditions, and layout-to-image methods [24, 54, 25, 7] synthesize images according to the given bounding boxes of objects. Despite the success, these methods still focus on specific tasks, thus unable to support unified image generation and editing.","[ 0.01689148 -0.01424408 -0.02748108 ... -0.01303101  0.03201294
 -0.03826904]",1,,text,,,,,7,19,792.0,4,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 377, 505, 487]","AI agent. Large language models (LLMs), like ChatGPT, Llama [48, 49], have demonstrated impressive capability in natural language processing. The involvement of vision ability for multimodal large language models (MLLMS), like LLaVA [26], Claude, GPT-4 [34], further enables the models to process visual data. Recently, LLMs begin to be adopted as agents for executing complex tasks. These works [57, 44, 29] apply LLMs to learn to use tools for tasks like visual interaction, speech processing, compositional visual tasks [16], software development [38], gaming [11], APP use [60] or math [55]. Recently, the idea of AI agents has also begun to be applied to image generation related tasks. For example, [25, 13] design scene layout with LLMs, [52] utilizes LLMs to assist self-correcting, [51, 59] target at MLLMs in complex text-to-image generation problems, and [39] leverages LLM for model selection in the text-to-image generation task.","[ 0.01343536  0.00976562  0.02330017 ... -0.02334595  0.00853729
 -0.0118866 ]",2,,text,,,,,7,19,792.0,4,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 504, 165, 518]",3 Method,"[ 0.04544067  0.03320312 -0.0249939  ...  0.03091431  0.01353455
 -0.00805664]",3,,text,,,,,7,19,792.0,4,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 531, 505, 586]","The overview of our GenArtist is illustrated in Fig. 2. The MLLM agent coordinates the whole system. Its primary responsibilities center around decomposing the complicated tasks and constructing the planning tree with step-by-step verification for image generation, editing, and self-correction. It invokes tools from an image generation tool library and an editing tool library to execute the specific operations, and an auxiliary tool library serves to provide missing position-related values.","[ 0.02372742  0.0109024  -0.02642822 ...  0.00580215  0.02871704
 -0.00923157]",4,,text,,,,,7,19,792.0,4,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 601, 321, 614]",3.1 Planning Tree with Step-by-Step Verification,"[ 0.0333252   0.01864624 -0.04052734 ... -0.01145935  0.05886841
  0.00489807]",5,,text,,,,,7,19,792.0,4,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 623, 506, 722]","Decomposition. When it comes to complicated prompt inputs, existing methods usually cannot understand all requirements, which hurts the controllability and reliability of model results. The MLLM agent thus first decomposes the complex problems. For generation tasks, it decomposes both object and background information according to the text prompts. It extracts the discrete objects embedded within the text prompts, along with their associated attributes. For background information, it mainly analyzes the overall scene and image style required by the input texts. For editing tasks, It decomposes complex editing operations into several specific actions, such as add, move, remove, into simple editing instructions. After decomposition, the simpler operations can be relatively easier to address, which thus improves the reliability of model execution.","[ 0.02926636 -0.00508881  0.01519012 ... -0.02693176 -0.00614929
 -0.00679398]",6,,text,,,,,7,19,792.0,4,612.0,src/resources/pdf/GenArtist.pdf,
"[365, 261, 493, 402]",,"[ 0.03811646 -0.01780701 -0.02302551 ... -0.01525116 -0.01267242
 -0.0609436 ]",0,,image,,"Figure 3: Illustration of the tree for planning. The sub-tree of the ""alternative generation tool"" node will be adaptively generated after verification, and the sub-tree of the ""instruction"" node is the same as the left.",,src/resources/pdf/GenArtist/auto/images/bb19979de242d48a9ef13e4455b489a5383f9035adfb510a641bfe00348cf344.jpg,7,19,792.0,5,612.0,src/resources/pdf/GenArtist.pdf,
"[106, 72, 505, 138]","Tree construction. After decomposition, we organize all operations into a structure of tree for planning. Such a tree primarily consists of three types of nodes: initial nodes, generation nodes, and editing nodes. The initial node serves as the root of the tree, marking the beginning of the system. Generation nodes are about image generation using tools from the generation tool library, while editing nodes are about performing a single editing operation using the corresponding tools from the editing tool library. For pure image editing tasks, the generation nodes will be absent.","[ 0.01593018 -0.03326416  0.01779175 ... -0.05264282  0.00362778
 -0.0368042 ]",1,,text,,,,,7,19,792.0,5,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 144, 505, 243]","In practice, as the correctness of generated images cannot be guaranteed, we introduce the self- correction mechanism to assess and rectify the results of generation. Each generation node thus has a sub-tree consisting entirely of editing nodes for self-correction. After the tools in the generation nodes are invoked and verification is conducted, this sub-tree will be adaptively generated by the MLLM agent. Specifically, after verification, we instruct the MLLM agent to devise a series of corresponding editing operations to correct the images. Take the example in Fig. 2 for example, editing actions including ""add a black bicycle"", ""edit the color of the scooter to blue"", ""add a bird"" should be conducted. These operations ainriteia lorganized into a tree structure to be the sub-tree of the generation node, allowing foar dsitiponetocoli:fic planning of self-correction.","[ 0.02589417 -0.01139832 -0.02050781 ... -0.00951385  0.00320244
 -0.01529694]",2,,text,,,,,7,19,792.0,5,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 246, 357, 477]","Each generation or ediattiribnutge  daiticngt itol:n corresponds to a node in the tree, with its subsequent operations as its child nodes. This con- struction initially forms a ""chain"", enabling a planning chain. Then, we note that we can usually utiliiznpe  tdexitfferent tools to address the same problem. For example, for adding an object into the image, we can employ a tool specifically designed for object addition or instruction-basedtooel:dLiMtDing modgenlesratibony translat- ing the adding operation into text instructions. Similarly, for attribute editing, we can use attribute editing models or utilize replacement or instruction-bablsacekdb yecldeitina bgl kmbicoycdl els. Moreover, numerous generation tools can achieve text-to-image genera- tion, and varying the random seeds can also produce different outputs. We consider tahttreibsute nditoindg:erse lacse siblinisntrugctsio, :all serving as child nodes of their parent nodes.b eTschooeteyr taolbsluoe share the same sub-tree, containing subsequent editing operations. The tool selected by the MLLM agent will be placed as the optimal child node and positioned on the far left. In this way, we establish the structure of the tree. An illustration example for the Fig. 2 case is provided in Fig. 3 (we omit some sub-trees with identical structures or adaptively generated after generation nodes, and some nodes about varying random seeds for simplicity).","[ 0.03500366 -0.04241943  0.01285553 ... -0.02937317 -0.00098515
 -0.04074097]",3,,text,,,,,7,19,792.0,5,612.0,src/resources/pdf/GenArtist.pdf,
"[106, 482, 509, 581]","Planning. Once the tree is established, planning for self- correction or the whole system can be viewed as the pre-order traversal of the structure. For a particular node, its corresponding tool is invoked to conduct the operation, followed by verification to determine whether the editing is successful. If successful, the process proceeds to its leftmost child node for subsequent operations, and its sibling nodes are deleted. If unsuccessful, the process backtracks to its sibling nodes, and its sub-tree is removed. This process continues until the generated image is correct, i.e., when a node at the lowest level successfully executes. We can also limit the branching factor or the number of nodes of the tree for early termination, and require the agent to return the most accurate image.","[ 0.00519943  0.01615906 -0.01343536 ... -0.00575256 -0.00451279
 -0.02461243]",4,,text,,,,,7,19,792.0,5,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 585, 505, 673]","Verification. As described above, the verification mechanism plays a crucial role both in tree construction and the execution process. Through the multimodal perception capability of the MLLM, the agent verifies the correctness of the generated images. The main aspect of verification involves the objects contained in the text, together with their own attributes like their color, shape, texture, the positions of the objects, the relationship among these different objects. Besides, the background, scene, overall style and the aesthetic quality of generated images are also considered. Since the perception ability of existing models tends to be superior to the generative ability, employing such verification allows for effectively assessing the correctness of generated images.","[-0.02030945 -0.00849915 -0.01276398 ... -0.0052681   0.00302696
  0.01413727]",5,,text,,,,,7,19,792.0,5,612.0,src/resources/pdf/GenArtist.pdf,
"[108, 678, 505, 711]","It is also worth mentioning that during verification, in addition to the accuracy of the generated images, the agent is also required to assess their aesthetic quality. If the overall quality is poor, the agent will utilize different generation tools or choose different random seeds to regenerate the images, in order to ensure their overall quality. Meanwhile, as an agent-centered system, the framework is also flexible in terms of human-computer interaction. During verification, human feedback can be appropriately integrated. By incorporating human evaluation and feedback on the overall quality of the images, the quality of the generated images can be further improved.","[ 0.0291748  -0.01390839 -0.02360535 ... -0.00922394 -0.00323296
 -0.0145874 ]",6,,text,,,,,7,19,792.0,5,612.0,src/resources/pdf/GenArtist.pdf,
"[114, 107, 493, 229]",,"[ 0.01152039 -0.03707886 -0.00804138 ... -0.03033447  0.04684448
 -0.03894043]",0,,table,,"Table 1: GenArtist utilized tool library, including the tool names and their skills. The main tools are from the generation tool library and the editing tool library. The following models represent all the tools used in our current version, while new models can be seamlessly added.",,src/resources/pdf/GenArtist/auto/images/04be5911829f63c28731ad1c4443739babc7b57c1b64066801a93443d3470c2b.jpg,8,19,792.0,6,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 251, 505, 295]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",1,,text,,,,,8,19,792.0,6,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 310, 185, 322]",3.2 Tool Library,"[ 0.01464081  0.02120972 -0.00570679 ...  0.014328    0.05166626
 -0.02278137]",2,,text,,,,,8,19,792.0,6,612.0,src/resources/pdf/GenArtist.pdf,
"[106, 331, 505, 419]","After constructing the planning tree, the agent proceeds to execute each node by calling external tools, ultimately solving the problem. We first introduce the tools used in GenArtist. The primary tools that the MLLM agent utilizes can be generally divided into the image generation tool library and the editing tool library. The specific tools we utilize currently are listed in Tab. 1, and some new tools can be seamlessly added, allowing for the expansion of the tool library. To assist the subsequent tool selection, we need to convey information to the MLLM agent about the specific task performed by the tool, its required inputs, and its characteristics and advantages. The prompts for introducing tools consist of the following parts specifically:","[ 0.04714966 -0.015625    0.00914764 ... -0.02107239  0.01911926
 -0.02445984]",3,,text,,,,,8,19,792.0,6,612.0,src/resources/pdf/GenArtist.pdf,
"[108, 421, 506, 563]","• The tool skill and name. It briefly describes the tool-related task and its name, as listed in Tab. 1, such as (text-to-image, SDXL), (canny-to-image, ControlNet), (object removal, LaMa). It serves as a unique identifier, enabling the agent to differentiate the utilized tools. • The tool required inputs. It pertains to the specific inputs required for the execution of the tool. For example, text-to-image models require ""text"" as input for generation, customization models also need ""subject images"" for personalized generation. Most of object-level editing tools demand instructions about ""object name"" and ""object position"". • The tool characteristic and advantage. It primarily provides a more detailed introduction of the tool, including its specific characteristics, serving as a key reference for the agent during tool selection. For example, SDXL can be a general text-to-image generation model, LMD usually controls scene layout strictly and is suitable for compositional text-to-image generation, where text prompts usually contain multiple objects, BoxDiff controls scene layout relatively loosely, TextDiffuser is specially designed for image generation with text rendering.","[ 0.05126953 -0.04638672 -0.01489258 ... -0.00577927  0.03305054
 -0.01757812]",4,,text,,,,,8,19,792.0,6,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 575, 261, 587]",3.3 Position-Aware Tool Execution,"[ 0.06137085  0.00570297  0.02371216 ...  0.00159645  0.07910156
 -0.04315186]",5,,text,,,,,8,19,792.0,6,612.0,src/resources/pdf/GenArtist.pdf,
"[108, 596, 503, 630]","With tool libraries, the MLLM agent will further perform tool selection and execution to utilize the suitable tool for fulfilling the image generation or editing task. Before tool execution, we compensate for the deficiency of position information in user inputs and the MLLM agent through two designs:","[ 0.01483154  0.0107193   0.01043701 ... -0.01591492  0.03491211
 -0.00860596]",6,,text,,,,,8,19,792.0,6,612.0,src/resources/pdf/GenArtist.pdf,
"[106, 634, 505, 722]","Position-related input compensation. In practice, it is common to encounter scenes where the agent selects a suitable tool but some necessary user inputs are missing. These user inputs are mostly related to positions. For example, for some complex text prompts where multiple objects exist, the layout-to-image tool can be suitable. However, users may not necessarily provide the scene layouts and usually only text prompts are provided. In such cases, due to the absence of some necessary inputs, these suitable tools cannot be directly invoked. We therefore introduce the auxiliary tool library to provide these position-related missing inputs. This auxiliary tool library mainly contains: 1) localization models like object detection [28] or segmentation [20] models, to provide position information of objects for some object-level editing tools; 2) the preprocessors of ControlNet [62] like the pose estimator, canny edge map extractor, depth map extractor; 3) some LLM-implemented tools, like the scene layout generator [25, 13]. The MLLM agent can invoke these auxiliary tools automatically if necessary, to guarantee that the most suitable tool to address the user instruction can be utilized, rather than solely relying on user-provided inputs to select tools.","[ 0.0139389  -0.00465012 -0.00514221 ...  0.01676941  0.06439209
 -0.01863098]",7,,text,,,,,8,19,792.0,6,612.0,src/resources/pdf/GenArtist.pdf,
"[115, 129, 495, 267]",,"[ 0.00705338 -0.0234375   0.01044464 ...  0.01889038  0.03302002
 -0.03088379]",0,,table,,"Table 2: Quantitative Comparison on T2I-CompBench with existing text-to-image generation models and compositional methods. Our method demonstrates superior compositional generation ability in both attribute binding, object relationships, and complex compositions. We use the officially updated code for evaluation, which updates the noun phrase number. Consequently, some metric values for certain methods may be lower than those reported in their original papers.",,src/resources/pdf/GenArtist/auto/images/d081582f6c025564e90faf0fd8462e0ae5d9133911c6d310d870aa5d4eaa9b0e.jpg,8,19,792.0,7,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 287, 504, 343]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",1,,text,,,,,8,19,792.0,7,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 347, 505, 447]","Position information introduction. Existing MLLMs primarily focus on text comprehension and holistic image perception, with relatively limited attention to precise position information within images. MLLMs can easily determine whether objects exist in the image, but sometimes struggle with discerning spatial relationships between objects, such as whether a specific object is to the left or right of another. It is also more challenging for these MLLMs to provide accurate guidance for tools that require position-related inputs, such as object-level editing tools. To address this, we employ an object detector on the input images, and include the detected objects along with their bounding boxes as part of the prompt, to provide a spatial reference for the MLLM agent. In this way, the agent can effectively determine the positions within the image where certain tools should operate.","[ 0.0174408  -0.01361084  0.01191711 ...  0.02468872  0.04138184
 -0.00222397]",2,,text,,,,,8,19,792.0,7,612.0,src/resources/pdf/GenArtist.pdf,
"[108, 451, 482, 462]",The prompts for the agent to conduct tool selection thus mainly consist of the following parts:,"[ 0.07598877 -0.03140259  0.01251221 ... -0.00663376  0.00301361
 -0.01716614]",3,,text,,,,,8,19,792.0,7,612.0,src/resources/pdf/GenArtist.pdf,
"[108, 468, 506, 577]","• Task instruction. Its main purpose is to clarify the task of the agent, i.e., tool selection within a unified generation and editing system. Simultaneously, it takes user instructions as input and specifies the output format. We request the agent to output in the format of {""tool_name"":tools, ""input"":inputs} and annotate missing inputs with the pre-defined specified identifier. • Tool introductions. We input the description of each tool into the agent in the format as described earlier. The detailed information about the tools will serve as the crucial references for the tool selection process. We also state that the primary criterion for tool selection is the suitability of the tool, rather than the content of given inputs, since missing inputs can be generated automatically. • Position information. The outputs from the object detector are utilized and provided to the MLLM agent to compensate for the lack of position information.","[ 0.04400635 -0.02590942 -0.00514221 ... -0.00242996  0.04345703
 -0.01520538]",4,,text,,,,,8,19,792.0,7,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 582, 505, 659]","In summary, the basic steps for tool execution are as follows: First, determine whether the task pertains to image generation or editing. Next, conduct tool selection according to the instructions and the characteristics of the tools, and output in the required format. Finally, for missing inputs which are necessary for the selected tools, utilize auxiliary tools to complete them. Upon completing these steps, the agent will be able to correctly execute the appropriate tools, thereby initially meeting the requirements of users. The integration, selection, and execution of diverse tools significantly facilitate the development of a unified image generation and editing system.","[ 0.04220581 -0.00167751  0.00421524 ... -0.02598572  0.03997803
 -0.01260376]",5,,text,,,,,8,19,792.0,7,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 675, 191, 688]",4 Experiments,"[-0.02713013  0.05657959 -0.03756714 ... -0.0177002  -0.0276947
 -0.00764084]",6,,text,,,,,8,19,792.0,7,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 699, 503, 722]","In this section, we demonstrate the effectiveness of our GenArtist and its unified ability through extensive experiments in image generation and editing. For image generation, we mainly conduct quantitative comparisons on the recent T2I-CompBench benchmark [18]. It is mainly about image generation with complex text prompts, involving multiple objects together with their own attributes or relationships. For image editing, we mainly conduct comparisons on the MagicBrush benchmark [61], which involves multiple types of text instructions, both single-turn and multi-turn dialogs for image editing. We choose GPT-4V [34] as our MLLM agent. In quantitative comparative experiments, we constrain the editing tree to be a binary tree.","[ 0.01160431  0.00340462 -0.01367188 ...  0.01544189  0.02420044
 -0.0120697 ]",7,,text,,,,,8,19,792.0,7,612.0,src/resources/pdf/GenArtist.pdf,
"[132, 96, 479, 217]",,"[ 0.04702759 -0.00947571 -0.05297852 ...  0.00480652  0.0191803
 -0.02349854]",0,,table,,Table 3: Quantitative Comparison on MagicBrush with existing image editing methods. Multi- turn setting evaluates images that iteratively edited on the previous source images in edit sessions.,,src/resources/pdf/GenArtist/auto/images/e152e42d7c89cc6e70ed35afecdb188de91a3a8ebddc6a5243a206ee4d5e485b.jpg,8,19,792.0,8,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 237, 505, 304]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",1,,text,,,,,8,19,792.0,8,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 316, 324, 328]",4.1 Comparison with Image Generation Methods,"[-0.00533676  0.0158844  -0.01165771 ... -0.01119995  0.02542114
 -0.03356934]",2,,text,,,,,8,19,792.0,8,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 337, 505, 512]","We list the quantitative metric results of our GenArtist in Tab. 2 and compare with existing state- of-the-art text-to-image synthesis methods. It can be seen that our GenArtist consistently achieves better performance on all sub-categories. This demonstrates that for the text-to-image generation task, our system effectively achieves better control over text-to-image correspondence and higher accuracy in generated images, especially in the case of complicated text prompts. It can be observed that based on Stable Diffusion, both scaling-up models such as SDXL, PixArt- , and those methods specifically designed for this context like Attn-Exct, GORS, can achieve higher accuracy. In contrast, our approach, by integrating various models as tools, effectively harnesses the strengths of these two categories of methods. Additionally, the self-correction mechanism further ensures the accuracy of the generated images. Compared to the current state-of-the-art model DALL-E 3, our method achieves nearly a  improvement in attribute binding, and a more than  improvement in spatial relationships, partly due to the inclusion of position-sensitive tools and the input of position information during tool selection. Compared to CompAgent, a method that also employs an AI agent for compositional text-to-image generation, GenArtist achieves a  improvement on average, partially because our system encompasses a more comprehensive framework for both generation and self-correction. The capability in image generation of our unified system can thus be demonstrated.","[ 0.00737    -0.01279449 -0.01930237 ...  0.0295105   0.04226685
 -0.04370117]",3,,text,,,,,8,19,792.0,8,612.0,src/resources/pdf/GenArtist.pdf,
"[106, 525, 307, 537]",4.2 Comparison with Image Editing Methods,"[ 0.00176525  0.025177   -0.00666046 ...  0.00823212  0.03482056
 -0.016922  ]",4,,text,,,,,8,19,792.0,8,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 545, 505, 644]","We then list the comparative quantitative comparisons on the image editing benchmark MagicBrush in Tab. 3. Our GenArtist also achieves superior editing results, no matter in the single-turn or multi-turn setting, compared to both previous global description-guided methods like Null Text Inversion and instruction-guided methods like InstrctPix2Pix and MagicBrush. The main reason is that editing operations are highly diverse, and it’s challenging for a single model to achieve excellent performance across all these diverse editing operations. In contrast, our method can leverage the strengths of different models comprehensively. Additionally, the planning tree can effectively consider scenarios where model execution fails, making editing results more reliable and accurate. The capability in image editing of our unified system can thus be demonstrated.","[-0.00106812 -0.01217651 -0.03610229 ... -0.01889038  0.0085144
 -0.01982117]",5,,text,,,,,8,19,792.0,8,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 657, 194, 669]",4.3 Ablation Study,"[ 0.0173645   0.03485107 -0.02394104 ...  0.0133667   0.02842712
 -0.02262878]",6,,text,,,,,8,19,792.0,8,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 678, 505, 722]","We finally conduct the ablation study on the T2I-CompBench benchmark and list the results in Tab. 4. We present the results of Stable Diffusion as a reference. The top section includes various tools relevant to the task, including text-to-image, layout-to-image, and customized generation methods. It can be observed that through scaling up or additional design, these tools have generally achieved","[ 0.01082611 -0.0074234  -0.00521851 ...  0.01649475  0.03237915
 -0.07427979]",7,,text,,,,,8,19,792.0,8,612.0,src/resources/pdf/GenArtist.pdf,
"[123, 250, 489, 326]",,"[ 0.00283432 -0.01139832 -0.00480652 ...  0.01152039  0.00475311
 -0.05603027]",0,,image,,,,src/resources/pdf/GenArtist/auto/images/0bd8d68cdcf23e88ba3e289a7c9f81e6ebd9807ce7e8e96f049b89be98ec295a.jpg,9,19,792.0,9,612.0,src/resources/pdf/GenArtist.pdf,
"[118, 96, 493, 207]",,"[ 0.02474976 -0.03598022  0.00167656 ...  0.00692749  0.05914307
 -0.05130005]",1,,table,,"Table 4: Ablation Study on T2I-CompBench. The upper section is about relevant tools from the generation tool library, then we study the tool selection and planning mechanisms respectively.",,src/resources/pdf/GenArtist/auto/images/41ba7a94f270e3b826736ae55ad3bf9dd4d5eb17fa7521a769817ad854007b51.jpg,9,19,792.0,9,612.0,src/resources/pdf/GenArtist.pdf,
"[108, 506, 304, 539]",,"[ 0.0451355  -0.00814819  0.03201294 ...  0.02035522  0.02073669
 -0.01187897]",2,,table,,Table 5: Abaldad taigiorafnf  Studstrye motno  htehfaermposition-aware tool executionnst eoamn efTt 2I-ComaddpitiBone: nch.,,src/resources/pdf/GenArtist/auto/images/f797047a94295c1c61ccf79f238ac8566bb226e25a4cafb611b6e0fbe88ff7c2.jpg,9,19,792.0,9,612.0,src/resources/pdf/GenArtist.pdf,
"[123, 230, 266, 246]","Two white sheep on the left, a black goat on the middle and a white goat on the right in a field.","[-0.02049255 -0.05426025 -0.00238419 ...  0.00118065  0.01956177
 -0.02159119]",3,,text,,,,,9,19,792.0,9,612.0,src/resources/pdf/GenArtist.pdf,
"[313, 227, 459, 251]",A man with long black hair in a pony tail has a beard and is wearing a red hat and dark colored suit while he looks at his cell phone and is smiling.,"[ 0.01835632  0.02108765 -0.00596619 ... -0.01158142  0.02284241
 -0.0144577 ]",4,,text,,,,,9,19,792.0,9,612.0,src/resources/pdf/GenArtist.pdf,
"[106, 372, 505, 471]","better results than Stable Diffusion. After tool selection by the MLLM agent, the quantitative metrics outperform all these tools. This demonstrates that the agent can effectively choose appropriate tools based on the content of text prompts, thus achieving superior performance compared to all these tools. If we use a chain structure for planning to further correct the images, we achieve an average improvement of , demonstrating the necessity of verification and correction of erroneous results. Furthermore, by utilizing a tree structure, we can further consider and handle cases where the editing tool fails, resulting in even more reliable output results. Such an ablation study illustrates the necessity of integrating multiple models as tools and utilizing rterde.eMesatrnwuhcilteu,remfovre tphlearnignhitnmgo. oTrbhikee.reasonableness of our agent-centric system designs can also be demonstrated.","[ 0.02732849  0.01986694 -0.02868652 ... -0.00089645  0.02174377
 -0.0385437 ]",5,,text,,,,,9,19,792.0,9,612.0,src/resources/pdf/GenArtist.pdf,
"[315, 476, 505, 542]","Regarding positieodinti-nga: ware tool eplxace :cution, we list the correspon#dheilnmegt  ablation stot ruedy in Tab. 5. We evaluate the performance on the spatial and complex aspects of T2I-CompBench, as these two aspects mainly involve p#omsoittoirboikne-sensitive text prompts for image generation. As multi-","[ 0.04898071 -0.01473999  0.01011658 ...  0.01875305  0.01947021
 -0.02284241]",6,,text,,,,,9,19,792.0,9,612.0,src/resources/pdf/GenArtist.pdf,
"[106, 542, 506, 586]","modal large models are usually not sensitive to position information, the performance is limited without the inclusion of position information, only a slight improvement over the tool selection re- sults. After introducing position information, which enhances spatial awareness, there is a significant improvement in both the spatial and complex aspects. This validates the reasonability of our design.","[ 0.06173706 -0.00725937  0.02105713 ...  0.01096344  0.05023193
 -0.04751587]",7,,text,,,,,9,19,792.0,9,612.0,src/resources/pdf/GenArtist.pdf,
"[106, 591, 505, 722]","We further list some visualized generation examples in Fig. 4 to illustrate our planning tree and how the system proceeds. In the first example, as the text prompts contain multiple objects, the agent chooses the LMD tool for generation. However, there are still some errors in the image. The agent first attempts to use the attribute editing tool to change the leftmost sheep to white, but it fails. The agent further attempts to modify the color using the replace tool, but after replacement, the size of the sheep becomes too small and not very noticeable. The agent then chooses to remove the black sheep and then adds a white sheep, successfully achieving the same effect as editing color. Finally, the agent uses the object addition tool to add a goat on the right side, ensuring that the image accurately matches the text prompt in the end. In the second example, due to the lack of clarity of the hair in the BoxDiff generated image, the editing tools cannot edit so that the hair correctly matches the description of ""long black hair"". Therefore, the agent invokes another generation tool to guarantee the final image is correct. Some image editing examples are also provided in Fig. 5.","[ 0.022995   -0.0267334   0.00785828 ... -0.01745605  0.00815582
 -0.04037476]",8,,text,,,,,9,19,792.0,9,612.0,src/resources/pdf/GenArtist.pdf,
"[115, 86, 497, 145]",,"[ 7.97271729e-03 -4.71115112e-03  8.87870789e-04 ... -4.07028198e-03
  6.86049461e-05 -3.53698730e-02]",0,,image,,"Let the color of the helmet on the rightmost person be red. Meanwhile, remove the right motorbike.",,src/resources/pdf/GenArtist/auto/images/2b747cc51218749b2c6db65ea81350237e54becd1c868950a8ce9327a4d0cfb9.jpg,9,19,792.0,10,612.0,src/resources/pdf/GenArtist.pdf,
"[112, 179, 499, 264]",,"[-0.00707245  0.01387787 -0.01268005 ... -0.00709534 -0.01045227
 -0.0166626 ]",1,,image,,Figure 6: The error cases of GenArtist caused by the ability of editing tools (the left) or the wrong output of localization tools (the right).,,src/resources/pdf/GenArtist/auto/images/2af548822767078ffee11a0fe34a84e70df83122cd6d7e8d40c86fbf2df3c10d.jpg,9,19,792.0,10,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 317, 216, 329]",4.4 Error Case Analysis,"[ 0.00382614  0.02311707 -0.00066566 ...  0.01675415  0.02774048
  0.00610733]",2,,text,,,,,9,19,792.0,10,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 338, 505, 437]","We further analyze some error cases from our GenArtist in Fig. 6. As can be seen, sometimes, despite the agent correctly planning the specific execution of tools, the limitations of the tools themselves prevent correct execution, leading to incorrect results. For example, in the first case, it is required to add a very small blue cup. However, due to the lack of fine resolution ability in existing editing tools, the generated blue cup’s size is inaccurate. In addition, as shown in the second case, errors in the output of localization tools can also affect the final result. For instance, when asked to remove the lettuce in the middle of a sandwich, the segmentation model fails to accurately identify the part of the object, leading to the erroneous removal operation. Utilizing more powerful tools or incorporating some human feedback during the verification stage can effectively address this issue.","[ 0.01869202 -0.01722717 -0.00725937 ...  0.00281906  0.02661133
 -0.00816345]",3,,text,,,,,9,19,792.0,10,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 454, 183, 468]",5 Conclusion,"[-0.00580978  0.04901123 -0.03207397 ... -0.02320862 -0.00346375
  0.0056572 ]",4,,text,,,,,9,19,792.0,10,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 481, 505, 579]","In this paper, we propose GenArtist, a unified image generation and editing system coordinated by a MLLM agent. By decomposing input problems, employing the tree structure for planning and invoking external tools for execution, the MLLM agent acts as the ""brain"" to generate high- fidelity and accurate images for various tasks. Extensive experiments demonstrate that GenArtist well addresses complex problems in image generation and editing, and achieves state-of-the-art performance compared to existing methods. Its ability in a wide range of generation tasks also validates its unified capacity. We believe our approach of leveraging the agent to achieve a unified image generation and editing system with enhanced controllability can provide valuable insights for future research, and we consider it an important step toward the future of autonomous agents.","[ 0.01794434  0.04571533 -0.02218628 ...  0.00312614  0.02883911
 -0.01339722]",5,,text,,,,,9,19,792.0,10,612.0,src/resources/pdf/GenArtist.pdf,
"[108, 597, 202, 610]",Acknowledgement,"[ 0.01491547  0.07092285  0.03390503 ... -0.01034546  0.02111816
 -0.03204346]",6,,text,,,,,9,19,792.0,10,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 623, 505, 645]","We gratefully acknowledge the support of Mindspore, CANN(Compute Architecture for Neural Networks) and Ascend AI Processor used for this research.","[ 1.07727051e-01  3.33251953e-02 -2.47955322e-02 ... -9.53674316e-06
 -8.30841064e-03 -4.58374023e-02]",7,,text,,,,,9,19,792.0,10,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 667, 505, 722]","Limitation and Potential Negative Social Impacts. Our method employs an MLLM agent as the core for the entire system operations. Therefore, the method effectiveness depends on the performance of the MLLM used. Current MLLMs, such as GPT-4, are capable of meeting most basic requirements. For tasks that exceed the capability of GPT-4, our method may fail. Additionally, the misuse of image generation or editing could potentially lead to negative social impacts.","[ 0.03022766 -0.00211906 -0.01751709 ...  0.00487518 -0.0193634
 -0.03158569]",8,,text,,,,,9,19,792.0,10,612.0,src/resources/pdf/GenArtist.pdf,
"[115, 188, 495, 287]",,"[ 0.00550461 -0.01623535  0.0102005  ...  0.00546265  0.02789307
 -0.02851868]",0,,table,,Table 6: Quantitative Comparison on T2I-CompBench with existing text-to-image generation models and compositional methods. The metric here is from the officially old-version code.,,src/resources/pdf/GenArtist/auto/images/90acc0559549a2caf229fa2174a00f9c9636e7ea0d0dbb426a58af32a4744c36.jpg,9,19,792.0,11,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 71, 158, 85]",Appendix,"[-0.00761414 -0.00054884 -0.01335144 ... -0.00577164  0.02636719
 -0.02845764]",1,,text,,,,,9,19,792.0,11,612.0,src/resources/pdf/GenArtist.pdf,
"[106, 96, 504, 118]","In the appendix, we primarily include more quantitative comparisons, along with additional visual results, to more comprehensively compare with existing state-of-the-art methods.","[ 0.01863098 -0.03601074 -0.02568054 ... -0.01899719  0.00767517
 -0.00939941]",2,,text,,,,,9,19,792.0,11,612.0,src/resources/pdf/GenArtist.pdf,
"[108, 134, 292, 148]",A More Quantitative Experiments,"[-0.01449585  0.07269287 -0.01615906 ...  0.00213814 -0.03695679
  0.01042938]",3,,text,,,,,9,19,792.0,11,612.0,src/resources/pdf/GenArtist.pdf,
"[106, 298, 506, 408]","Considering that many existing methods on T2I-CompBench report results based on the official old version evaluation code, here we utilize the same old version evaluation method and list the results in Tab. 6. It can be observed that the performance improvement keeps consistently under this metric. Compared to the current state-of-the-art text-to-image method, DALL-E 3, our approach achieves over a  improvement in attribute binding. For shape-related attributes, the improvement is even up to . Additionally, compared to RPG, which also utilizes MLLM to assist image generation, our method demonstrates an over  enhancement. This is because our GenArtist incorporates MLLM for step-by-step verification and the corresponding planning, thereby better ensuring the correctness of the images. This quantitative comparison more comprehensively demonstrates the effectiveness of our method.","[ 0.00760651 -0.0368042  -0.03594971 ...  0.01147461  0.0375061
 -0.03768921]",4,,text,,,,,9,19,792.0,11,612.0,src/resources/pdf/GenArtist.pdf,
"[108, 423, 284, 437]",B More Qualitative Experiments,"[-0.01638794  0.05334473 -0.01887512 ... -0.02224731 -0.01715088
 -0.00959778]",5,,text,,,,,9,19,792.0,11,612.0,src/resources/pdf/GenArtist.pdf,
"[106, 448, 504, 471]","In this section, we provide more visual analyses to further illustrate our GenArtist and to compare it more thoroughly with existing methods.","[-0.01618958 -0.03713989 -0.01881409 ...  0.02371216 -0.00545502
 -0.03024292]",6,,text,,,,,9,19,792.0,11,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 476, 505, 662]","Comparative visualized results on image generation. We first present visual comparisons with existing methods in Fig. 7. It can be observed that our GenArtist achieves superior results in multiple aspects: 1) Attribute Binding. For example, in the fourth example, there are strict requirements for the clothes and pants each person is wearing. Such numerous requirements are challenging for existing methods to meet. In this case, GenArtist can continuously verify and edit to ensure all these requirements are correctly satisfied. 2) Numeric Accuracy. In the second example, detailed quantity requirements are given for various objects. Our method can gradually achieve the correct quantities through addition and removal operations. In contrast, even though methods like  can meet numeric accuracy, they struggle to maintain the accuracy of other aspects, such as the atmosphere of the image. 3) Position Accuracy. By position-aware tool execution, better position-related accuracy can be guaranteed. In the first example, although DALL-E 3 can correctly predict many other aspects, it fails to accurately place the book on the left floor, which our method can achieve. 4) Complex Relationships, like the complex requirements for the relationship between the panda and bamboo in the fifth example. 5) Other Diverse Requirements. By integrating various tools, GenArtist effectively leverages the strengths of different tools to meet diverse requirements, the ability that a single model lacks. For instance, the text requirements in the third example are better handled by our method. Such visualized results strongly demonstrate the effectiveness of our method in image generation.","[ 0.01522064 -0.05731201 -0.01354218 ...  0.02156067  0.02818298
 -0.04284668]",7,,text,,,,,9,19,792.0,11,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 667, 504, 722]","Comparative visualized results on image editing. We further present comparisons with existing image editing methods in Fig. 8. GenArtist shows superior performance in several aspects: 1) Highly Specific Editing Instructions. For instance, in the first example, only a particular pizza needs to be modified, while the second example requires changes to the color and placement of a vase. Existing methods often struggle to satisfy such specific requirements. 2) Reasoning-based Instructions. The third example requires the model to autonomously determine which person needs to be removed. Because of the reasoning capability of the MLLM agent, our method can accurately make this determination. In contrast, even MGIE, which also uses MLLM assistance, fails to make the correct modification. 3) Instructions with Excessive Requirements. The fourth example requires different modifications to both birds, which existing methods struggle to achieve. 4) Multi-step Instructions.","[ 0.01776123 -0.01438904 -0.03747559 ... -0.01733398  0.02760315
 -0.02667236]",8,,text,,,,,9,19,792.0,11,612.0,src/resources/pdf/GenArtist.pdf,
"[118, 73, 499, 680]",,"[-0.03884888 -0.06555176 -0.04708862 ...  0.01278687  0.00859833
 -0.03695679]",0,,image,,"Figure 7: Qualitative comparison with existing state-of-the-art methods for image generation tasks. We compare our GenArtist with SOTA text-to-image models including SDXL [37], LMD  [25], RPG [59], PixArt-  [6], Playground [22], Midjourney [31], DALL-E 3 [1].",,src/resources/pdf/GenArtist/auto/images/afc042c650de6e05516c4b8f96412ab72214a5bc3d0bfb51a972eca9f806d58f.jpg,1,19,792.0,12,612.0,src/resources/pdf/GenArtist.pdf,
"[117, 70, 500, 615]",,"[ 0.01322174 -0.00076771 -0.03369141 ... -0.00442505  0.00814056
 -0.03091431]",0,,image,,"Figure 8: Qualitative comparison with existing state-of-the-art methods for image editing tasks. We compare our GenArtist with SOTA image editing models including InstructPix2Pix [2], MagicBrush [61], MGIE [14], InstructDiffusion [15]",,src/resources/pdf/GenArtist/auto/images/7deb398dc1b0db69303da20db7d59e7fd53a78dfa507dcb7b7d09c41b5f1a9b0.jpg,2,19,792.0,13,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 667, 505, 722]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",1,,text,,,,,2,19,792.0,13,612.0,src/resources/pdf/GenArtist.pdf,
"[117, 96, 496, 491]",,"[ 0.00722885 -0.07745361 -0.00059557 ... -0.03012085  0.05303955
  0.01075745]",0,,image,,"A girl is reading a book in the autumn park, with the same pose as the boy. Plenty of red and yellow leaves are in the park.",,src/resources/pdf/GenArtist/auto/images/83776e57f6079fe26e4afd6053760bf3cb58a482254a6103577ce1688fa9cb6c.jpg,4,19,792.0,14,612.0,src/resources/pdf/GenArtist.pdf,
"[116, 496, 495, 592]",,"[-0.05148315 -0.04400635 -0.01378632 ... -0.02435303  0.02302551
 -0.01905823]",1,,image,,"In the dim scene, a sea of molten lava, a valiant and lonely warrior is preparing to confront a monster, based on the given image.",,src/resources/pdf/GenArtist/auto/images/e11997e2d7fb7342735204c84895b22cc425f055af67acfbd3341f5be1e68d0d.jpg,4,19,792.0,14,612.0,src/resources/pdf/GenArtist.pdf,
"[116, 602, 496, 700]",,"[-0.04650879 -0.04211426 -0.02233887 ... -0.01861572  0.00105381
 -0.00437927]",2,,image,,Figure 9: Visualized results of GenArtist about various tasks and user instructions.,,src/resources/pdf/GenArtist/auto/images/58abbd9a80884b31bfe03cf5bdac2b581a4981bcc5bede8857e24b616cafb056.jpg,4,19,792.0,14,612.0,src/resources/pdf/GenArtist.pdf,
"[295, 79, 439, 93]","A winter beach at sunset, snow covered everywhere, with a layout similar to the given beach.","[-0.00558853 -0.06103516 -0.02908325 ...  0.01103973  0.03024292
  0.00169086]",3,,text,,,,,4,19,792.0,14,612.0,src/resources/pdf/GenArtist.pdf,
"[115, 74, 496, 286]",,"[ 0.00683594 -0.05725098 -0.00907898 ... -0.01930237  0.01780701
 -0.04504395]",0,,image,,Figure 10: Visualization of the step-by-step process for image generation tasks.,,src/resources/pdf/GenArtist/auto/images/5de2d9ca77f9ef87743ba44181463158baba12f85c1882d81d18c550be983130.jpg,4,19,792.0,15,612.0,src/resources/pdf/GenArtist.pdf,
"[120, 320, 495, 542]",,"[ 0.02781677 -0.04345703 -0.00269127 ... -0.02137756 -0.00657654
 -0.0526123 ]",1,,image,,Figure 11: Visualization of the step-by-step process for image editing tasks.,,src/resources/pdf/GenArtist/auto/images/fe0d2840d583c10272387b4ee1474ff2b677b2852f96af3fcfbfca737e709fb3.jpg,4,19,792.0,15,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 585, 505, 640]","The fifth example involves complex instructions including multiple operations. The MLLM agent can decompose the problem into multiple single-step operations, simplifying complex tasks. 5) Diverse Operations. It can be seen that our method excels in various editing operations, such as addition, removal, and attribute editing, due to the integration of different tools. These comparisons strongly demonstrate the effectiveness of our method in image editing.","[ 0.0425415   0.00861359  0.00452423 ... -0.02635193  0.00026226
 -0.03482056]",2,,text,,,,,4,19,792.0,15,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 645, 505, 722]","Visualized results about various tasks and user instructions. To demonstrate that our GenArtist can meet a wide range of user requirements, we provide visual examples in Fig. 9. As can be seen, because of the integration of various tools, our framework can efficiently address these diverse requirements. For instance, it can generate images with a layout or pose similar to a given image, as well as customization-related generation. Through the use of multiple generation and editing tools, our method also achieves greater control, such as representing more objects and more complex relationships between objects in customization generation. These visualization examples strongly illustrate the necessity of employing an agent for image generation and demonstrate that our approach effectively accomplishes the goal of unified image generation and editing.","[ 2.16364861e-05 -4.45251465e-02 -1.22070312e-02 ...  1.72805786e-03
  6.90917969e-02 -4.70581055e-02]",3,,text,,,,,4,19,792.0,15,612.0,src/resources/pdf/GenArtist.pdf,
"[105, 73, 504, 95]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,4,19,792.0,16,612.0,src/resources/pdf/GenArtist.pdf,
"[106, 100, 505, 199]","Visualization for the step-by-step process. Finally, we present our step-by-step visualized results in Fig. 10 and Fig. 11. For image generation, our method initially utilizes the most suitable tool to generate the initial image. If the image quality is too low or cannot be corrected after some modifica- tion operations, additional tools are invoked to continue generation. Further, for parts of the image that do not meet the text requirements, editing tools are continuously called to make modifications until the image correctly matches the text. For image editing, our method effectively decomposes the input problem and iteratively utilizes different tools to make step-by-step modifications until the image is correctly edited. This visualization clearly demonstrates the process, from decomposition and planning tree with step-by-step verification, to the final tool execution.","[ 0.01641846 -0.01757812  0.00196648 ... -0.02185059  0.01278687
 -0.03610229]",1,,text,,,,,4,19,792.0,16,612.0,src/resources/pdf/GenArtist.pdf,
"[107, 216, 163, 228]",References,"[-0.00434494  0.04223633 -0.02407837 ... -0.01483154  0.02075195
  0.00366783]",2,,text,,,,,4,19,792.0,16,612.0,src/resources/pdf/GenArtist.pdf,
"[109, 235, 506, 722]","[1] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai.com/papers/dall-e-3.pdf, 2023. [2] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. [3] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. TOG, 2023. [4] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser-2: Unleashing the power of language models for text rendering. arXiv preprint arXiv:2311.16465, 2023. [5] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: Diffusion models as text painters. In NeurIPS, 2023. [6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart- : Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024. [7] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention guidance. In WACV, 2024. [8] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. In CVPR, 2024. [9] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion- based semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022. [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. [11] Meta Fundamental AI Research Diplomacy Team (FAIR)†, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 2022. [12] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. In ICLR, 2023. [13] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. In NeurIPS, 2023. [14] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. In ICLR, 2024. [15] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Han Hu, Dong Chen, et al. Instructdiffusion: A generalist modeling interface for vision tasks. In CVPR, 2024. [16] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In CVPR, 2023. [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. [18] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A com- prehensive benchmark for open-world compositional text-to-image generation. In NeurIPS, 2023. [19] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction- based image editing with multimodal large language models. In CVPR, 2024. [20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. [21] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi- concept customization of text-to-image diffusion. In CVPR, 2023. [22] Daiqing Li, Aleks Kamko, Ali Sabet, Ehsan Akhgari, Linmiao Xu, and Suhail Doshi. Playground v2. [23] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. In NeurIPS, 2023. [24] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In CVPR, 2023. [25] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. arXiv preprint arXiv:2305.13655, 2023. [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [27] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual generation with composable diffusion models. In ECCV, 2022. [28] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. [29] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, et al. Internchat: Solving vision-centric tasks by interacting with chatbots beyond language. arXiv preprint arXiv:2305.05662, 2023. [30] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones 2: Customizable image synthesis with multiple subjects. arXiv preprint arXiv:2305.19327, 2023. [31] Midjourney. Midjourney, 2023. [32] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In CVPR, 2023. [33] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. In ICLR, 2024. [34] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [35] Maitreya Patel, Sangmin Jung, Chitta Baral, and Yezhou Yang.  -eclipse: Multi-concept personalized text-to-image diffusion models by leveraging clip latent space. arXiv preprint arXiv:2402.05195, 2024. [36] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [37] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [38] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023. [39] Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, and Shilei Wen. Diffusiongpt: Llm-driven text-to-image generation system. arXiv preprint arXiv:2401.10061, 2024. [40] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [42] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. [43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. [44] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. In NeurIPS, 2023. [45] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. arXiv preprint arXiv:2311.10089, 2023. [46] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdif- fusion: Harnessing diffusion models for interactive point-based image editing. In CVPR, 2024. [47] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In WACV, 2022. [48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [49] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [50] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or. Sketch-guided text-to-image diffusion models. In SIGGRAPH, 2023. [51] Zhenyu Wang, Enze Xie, Aoxue Li, Zhongdao Wang, Xihui Liu, and Zhenguo Li. Divide and conquer: Language models can plan and self-correct for compositional text-to-image generation. arXiv preprint arXiv:2401.15688, 2024. [52] Tsung-Han Wu, Long Lian, Joseph E Gonzalez, Boyi Li, and Trevor Darrell. Self-correcting llm-controlled diffusion models. In CVPR, 2024. [53] Guangxuan Xiao, Tianwei Yin, William T Freeman, Frédo Durand, and Song Han. Fastcom- poser: Tuning-free multi-subject image generation with localized attention. arXiv preprint arXiv:2305.10431, 2023. [54] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffu- sion. In ICCV, 2023. [55] Huajian Xin, Haiming Wang, Chuanyang Zheng, Lin Li, Zhengying Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, et al. Lego-prover: Neural theorem proving with growing libraries. In ICLR, 2023. [56] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In CVPR, 2023. [57] Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and additional opinions. arXiv preprint arXiv:2306.02224, 2023. [58] Ling Yang, Jingwei Liu, Shenda Hong, Zhilong Zhang, Zhilin Huang, Zheming Cai, Wentao Zhang, and Bin Cui. Improving diffusion-based image synthesis with context prediction. In NeurIPS, 2023. [59] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In ICML, 2024. [60] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023. [61] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually annotated dataset for instruction-guided image editing. In NeurIPS, 2023. [62] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. [63] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. Hive: Harnessing human feedback for instructional visual editing. In CVPR, 2024. [64] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu. Inversion-based style transfer with diffusion models. In CVPR, 2023.","[ 0.00241089 -0.01178741  0.01928711 ...  0.01100159  0.00227928
 -0.0098114 ]",3,,text,,,,,4,19,792.0,16,612.0,src/resources/pdf/GenArtist.pdf,
"[105, 34, 507, 723]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,1,19,792.0,17,612.0,src/resources/pdf/GenArtist.pdf,
"[105, 37, 508, 726]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,1,19,792.0,18,612.0,src/resources/pdf/GenArtist.pdf,
"[105, 71, 507, 311]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,1,19,792.0,19,612.0,src/resources/pdf/GenArtist.pdf,
"[50, 197, 559, 393]",,"[ 0.00983429 -0.0541687   0.02256775 ...  0.01502228  0.04742432
  0.01849365]",0,,image,,"Fig. 1: Failure cases of Stable Diffusion v2 [1]. Our compositional text-to-image generation benchmark consists of three categories: attribute binding (including color, shape, and texture), generative numeracy, object relationships (including 2D/3D-spatial relationship and non-spatial relationship), and complex compositions.",,src/resources/pdf/T2I-CompBench/auto/images/0671a1d4784ef568c9f06ce027e5776862a1cc500508bc11cd05dd940f942ad9.jpg,9,19,792.0,1,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[78, 56, 533, 138]",T2I-CompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text-to-image Generation,"[ 0.00827789  0.0329895   0.00953674 ...  0.01033783  0.02430725
 -0.01033783]",1,,text,,,,,9,19,792.0,1,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[129, 145, 477, 159]","Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, Xihui Liu","[ 0.0645752   0.05651855 -0.02452087 ...  0.00195885 -0.01252747
 -0.03120422]",2,,text,,,,,9,19,792.0,1,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 447, 300, 587]","Abstract—Despite the impressive advances in text-to-image models, they often struggle to effectively compose complex scenes with multiple objects, displaying various attributes and relation- ships. To address this challenge, we present T2I-CompBench , an enhanced benchmark for compositional text-to-image gen- eration. T2I-CompBench  comprises 8,000 compositional text prompts categorized into four primary groups: attribute binding, object relationships, generative numeracy, and complex com- positions. These are further divided into eight sub-categories, including newly introduced ones like 3D-spatial relationships and numeracy. In addition to the benchmark, we propose enhanced evaluation metrics designed to assess these diverse compositional challenges. These include a detection-based metric tailored for evaluating 3D-spatial relationships and numeracy, and an analysis leveraging Multimodal Large Language Models (MLLMs), i.e. GPT-4V, ShareGPT4v as evaluation metrics. Our experiments benchmark 11 text-to-image models, including state- of-the-art models, such as FLUX.1, SD3, DALLE-3, Pixart- , and SD-XL on T2I-CompBench++. We also conduct comprehensive evaluations to validate the effectiveness of our metrics and explore the potential and limitations of MLLMs. Project page is available at https://karine-h.github.io/T2I-CompBench-new/.","[-0.02398682  0.0049057   0.01776123 ...  0.00881958  0.02505493
  0.02282715]",3,,text,,,,,9,19,792.0,1,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 447, 563, 528]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",4,,text,,,,,9,19,792.0,1,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 533, 563, 552]","Index Terms—Image generation, compositional text-to-image generation, benchmark and evaluation.","[ 0.00083733  0.03271484 -0.00066805 ... -0.01760864  0.01486206
 -0.02676392]",5,,text,,,,,9,19,792.0,1,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[397, 564, 477, 574]",I. INTRODUCTION,"[-0.03158569  0.07940674 -0.03433228 ...  0.01701355  0.04846191
 -0.00282288]",6,,text,,,,,9,19,792.0,1,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 578, 563, 696]","R sEhCoEwNcaTsepdr rgeremsasriknabtleex -ctaop-iambialigteiegs inne ctrieoant [n1g] [d6iv]ehrsaes and high-fidelity images based on natural language prompts. However, we observe that even state-of-the-art text-to-image models often fail to compose multiple objects with different attributes and relationships into a complex and coherent scene, as shown in the failure cases of Stable Diffusion [1] in Figure 1. For example, given the text prompt “a blue bench on the left of a green car”, the model might bind attributes to the wrong objects or generate the spatial layout incorrectly.","[-0.01812744 -0.00962067  0.00891876 ...  0.02008057  0.02978516
  0.00395203]",7,,text,,,,,9,19,792.0,1,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 698, 563, 732]","Previous works have explored compositional text-to-image generation from different perspectives, such as concept con- junction [7], attribute binding (focusing on color) [8], [9], and spatial relationship [10]. Most of those works focus on a sub-problem and propose their own benchmarks for evaluating their methods. However, there is no consensus on the problem definition and standard benchmark of compositional text-to- image generation. Another challenge is the assessment of com- positional text-to-image models. Most previous works evaluate the models by image-text similarity or text-text similarity (between the caption predicted from the generated images and the original text prompts) with CLIPScore [11], [12] or BLIP [13], [14]. However, both metrics do not perform well for compositionality evaluation due to the ambiguity and difficulty in compositional vision-language understanding.","[-0.00325203 -0.02986145 -0.00208664 ... -0.01502228  0.00431442
 -0.00774384]",8,,text,,,,,9,19,792.0,1,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[49, 55, 300, 198]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,10,19,792.0,2,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 199, 299, 306]","In this paper, we propose an enhanced benchmark for compositional text-to-image generation, namely T2I- CompBench , which is the first comprehensive bench- mark that fills a critical gap in evaluating the composi- tional capabilities of text-to-image generation models. T2I- CompBench  not only provides a robust evaluation frame- work for assessing compositionality but also drives advance- ments in generating complex, high-fidelity images from intri- cate compositional text descriptions.","[-0.00674438  0.0355835   0.00768661 ...  0.0111618   0.02226257
 -0.00808716]",1,,text,,,,,10,19,792.0,2,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 307, 300, 544]","First, we propose a compositional text-to-image generation benchmark, T2I-CompBench , which consists of four cate- gories and eight sub-categories of compositional text prompts: (1) Attribute binding. Each text prompt in this category contains at least two objects and two attributes, and the model should bind the attributes with the correct objects to generate the complex scene. This category is divided into three sub-categories (color, shape, and texture) based on the attribute type. (2) Object relationships. The text prompts in this category each contain at least two objects with specified relationships between the objects. Based on the type of the relationships, this category consists of three sub-categories, 2D/3D-spatial relationship and non-spatial relationship. (3) Generative numeracy. Each prompt in this category involves one or multiple object categories with numerical quantities, ranging from one to eight. (4) Complex compositions, where the text prompts contain more than two objects or more than two sub-categories mentioned above. For example, a text prompt that describes three objects with their attributes and relationships.","[-0.00017703 -0.02160645  0.03863525 ...  0.00782776  0.02684021
  0.00714111]",2,,text,,,,,10,19,792.0,2,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 546, 300, 699]","Second, we introduce a set of evaluation metrics tailored to different categories of compositional prompts. For attribute binding evaluation, we propose the Disentangled BLIP-VQA metric, designed to address the challenges of ambiguous attribute-object correspondences. For assessing spatial rela- tionships and numeracy, we introduce the UniDet-based met- ric, which also incorporates depth estimation techniques and object detection mechanisms to evaluate 3D spatial relation- ships. Additionally, we propose an MLLM (Multimodal Large Language Model)-based metric for non-spatial relationships and complex compositions, examining the performance and limitations of MLLMs such as ShareGPT4V [15], and GPT- 4V [16] for compositionality evaluation.","[ 0.00099277 -0.01422882  0.02003479 ... -0.0158844   0.0231781
  0.0057869 ]",3,,text,,,,,10,19,792.0,2,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[47, 701, 300, 748]","Finally, we propose a new approach, Generative mOdel finetuning with Reward-driven Sample selection (GORS), for compositional text-to-image generation. We finetune Stable Diffusion v2 [1] model with generated images that highly align with the compositional prompts, where the fine-tuning loss is weighted by the reward which is defined as the alignment score between compositional prompts and generated images. This approach is simple but effective in boosting the model’s compositional abilities and can serve as a new baseline for future explorations.","[ 0.03213501  0.02427673  0.00859833 ...  0.00169563  0.04046631
 -0.02778625]",4,,text,,,,,10,19,792.0,2,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 55, 563, 126]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",5,,text,,,,,10,19,792.0,2,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 135, 563, 230]","Experimental results across four categories and eight sub- categories, validated with human correlation, demonstrate the effectiveness of our proposed evaluation metrics. Our method, GORS, consistently outperforms the baseline models. We benchmark 11 models on T2I-CompBench , including FLUX.1 [17], SD3 [18], DALLE-3 [19], Pixart-  [20], and SD-XL [21], highlighting the performances of current models in handling compositional tasks.","[ 0.03842163  0.02093506 -0.00270462 ...  0.02365112  0.02661133
 -0.0193634 ]",6,,text,,,,,10,19,792.0,2,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 239, 562, 274]","Compared with the preliminary conference version [22], this work introduces several non-trivial extensions in the following aspects:","[ 0.05627441 -0.0369873  -0.0008831  ...  0.01055145  0.06512451
  0.01499176]",7,,text,,,,,10,19,792.0,2,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[321, 294, 563, 413]","Broadening the problem definition and categories of compositional text-to-image generation. As the rapid evolvement of text-to-image generation models in the past year call for more comprehensive benchmarks, we broaden the problem definition of our benchmark by adding two sub-categories: generative numeracy (e.g., “four swans and two suitcases”) and 3D-spatial relation- ships (e.g.,“a cat in front of a chair”), resulting in a more enhanced benchmark with four categories and eight sub- categories.","[-0.01313782  0.01165009  0.01965332 ...  0.01086426  0.02949524
  0.01148224]",8,,text,,,,,10,19,792.0,2,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[320, 332, 563, 748]","• Introducing more evaluation metrics and in-depth discussions on MLLMs as evaluation metrics. We introduce evaluation metrics specifically tailored for the newly added categories of numeracy and 3D-spatial re- lationships. Utilizing depth estimation techniques and object detection mechanisms, these metrics offer a ro- bust framework for assessing performance across various compositional domains. Additionally, we analyzed the capabilities of advanced Multimodal Large Language Models (MLLMs), such as ShareGPT4V and GPT-4V, fo- cusing on their effectiveness in addressing compositional challenges. We assess MLLM through comparing human correlation across eight sub-categories and examining their performances of stability over multiple executions.  More comprehensive benchmarks and analysis. As the community of text-to-image generation has been evolving rapidly and many new foundation text-to-image generation models have emerged in the past year, apart from the SD v2 [1] models benchmarked in the confer- ence version, we conduct extensive benchmarks on 11 text-to-image models, including state-of-the-art examples like FLUX.1 [17], SD3 [18], DALLE-3 [19], Pixart-  [20], and SD-XL [21]. Our analysis provides deeper insights into current performances and limitations of both T2I models. We believe such extensive benchmarks and analysis will shed light on future works on improving compositionality of text-to-image generation models and inspire future research in visual content generation.","[-0.00759125 -0.00065374 -0.00018537 ... -0.00046945  0.02256775
 -0.00169373]",9,,text,,,,,10,19,792.0,2,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[49, 68, 560, 181]",,"[ 0.00235367 -0.02848816  0.02853394 ...  0.01174164 -0.03704834
  0.02606201]",0,,table,,TABLE I: Comparison with previous compositional text-to-image benchmarks.,,src/resources/pdf/T2I-CompBench/auto/images/4b1fd588a5e32840ceb1fd8d985062ff03dc65ec91b01694f692eaff7fd1ac63.jpg,9,19,792.0,3,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[131, 202, 216, 213]",II. RELATED WORK,"[ 0.02278137  0.01646423 -0.00738525 ...  0.02731323  0.02423096
 -0.00495148]",1,,text,,,,,9,19,792.0,3,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 222, 300, 411]","Text-to-image generation. Early works [24]–[29] explore different network architectures and loss functions based on generative adversarial networks (GAN) [30]. Recently, dif- fusion models have achieved remarkable success for text-to- image generation [1], [19], [20], [31]–[36]. By training on web-scale data, T2I models such as Stable Diffusion [1], [18], [21], DALL-E 3 [19], MDM [35], and Pixart-  [20], have shown remarkable generative power. Current state-of-the-art models such as Stable Diffusion [1] still struggle to compose multiple objects with attributes and relationships in a complex scene. Some recent works attempt to align text-to-image models with human feedback [37], [38]. RAFT [39] proposes reward-ranked fine-tuning to align text-to-image models with certain metrics. Our proposed GORS approach is a simpler finetuning approach that does not require multiple iterations of sample generation and selection.","[ 0.00048709  0.0011797   0.02679443 ...  0.02006531  0.03363037
 -0.00780106]",2,,text,,,,,9,19,792.0,3,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 414, 300, 615]","Compositional text-to-image generation. Researchers have delved into various aspects of compositionality in text-to- image generation to achieve visually coherent and semantically consistent results [7], [8], [10], [22], [40]–[42]. Previous work focused on concept conjunction and negation [7], attribute binding with colors [8], [9], [43], generative numeracy [44], and spatial relationships between objects [10], [45]. However, those work each target at a sub-problem, and evaluations are conducted in constrained scenarios. Recent compositional studies typically fall into two categories [46]: one relies on cross attention maps for compositional generation [47]– [49], while the other integrates layout as a generation condi- tion [50]–[54]. LMD [44], RPG [55], and RealCompo [56] utilize LLMs or MLLMs to reason out layouts or decompose compositional problems. Our work is the first to introduce a comprehensive benchmark for compositional text-to-image generation.","[-0.01701355 -0.02296448 -0.00788879 ... -0.00099659  0.02633667
 -0.00283432]",3,,text,,,,,9,19,792.0,3,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 618, 300, 747]","Benchmarks for text-to-image generation. Early works evaluate text-to-image on CUB birds [57], Oxford flowers [58], and COCO [59] which are easy with limited diversity. As the text-to-image models become stronger, more challenging benchmarks have been introduced. DrawBench [3] consists of 200 prompts to evaluate counting, compositions, conflicting, and writing skills. DALL-EVAL [60] proposes PaintSkills to evaluate visual reasoning skills, image-text alignment, image quality, and social bias by 7,330 prompts. HE-T2I [61] pro- poses 900 prompts to evaluate counting, shapes, and faces for text-to-image. Several compositional text-to-image bench- marks have also been proposed. Park et al. [43] proposes a benchmark on CUB Birds [57] and Oxford Flowers [58] to evaluate the models’ ability to generate images with object- color and object-shape compositions. ABC-6K and CC500 [8] benchmarks are proposed to evaluate attribute binding for text- to-image models, but they only focused on color attributes. HRS-Bench [23] is a general-purpose benchmark that evalu- ates 13 skills with 45,000 prompts. Compositionality is only one of the 13 evaluated skills which is not extensively studied. We propose the first comprehensive benchmark for open-world compositional text-to-image generation, shown in Table I.","[ 0.00603485 -0.00683594 -0.01237488 ...  0.00181484  0.00930023
 -0.00082111]",4,,text,,,,,9,19,792.0,3,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 202, 563, 332]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",5,,text,,,,,9,19,792.0,3,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 334, 563, 548]","Evaluation metrics for text-to-image generation. Existing metrics for text-to-image generation can be categorized into fidelity assessment, alignment assessment, and LLM-based metrics. Traditional metrics such as Inception Score (IS) [62] and Frechet Inception Distance (FID) [63] are commonly used to evaluate the fidelity of synthesized images. To assess the image-text alignment, text-image matching by CLIP [11] and BLIP2 [14] and text-text similarity by BLIP [13] cap- tioning and CLIP text similarity are commonly used. Some works leverage the strong reasoning abilities of large language models (LLMs) for evaluation [64]–[66]. Besides, human preferences or feedbacks are also included in text-to-image generation evaluation [67]–[74]. More fine-grained metrics are proposed in [75]. However, there was no comprehensive study on how well those evaluation metrics work for composi- tional text-to-image generation. We propose evaluation metrics specifically designed for our benchmark and validate that our proposed metrics align better with human perceptions.","[-0.00220871  0.0032196  -0.01305389 ... -0.00551224  0.01597595
 -0.02424622]",6,,text,,,,,9,19,792.0,3,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[383, 564, 490, 576]",III. T2I-COMPBENCH++,[0.02238464 0.0340271  0.00279999 ... 0.01035309 0.04962158 0.0046463 ],7,,text,,,,,9,19,792.0,3,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 581, 563, 748]","Compositionality of text-to-image models refers to the abil- ity of models to compose different concepts into a complex and coherent scene according to text prompts. To provide a clear definition of the problem and to build our benchmark, we introduce four categories and eight sub-categories of com- positionality, attribute binding (including three sub-categories: color, shape, and texture), object relationships (including three sub-categories: 2D/3D-spatial relationship and non-spatial re- lationship), numeracy, and complex compositions. We generate 1,000 text prompts (700 for training and 300 for testing) for each sub-category, resulting in 8,000 compositional text prompts in total. We take the balance between seen v.s. unseen compositions in the test set, prompts with fixed sentence template v.s. natural prompts, and simple v.s. complex prompts into consideration when constructing the benchmark. The text prompts are generated with either predefined rules or ChatGPT [76], so it is easy to scale up.","[ 0.01089478  0.01399231  0.01235199 ...  0.00485229  0.0146637
 -0.00548172]",8,,text,,,,,9,19,792.0,3,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[47, 53, 299, 183]",,"[-0.0357666  -0.05245972  0.01381683 ...  0.01960754 -0.04568481
  0.02572632]",0,,image,,Fig. 2: Data statistics of T2I-CompBench .,,src/resources/pdf/T2I-CompBench/auto/images/3a6138ef6e88b9cc3163d61637947781efd79e8e92d5975ac0dd9fa2e7bc0957.jpg,15,19,792.0,4,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 226, 299, 261]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",1,,text,,,,,15,19,792.0,4,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 263, 299, 369]","It is important to note that prompts generated by Chat- GPT [76] may include scenarios that do not conform to phys- ical laws. This is an intentional design choice in our dataset to challenge models with both common and uncommon com- positions, including those that break real-world constraints. This consideration is essential for evaluating and advancing the compositional capabilities of text-to-image generative models, ensuring they do not just memorize the dataset but can generalize to unseen combinations.","[ 0.01481628  0.0149765  -0.00472641 ... -0.00284004 -0.01284027
 -0.01618958]",2,,text,,,,,15,19,792.0,4,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[47, 371, 299, 394]","Comparisons between our benchmark and previous bench- marks are shown in Table. I, and data statistics in Figure 2.","[ 1.65100098e-02  4.94766235e-03 -2.45361328e-02 ...  7.77816772e-03
 -1.01165771e-02 -3.30805779e-05]",3,,text,,,,,15,19,792.0,4,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 419, 132, 429]",A. Attribute Binding,"[-0.01734924  0.01403046 -0.02655029 ...  0.00925446  0.00714874
 -0.0413208 ]",4,,text,,,,,15,19,792.0,4,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 437, 300, 675]","A critical challenge for compositional text-to-image gener- ation is attribute binding, where attributes must be associated with corresponding objects in the generated images. We find that models tend to confuse the association between attributes and objects when there are more than one attribute and more than one object in the text prompt. For example, with the text prompt “A room with blue curtains and a yellow chair”, the text-to-image model might generate a room with yellow curtains and a blue chair. We introduce three sub-categories, color, shape, and texture, according to the attribute type, and construct 1000 text prompts for each sub-category. For each sub-category, there are 800 prompts with the fixed sentence template “a   {noun} and a     (e.g., “a red flower and a yellow vase”) and 200 natural prompts without predefined sentence template (e.g., “a room with blue curtains and a yellow chair”). The 300-prompt test set of each sub-category consists of 200 prompts with seen adj-noun compositions (adj-noun compositions appeared in the training set) and 100 prompts with unseen adj-noun compositions (adj- noun compositions not in the training set).",[0.00182915 0.02238464 0.0134201  ... 0.01025391 0.02612305 0.00169563],5,,text,,,,,15,19,792.0,4,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 677, 300, 748]","Color. Color is the most commonly-used attribute for de- scribing objects in images, and current text-to-image models often confuse the colors of different objects. The 1,000 text prompts related to color binding are constructed with 480 prompts from CC500 [8], 200 prompts from COCO [59], and 320 prompts generated by ChatGPT.","[-0.02262878  0.01403046 -0.00374413 ...  0.04064941 -0.00278282
 -0.02471924]",6,,text,,,,,15,19,792.0,4,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[309, 55, 563, 90]","The prompt for ChatGPT is: Please generate prompts in the format of “a adj noun and a adj noun ” by using the color adj. , such as “a green bench and a red car”.","[ 0.0625      0.01345825  0.01968384 ... -0.01681519 -0.0038681
 -0.00152874]",7,,text,,,,,15,19,792.0,4,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 91, 563, 186]","Shape. We define a set of shapes that are commonly used for describing objects in images: long, tall, short, big, small, cubic, cylindrical, pyramidal, round, circular, oval, oblong, spherical, triangular, square, rectangular, conical, pentagonal, teardrop, crescent, and diamond. We provide those shape attributes to ChatGPT and ask ChatGPT to generate prompts by composing those attributes with arbitrary objects, for example, “a rectan- gular clock and a long bench”.","[-0.01641846  0.00026536  0.01853943 ...  0.00044847 -0.00374031
  0.01494598]",8,,text,,,,,15,19,792.0,4,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 188, 563, 329]","For fixed sentence template, the prompt for ChatGPT is: Please generate prompts in the format of “a {adj} {noun} and a adj noun ” by using the shape adj.: long, tall, short, big, small, cubic, cylindrical, pyramidal, round, circu- lar, oval, oblong, spherical, triangular, square, rectangular, conical, pentagonal, teardrop, crescent, and diamond. For natural prompts, the prompt for ChatGPT is: Please generate objects with shape adj. in a natural format by using the shape adj.: long, tall, short, big, small, cubic, cylindrical, pyramidal, round, circular, oval, oblong, spherical, triangular, square, rectangular, conical, pentagonal, teardrop, crescent, and diamond.","[-7.09152222e-03 -6.20484352e-05  2.30102539e-02 ... -1.24130249e-02
  2.84118652e-02  1.15203857e-02]",9,,text,,,,,15,19,792.0,4,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 331, 563, 450]","Texture. Textures are also commonly used to describe the appearance of objects. They can capture the visual properties of objects, such as smoothness, roughness, and granularity. We often use materials to describe the texture, such as wooden, plastic, and rubber. We define several texture attributes and the objects that can be described by each attribute. We generate 800 text prompts by randomly selecting from the possible combinations of two objects each associated with a textural attribute, e.g., “A rubber ball and a plastic bottle”. We also generate 200 natural text prompts by ChatGPT.","[-0.04904175  0.02484131  0.01612854 ...  0.01012421  0.00322342
  0.0174408 ]",10,,text,,,,,15,19,792.0,4,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 451, 563, 569]","We generate 200 natural text prompts by ChatGPT with the following prompt: Please generate objects with texture adj. in a natural format by using the texture adj.: rubber, plastic, metallic, wooden, fabric, fluffy, leather, glass. Besides the ChatGPT-generated text prompts, we also provide the predefined texture attributes and objects that can be described by each texture, as shown in Table II. We generate 800 text prompts by randomly selecting from the possible combinations of two objects each associated with a textural attribute, e.g., “A rubber ball and a plastic bottle”.","[-0.00405502 -0.00314903  0.00095177 ...  0.01238251  0.0284729
  0.0037365 ]",11,,text,,,,,15,19,792.0,4,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 589, 406, 600]",B. Object Relationship,"[-0.01335907 -0.00999451  0.02629089 ... -0.0077095   0.06121826
 -0.03158569]",12,,text,,,,,15,19,792.0,4,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 605, 562, 652]","When composing objects in a complex scene, the relation- ship between objects is a critical factor. We introduce 1,000 text prompts for 2D/3D-spatial relationships and non-spatial relationships, respectively.","[ 0.00359535 -0.01208496  0.02758789 ...  0.01861572  0.06939697
  0.00710297]",13,,text,,,,,15,19,792.0,4,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 653, 563, 748]","2D-spatial relationships. We use “on the side of”, “next to”, “near”, “on the left of”, “on the right of”, “on the bottom of”, and “on the top of” to define 2D-spatial relationships. The two nouns are randomly selected from persons (e.g., man, woman, girl, boy, person, etc.), animals (e.g., cat, dog, horse, rabbit, frog, turtle, giraffe, etc.), and objects (e.g., table, chair, car, bowl, bag, cup, computer, etc.). For spatial relationships including left, right, bottom, and top, we construct contrastive prompts by swapping the two nouns, for example, “a girl on the left of a horse” and “a horse on the left of a girl”.","[ 0.01641846 -0.04718018  0.03936768 ... -0.00862122  0.065979
  0.0111084 ]",14,,text,,,,,15,19,792.0,4,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[50, 68, 558, 185]",,"[ 0.01529694 -0.04141235 -0.01451874 ... -0.01573181  0.00585175
  0.01786804]",0,,table,,TABLE II: Textural attributes and associated objects to construct the attribute-texture prompts.,,src/resources/pdf/T2I-CompBench/auto/images/44eb6a79841b734fbc23f8c147f5834a7f5fe00872018f46c87519f9c22eafc5.jpg,13,19,792.0,5,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 207, 299, 230]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",1,,text,,,,,13,19,792.0,5,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 232, 299, 291]","3D-spatial relationships. To illustrate the 3D-spatial rela- tionships, we use “in front of”, “behind” and “hidden by” to define 3D-spatial relationships. Similar to 2D-spatial relation- ships, we use the same vocabulary to construct the pairs in a prompt, such as “a girl in front of a horse”.","[-0.00255013 -0.04324341  0.00725174 ... -0.00257301  0.0475769
  0.04434204]",2,,text,,,,,13,19,792.0,5,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 295, 300, 366]","Non-spatial relationships. Non-spatial relationships usu- ally describe the interactions between two objects. We prompt ChatGPT to generate text prompts with non-spatial relation- ships (e.g., “watch”, “speak to”, “wear”, “hold”, “have”, “look at”, “talk to”, “play with”, “walk with”, “stand on”, “sit on”, etc.) and arbitrary nouns.","[ 0.00359535 -0.03781128  0.02284241 ...  0.00901031  0.0692749
  0.009552  ]",3,,text,,,,,13,19,792.0,5,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[47, 368, 300, 415]","The prompt for ChatGPT is: Please generate natural prompts that contain subjects and objects by using relationship words such as wear, watch, speak, hold, have, run, look at, talk to, jump, play, walk with, stand on, and sit on.","[ 0.0213623  -0.01860046  0.03649902 ... -0.00265312  0.03512573
  0.0401001 ]",4,,text,,,,,13,19,792.0,5,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 450, 104, 460]",C. Numeracy.,"[-0.04086304  0.04605103 -0.01158905 ...  0.02017212 -0.0324707
 -0.00747299]",5,,text,,,,,13,19,792.0,5,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 207, 420, 218]",D. Complex Compositions,"[-0.00250626  0.00889587  0.02748108 ... -0.02212524  0.01525879
 -0.002388  ]",6,,text,,,,,13,19,792.0,5,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 472, 300, 662]","In the construction of numeracy dataset, we introduce 1,000 prompts, and adopt a tripartite structure based on the quantities of objects. The initial  part of the numeracy contains scenarios involving a singular object, followed by the subsequent  comprising two objects, and the remain- ing   encompassing multiple objects, with the quantities ranging from one to eight. We construct a list of objects, and randomly combine objects and quantities in a standardized format: “number object” (e.g., “one pear and three knives”). In order to incorporate diversity of expressions, each part is characterized by a combination of fixed templates and flexible prompts at a ratio of 4:1. The Fixed templates adhere to the predefined format, while flexible prompts incorporate natural language expressions encountered in our daily lives with multiple ’number object’ included (e.g., “three plates and three pens were on the table”).","[-0.02133179 -0.00428772  0.00706482 ...  0.027771    0.00828552
  0.01228333]",7,,text,,,,,13,19,792.0,5,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 665, 300, 748]","We construct a list of objects by asking ChatGPT: Please generate a list of 150 types of objects and their plurals. Following that, we manually filtered out repeated entries and uncommon objects. For generating flexible prompts, we ask ChatGPT, Convert to natural prompt: You are given number sentences, each containing multiple objects. For each sentence, use the objects in it to create a natural compositional sentence.","[ 0.0092392  -0.00469208  0.00694275 ... -0.02114868 -0.00847626
  0.02655029]",8,,text,,,,,13,19,792.0,5,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 221, 563, 460]","To test text-to-image generation approaches with more nat- ural and challenging compositional prompts in the open world, we introduce 1,000 text prompts with complex compositions of concepts beyond the pre-defined patterns. Regarding the number of objects, we create text prompts with more than two objects, for example, “a room with a blue chair, a black table, and yellow curtains”. In terms of the attributes associated with objects, we can use multiple attributes to describe an object (denoted as multiple attributes, e.g., “a big, green apple and a tall, wooden table”), or leverage different types of attributes in a text prompt (denoted as mixed attributes, e.g., the prompt “a tall tree and a red car” includes both shape and color attributes). We generate 250 text prompts with ChatGPT for each of the four scenarios: two objects with multiple attributes, two objects with mixed attributes, more than two objects with multiple attributes, and more than two objects with mixed attributes. Relationship words can be adopted in each scenario to describe the relationships among two or more objects. For each scenario, we split 175 prompts for the training set and 75 prompts for the test set.","[ 0.01538849 -0.01564026  0.01896667 ...  0.00583267  0.00066805
 -0.00144768]",9,,text,,,,,13,19,792.0,5,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 461, 563, 699]","(1) For 2 objects with mixed attributes, the prompt for Chat- GPT is: Please generate natural compositional phrases, con- taining 2 objects with each object one adj. from color, shape, texture descriptions and spatial (left/right/top/bottom/next to/near/on side of) or non-spatial relationships. (2) For 2 objects with multiple attributes, the prompt for ChatGPT is: Please generate natural compositional phrases, containing 2 objects with several adj. from color, shape, texture descrip- tions and spatial (left/right/top/bottom/next to/near/on side of) or non-spatial relationships. (3) For multiple objects with mixed attributes, the prompt for ChatGPT is: Please generate natural compositional phrases, containing multiple objects (number ) with each one adj. from {color, shape, texture} descriptions and spatial (left/right/top/bottom/next to/near/on side of) non-spatial relationships. (4) For multiple objects with multiple attributes, the prompt for ChatGPT is: Please generate natural compositional phrases, containing multiple objects (number ) with several adj. from {color, shape, texture} descriptions and spatial (left/right/top/bottom/next to/near/on side of) or non-spatial relationships.","[ 0.01010132 -0.05340576  0.02313232 ...  0.00277138  0.04000854
  0.00510788]",10,,text,,,,,13,19,792.0,5,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[379, 710, 495, 721]",IV. EVALUATION METRICS,"[ 0.03109741  0.04959106 -0.0006752  ...  0.01768494 -0.00479126
 -0.02897644]",11,,text,,,,,13,19,792.0,5,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[310, 725, 562, 748]","Evaluating compositional text-to-image generation is chal- lenging as it requires comprehensive and fine-grained cross- modal understanding. Existing evaluation metrics leverage vision-language models trained on large-scale data for eval- uation. CLIPScore [11], [12] calculates the cosine similar- ity between text features and generated-image features ex- tracted by CLIP. Text-text similarity by BLIP-CLIP [9] applies BLIP [13] to generate captions for the generated images, and then calculates the CLIP text-text cosine similarity between the generated captions and text prompts. Those evaluation metrics can measure the coarse text-image similarity, but fails to capture fine-grained text-image correspondences in attribute binding and spatial relationships. To address those limitations, we propose new evaluation metrics for compositional text- to-image generation, shown in Fig. 3. Concretely, we pro- pose disentangled BLIP-VQA for attribute binding evaluation, UniDet-based metric for 2D/3D-spatial relationship and nu- meracy evaluation, and MLLM-based metric for non-spatial relationship and complex prompts. We further investigate the potential and limitations of multimodal large language models such as MiniGPT-4 [77] with Chain-of-Thought [78], ShareGPT4V [15], and GPT-4V [16] for compositionality evaluation.","[ 0.0078125   0.01721191  0.01577759 ... -0.00405884  0.01108551
 -0.01751709]",12,,text,,,,,13,19,792.0,5,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[53, 64, 558, 262]",,"[ 0.01637268 -0.0352478  -0.02574158 ... -0.02197266 -0.00682449
 -0.01905823]",0,,image,,"Fig. 3: Illustration of our proposed evaluation metrics: (a) Disentangled BLIP-VQA for attribute binding evaluation, (b) UniDet for 2D/3D-spatial relationship evaluation, (c) UniDet for numeracy evaluation, and (d) MLLM as a potential unified metric.",,src/resources/pdf/T2I-CompBench/auto/images/3116d771bfacce48406af2eb1ef2e797ebb7f6bf4a6dcbb1c2cee7ba40e70436.jpg,10,19,792.0,6,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 320, 299, 570]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",1,,text,,,,,10,19,792.0,6,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[47, 589, 299, 600]",A. Disentangled BLIP-VQA for Attribute Binding Evaluation,"[ 0.01750183  0.01435852  0.01802063 ... -0.00175381 -0.0064888
 -0.01742554]",2,,text,,,,,10,19,792.0,6,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 605, 300, 700]","We observe that the major limitation of the BLIP-CLIP evaluation is that the BLIP captioning models do not always describe the detailed attributes of each object. For example, the BLIP captioning model might describe an image as “A room with a table, a chair, and curtains”, while the text prompt for generating this image is “A room with yellow curtains and a blue chair”. So explicitly comparing the text-text similarity might cause ambiguity and confusion.","[-0.00880432 -0.05123901  0.01519012 ... -0.00250053 -0.01094818
 -0.00692368]",3,,text,,,,,10,19,792.0,6,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[47, 701, 300, 748]","Therefore, we leverage the visual question answering (VQA) ability of BLIP [13] for evaluating attribute binding. For instance, given the image generated with the text prompt “a green bench and a red car”, we ask two questions separately:","[ 0.00231171  0.01760864  0.0149231  ... -0.01051331 -0.00614548
 -0.01809692]",4,,text,,,,,10,19,792.0,6,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 321, 563, 439]","“a green bench?”, and “a red car?”. By explicitly disentangling the complex text prompt into two independent questions where each question contains only one object-attribute pair, we avoid confusion of BLIP-VQA. The BLIP-VQA model takes the generated image and several questions as input and we take the probability of answering “yes” as the score for a question. We compute the overall score by multiplying the probability of answering “yes” for each question. The proposed disentangled BLIP-VQA is applied to evaluate the attribute binding for color, shape, and texture.","[ 0.00440979  0.00046706  0.01179504 ...  0.01145172 -0.01481628
 -0.02316284]",5,,text,,,,,10,19,792.0,6,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[310, 458, 562, 481]",B. UniDet-based Metric for Spatial Relationships and Numer- acy Evaluation,"[ 0.01133728 -0.00144958 -0.03051758 ... -0.02229309  0.03025818
 -0.01076508]",6,,text,,,,,10,19,792.0,6,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 485, 563, 544]","Many vision-language models exhibit limitations in reason- ing spatial relationships, such as distinguishing between ”left” and ”right,” as well as in numerical counting. Consequently, we propose the use of a detection-based evaluation metric to assess performance in spatial relationships and numeracy.","[-0.03100586  0.00761032  0.03213501 ...  0.00463867  0.01438141
 -0.01319885]",7,,text,,,,,10,19,792.0,6,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 546, 563, 687]","2D-spatial relationships. We first use UniDet [79] to detect objects in the generated image. Then we determine the spatial relationship between two objects by comparing the locations of the centers of the two bounding boxes. Denote the center of the two objects as   and  , respectively. The first object is on the left of the second object if  , and the intersection-over-union (IoU) between the two bounding boxes is below the threshold of 0.1. Other spatial relationships “right”, “top”, and “bottom” are evaluated similarly. We evaluate “next to”, “near”, and “on the side of” by comparing the distances between the centers of two objects with a threshold.","[-0.01570129 -0.04412842 -0.01251221 ...  0.00731659  0.05688477
  0.00013959]",8,,text,,,,,10,19,792.0,6,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 689, 562, 748]","3D-spatial relationships. For 3D-spatial relationships, we leverage depth estimation [80], in conjunction with the detection-based metric. Let   and   represent the mean values of the depth maps corresponding to the two objects. Specifically, if   and the IoU metric exceeds the predetermined threshold of 0.5, it is inferred that the first object is positioned in front of the second object. Other 3D-spatial relationships “behind”, “hidden by” are evaluated similarly.","[-0.01872253 -0.04040527 -0.00634003 ... -0.01777649  0.05627441
  0.00905609]",9,,text,,,,,10,19,792.0,6,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[312, 463, 561, 534]",,"[-0.00537109 -0.00195026 -0.02052307 ...  0.01875305  0.01261139
 -0.00577545]",0,,table,,TABLE III: Prompts details for MLLM evaluation on attribute binding.,,src/resources/pdf/T2I-CompBench/auto/images/52395dc2bc5dc9f835ad28bfa4f459660ca51fff9bd356994be6ccf2d3e27c0c.jpg,14,19,792.0,7,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[312, 583, 561, 648]",,"[-0.01782227 -0.01350403 -0.03204346 ...  0.01834106  0.03219604
 -0.00155449]",1,,table,,,,src/resources/pdf/T2I-CompBench/auto/images/a30694dfee50ed78525276dadc76465ef34b355f65ac3964c75777205a8f6d0e.jpg,14,19,792.0,7,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 55, 299, 102]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",2,,text,,,,,14,19,792.0,7,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 104, 300, 245]","Numeracy. For numeracy, we first extract the names of objects and their corresponding quantities from the prompts. Then, we leverage UniDet to detect objects in the images. This scoring mechanism is designed to consider both objects and their numerical correctness proportionally based on the number of object categories mentioned in the prompt. Let the variable   represent the count of distinct object categories referenced in the given prompt. For every identified object in the image, the assigned score is calculated as  points. Furthermore, another   points are allocated if the generated quantity aligns accurately with the specified category.","[-0.00832367  0.0218811  -0.00649643 ... -0.00699615 -0.01953125
 -0.01901245]",3,,text,,,,,14,19,792.0,7,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 262, 277, 273]",C. 3-in-1 Metric for Complex Compositions Evaluation,"[ 0.06390381  0.05767822 -0.00107384 ... -0.02134705  0.00198174
  0.01948547]",4,,text,,,,,14,19,792.0,7,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 276, 300, 407]","Since different evaluation metrics are designed for evalu- ating different types of compositionality, there is no single metric that works well for all categories. For non-MLLM metric, we empirically find that the Disentangled BLIP-VQA works best for attribute binding evaluation, UniDet-based metric works best for 2D/3D-spatial relationship and numeracy evaluation, and CLIPScore works best for non-spatial relation- ship evaluation. Thus, we design a 3-in-1 evaluation metric which computes the average score of CLIPScore, Disentangled BLIP-VQA, and UniDet, as the evaluation metric for complex compositions.","[ 0.02679443 -0.00753021  0.00619507 ... -0.02163696  0.02406311
 -0.01318359]",5,,text,,,,,14,19,792.0,7,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[49, 423, 195, 434]",D. MLLM-based Evaluation Metric,"[ 0.0362854   0.01156616 -0.02864075 ... -0.00902557  0.00804901
 -0.01117706]",6,,text,,,,,14,19,792.0,7,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 438, 300, 556]","Multimodal Large Language Models enable users to instruct LLMs to analyze user-provided image inputs. By integrat- ing the visual modality, MLLMs enhance the capability of language-only systems, providing them with new interfaces to address a variety of tasks. However, the multimodal abilities of MLLMs regarding the compositional text-to-image generation remain unclear. Within this context, we analyze the abilities of three types of MLLMs, namely MiniGPT-4, ShareGPT4V [15] and GPT-4V [16], focusing on their performance in composi- tional problems.","[-0.00355911  0.01084137 -0.00692749 ...  0.00047684  0.0025425
 -0.00016689]",7,,text,,,,,14,19,792.0,7,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 558, 300, 747]","Evaluation methods of MLLMs. By aligning a pretrained visual encoder with a frozen large language model, multi- modal large language models such as MiniGPT-4 [77] have demonstrated great abilities in vision-language cross-modal understanding. But the current MiniGPT-4 model exhibit lim- itations such as inaccurate understanding of images and hallu- cination issues. ShareGPT4V [15] is a large-scale image-text dataset featuring 1.2 million detailed captions characterized by richness and diversity. GPT-4V, a state-of-the-art MLLM, is developed on the foundation of the state-of-the-art LLM, GPT-4 [81], and trained extensively on a large-scale dataset containing multimodal information [81]. Employing MLLM as an evaluation metric, we submit generated images to the model and assess their alignment with the provided text prompt. This evaluation involves soliciting predictions for the image-text alignment score.","[-0.00768661  0.00195503 -0.03086853 ...  0.01960754  0.00328064
  0.00333977]",8,,text,,,,,14,19,792.0,7,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 55, 563, 258]","Prompt template for MLLM evaluation. We leverage MLLMs as an evaluation metric by feeding the generated images to the model and asking two questions with Chain-of- Thought [78]: “describe the image” and “predict the image- text alignment score”. We detail the prompts used for MLLM evaluation metric. For each sub-category, we ask two questions in sequence: “describe the image” and “predict the image-text alignment score”. Specifically, Table III shows the prompts for evaluating attribute binding (color, shape, texture). Similar to BLIP-VQA, for each noun phrase in a prompt, we request a number equivalent to the noun phrase and perform a multipli- cation. Table IV, Table VI, Table V, and Table VII demonstrate the prompt templates used for 2D/3D-spatial relationships, non-spatial relationships, numeracy, and complex composi- tions, respectively*. Due to the robust capabilities of GPT-4V and limited quota constraints, we have omitted “describe the image” for prompt of GPT-4V.","[-0.00485229 -0.02584839 -0.00164509 ...  0.00845337  0.00263405
 -0.01379395]",9,,text,,,,,14,19,792.0,7,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 259, 563, 425]","For MLLM without Chain-of-Thought in addressing specific compositional problems, we utilize predefined prompts that prompt MLLM to provide a score ranging from 0 to 100. For attribute binding, we focus on the presence of specific objects and their corresponding attributes. We utilize a prompt template such as “Is there object in the image? Give a score from 0 to 100. If {object} is not present or if {object} is not color/shape/texture description , give a lower score.” We leverage this question for each noun phrase in the text and compute the average score. For the spatial relationships, non- spatial relationships, and complex compositions, we employ a more general prompt template such as “Rate the overall alignment between the image and the text prompt {prompt}. Give a score from 0 to 100.”.","[ 0.01168823 -0.02426147 -0.0021801  ... -0.00041866  0.03074646
 -0.00597   ]",10,,text,,,,,14,19,792.0,7,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 552, 562, 575]",TABLE IV: Prompts details for MLLM evaluation on 2D/3D- spatial relationship.,"[-0.02351379 -0.04077148  0.00209999 ... -0.00211143  0.03713989
  0.01766968]",11,,text,,,,,14,19,792.0,7,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[332, 670, 542, 693]",V. BOOSTING COMPOSITIONAL TEXT-TO-IMAGE GENERATION WITH GORS,"[ 0.01153564  0.04211426  0.01125336 ...  0.00560379  0.03500366
 -0.04071045]",12,,text,,,,,14,19,792.0,7,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 697, 562, 720]","We introduce a simple but effective approach, Genera- tive mOdel finetuning with Reward-driven Sample selec- tion (GORS), to improve the compositional ability of pre- trained text-to-image models. Our approach finetunes a pre- trained text-to-image model such as Stable Diffusion [1] with generated images that highly align with the compositional prompts, where the fine-tuning loss is weighted by the reward which is defined as the alignment score between compositional prompts and generated images.","[ 0.02783203  0.04580688  0.00041986 ...  0.00227165  0.03851318
 -0.03607178]",13,,text,,,,,14,19,792.0,7,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 81, 299, 147]",,"[-0.01631165  0.00835419 -0.00654984 ... -0.00050068  0.0087738
  0.00532913]",0,,table,,TABLE V: Prompts details for MLLM evaluation on non- spatial relationship.,,src/resources/pdf/T2I-CompBench/auto/images/2d33586b7e2c8fb903e8ee856520055c99bc1570b3eddc9adb26ad1807427894.jpg,15,19,792.0,8,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 187, 299, 251]",,"[-0.01289368  0.00841522 -0.02516174 ...  0.02243042  0.03164673
  0.00832367]",1,,table,,TABLE VI: Prompts details for MLLM evaluation on numer- acy.,,src/resources/pdf/T2I-CompBench/auto/images/d96043fedcddeea5fea8179241ae3af6da68a1dc5229344760afb59ce5624f34.jpg,15,19,792.0,8,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[312, 81, 562, 145]",,"[-0.01293182  0.00439072 -0.02622986 ... -0.00469589  0.00494003
 -0.01313782]",2,,table,,TABLE VII: Prompts details for MLLM evaluation on com- plex compositions.,,src/resources/pdf/T2I-CompBench/auto/images/e576ac6d681df30e2b684abc3f37370943f560d2eb6f02d8ae139c4a2a4b7567.jpg,15,19,792.0,8,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 273, 300, 357]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",3,,text,,,,,15,19,792.0,8,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 358, 300, 488]","Specifically, given the text-to-image model   and a set of text prompts  , we first generate   images for each text prompt, resulting in   generated images . Text-image alignment scores are predicted as rewards. We select the generated images whose rewards are higher than a threshold to fine-tune the text-to-image model. The selected set of samples are denoted as . During fine-tuning, we weight the loss with the reward of each sample. Generated images that align with the compo- sitional prompt better are assigned higher loss weights, and vice versa. The loss function for fine-tuning is","[ 0.02450562  0.0262146  -0.01515961 ... -0.00240898  0.0141983
 -0.03041077]",4,,text,,,,,15,19,792.0,8,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[83, 494, 263, 515]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",5,,text,,,,,15,19,792.0,8,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[47, 520, 300, 556]","where   is the triplet of the image, text prompt, and reward, and   represents the latent features of   at timestep . We adopt LoRA [82] for efficient finetuning.","[ 0.06488037  0.02719116  0.02418518 ... -0.00045133  0.00811005
 -0.02505493]",6,,text,,,,,15,19,792.0,8,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[133, 572, 214, 583]",VI. EXPERIMENTS,"[-0.01098633  0.08270264 -0.00298119 ...  0.00688553 -0.01853943
  0.01109314]",7,,text,,,,,15,19,792.0,8,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 589, 142, 600]",A. Experimental Setup,"[ 0.00704956  0.07507324  0.00015724 ...  0.00453186  0.00302505
 -0.00494003]",8,,text,,,,,15,19,792.0,8,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 605, 300, 711]","Evaluated models. We evaluate the performance of 11 text-to-image models on T2I-CompBench : Stable Diffusion , Stable Diffusion   [1], Composable Diffusion [7], Structured Diffusion [8], Attend-and-Excite [9], and GORS. We further conduct evaluations on 5 state-of-the-art text-to- image models: PixArt-  [20], Stable Diffusion XL [21], DALL-  [19], Stable Diffusion 3 [18] and FLUX.1 [17]. GORS is our proposed approach which finetunes Stable Diffusion v2 with selected samples and their rewards.","[ 0.02125549  0.02548218 -0.01290131 ...  0.02999878  0.0249939
 -0.02655029]",9,,text,,,,,15,19,792.0,8,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 713, 300, 748]","Stable Diffusion  and Stable Diffusion v2 [1] are text- to-image models trained on large amount of image-text pairs. Composable Diffusion [7] is designed for conjunction and negation of concepts for pretrained diffusion models. Struc- tured Diffusion [8] and Attend-and-Excite [9] are designed for attribute binding for pretrained diffusion models. We re- implement those approaches on Stable Diffusion v2 to enable fair comparisons. PixArt-  [20] is a Transformer-based T2I diffusion model, boasting competitive image generation qual- ity comparable to state-of-the-art image generators with low training costs. We test PixArt-  fine-tuned on GORS, denoted as PixArt- -ft. Stable Diffusion XL 1.0 [21] building upon previous iterations of Stable Diffusion models, represents a powerful text-to-image generation framework. DALLE-3 [19] is a new text-to-image generation system, enhancing the align- ment between generated images and provided text description. Stable Diffusion 3 [18] combines a diffusion transformer architecture and flow matching. FLUX.1 [schnell] [17] is a 12 billion parameter rectified flow transformer.","[ 0.00309181  0.00579071 -0.00316238 ...  0.03167725  0.02702332
 -0.04550171]",10,,text,,,,,15,19,792.0,8,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 171, 563, 361]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",11,,text,,,,,15,19,792.0,8,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 363, 563, 578]","To avoid the bias from selecting samples by evaluation metrics as reward [43], [83], we introduce new reward models which are different from our proposed evaluation metrics, denoted as GORS-unbiased. Specifically, we adopt Grounded- SAM [84] as the reward model for the attribute binding category. We extract the segmentation masks of attributes and their associated nouns separately with Grounded-SAM, and use the Intersection-over-Union (IoU) between the attribute masks and the noun masks together with the grounding mask confidence to represent the attribute binding performance. We apply GLIP-based [85] selection method for 2D/3D-spatial relationships and numeracy. For non-spatial relationships, we adopt BLIP [13] to generate image captions and CLIP [11], [12] to measure the text-text similarity between the generated captions and the input text prompts. For complex composi- tions, we integrate the 3 aforementioned reward models as the total reward. Those sample selection models are different from the models used as evaluation metrics.","[ 0.0369873  -0.01774597 -0.00112438 ...  0.01263428  0.05105591
 -0.0189209 ]",12,,text,,,,,15,19,792.0,8,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 580, 563, 723]","Implementation details. We employ pre-trained models for our proposed evaluation metrics. For BLIP-VQA, we utilize the BLIP w/ ViT-B and CapFilt-L [13] pretrained on image-text pairs and fine-tuned on VQA. We employ the UniDet [79] model trained on 4 large-scale detection datasets (COCO [59], Objects365 [86], OpenImages [87], and Mapil- lary [88]). For CLIPScore, we use the “ViT-B/32” pretrained CLIP model [11], [12]. For MiniGPT4-CoT, we utilize the Vicuna 13B of MiniGPT4 [77] variant with a temperature setting of 0.7 and a beam size of 1. For ShareGPT4V [15] and GPT-4V [16], we use the default parameters, with temperature setting of 0.2 and 1, respectively.","[ 0.01551056 -0.01016235  0.00396729 ... -0.0147171  -0.03083801
 -0.01363373]",13,,text,,,,,15,19,792.0,8,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 725, 563, 748]","We implement our proposed GORS upon the codebase of diffusers [89] (Apache License), and finetune the self-attention layers of the CLIP text encoder and the attention layers of U-net using LoRA [82]. The model is trained by AdamW optimizer [90] with  ,  ,  , and weight decay of 0.01. The batch size is 5. The model is trained on 8 32GB NVIDIA v100 GPUs.","[ 0.02684021  0.03092957  0.00023663 ...  0.0289917   0.00837708
 -0.00967407]",14,,text,,,,,15,19,792.0,8,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[47, 92, 562, 159]",,"[-0.0025444   0.00667191 -0.00094128 ...  0.01351929  0.03948975
 -0.02770996]",0,,table,,"TABLE VIII: Comparisons of evaluation metrics on attribute binding (color and shape), with scores unnormalized. Bold stands for the best score across 7 models in T2I-Compbench [22]. Blue represents the proposed metric for the category, and green stands for the human evaluation, applicable to the following Table IX, X and XI.",,src/resources/pdf/T2I-CompBench/auto/images/d7d54f2063260ec7a4c5c50c864c2a127f83a5f597679eb08319d6747149248b.jpg,14,19,792.0,9,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 180, 300, 239]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",1,,text,,,,,14,19,792.0,9,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 268, 140, 279]",B. Evaluation Metrics,"[ 0.02281189  0.02975464 -0.02030945 ... -0.01045227 -0.01352692
 -0.04769897]",2,,text,,,,,14,19,792.0,9,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[47, 289, 299, 335]","We generate 10 images for each text prompt in T2I- CompBench for automatic evaluation. To ensure a fair com- parison, the images are generated using the fixed seed across all models†.","[-0.01705933 -0.00190735  0.02090454 ...  0.01296997  0.01457214
 -0.02839661]",3,,text,,,,,14,19,792.0,9,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 338, 300, 444]","Previous metrics. CLIPScore [11], [12] (denoted as CLIP) calculates the cosine similarity between text features and generated-image features extracted by CLIP. BLIP-CLIP [9] (denoted as B-CLIP) applies BLIP [13] to generate captions for the generated images, and then calculates the CLIP text- text cosine similarity between the generated captions and text prompts. BLIP-VQA-naive (denoted as B-VQA-n) applies BLIP VQA to ask a single question (e.g., a green bench and a red car?) with the whole prompt.","[ 0.04766846 -0.02046204 -0.02297974 ... -0.04394531 -0.00930023
 -0.03652954]",4,,text,,,,,14,19,792.0,9,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 447, 299, 494]",Our proposed metrics. Disentangled BLIP-VQA (denoted as B-VQA) is our proposed evaluation metric for attribute binding. UniDet is our proposed metric for 2D/3D-spatial relationships and numeracy evaluation metric.,"[ 0.00694275  0.00260544 -0.02062988 ... -0.01327515  0.00136375
 -0.04852295]",5,,text,,,,,14,19,792.0,9,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 496, 300, 602]","For evaluating MLLM to serve as a metric, we assess three models, and propose MLLM-based metric for non-spatial relationship and complex compositions: MiniGPT4 (denoted as mGPT), ShareGPT4V (denoted as Share), and GPT-4V. Due to the restricted quota for GPT-4V, we conducted evaluations on one-fifth of the images of T2I-CompBench  (i.e., 600 images for each category). To boost capability of MLLM, we apply Chain-of-Thought [78] (denoted as -CoT) for mGPT and ShareGPT4V.","[ 0.03527832 -0.02696228  0.00812531 ...  0.01341248  0.0322876
  0.0279541 ]",6,,text,,,,,14,19,792.0,9,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 605, 300, 747]","Human evaluation. For human evaluation of each sub- category, we randomly select 25 prompts and generate 2 images per prompt, resulting in 300 images generated with 200 prompts per model in total. The testing set includes 300 prompts for each sub-category, resulting in 2,400 prompts in total. The prompt sampling rate for human evaluation is . We utilize Amazon Mechanical Turk and ask three workers to score each generated-image-text pair independently based on the image-text alignment. The worker can choose a score from , as shown in Figure 4-6. We normalize the scores by dividing them by 5. We then compute the average score across all images and all workers.","[ 0.01519775  0.00410843 -0.00718307 ...  0.03137207 -0.00542831
 -0.01456451]",7,,text,,,,,14,19,792.0,9,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 180, 524, 191]",C. Comparison across Different Evaluation Metrics,"[ 0.04351807  0.00260353 -0.00181007 ... -0.00954437  0.01229095
 -0.00101376]",8,,text,,,,,14,19,792.0,9,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 198, 563, 281]","Comparions of different evaluation metrics and human evaluation. We show the results of different evaluation metrics in Table VIII-XI. Previous evaluation metrics, CLIP and B- CLIP, predict similar scores across different models and cannot reflect the differences between models. Our proposed metrics (highlighted in blue column) show a similar trend with human evaluation (highlighted in green column).","[ 0.0206604   0.01515961 -0.00465393 ... -0.01777649  0.02351379
 -0.03540039]",9,,text,,,,,14,19,792.0,9,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 284, 563, 460]","Human correlation of the evaluation metrics. We calcu- late Kendall’s tau  and Spearman’s rho  to evaluate the ranking correlation between automatic evaluation and human evaluation. The human correlation results are illustrated in Table XII. The results verify the effectiveness of our pro- posed evaluation metrics, BLIP-VQA for attribute binding, UniDet-based metric for spatial relationships and numeracy. For MLLM evaluation, MiniGPT4 does not perform well in terms of correlation with human perception. Share-CoT and GPT-4V excel in terms of non-spatial and complex categories, followed by CLIPScore and 3-in-1 evaluation metrics. In shape attribute, they exhibit performances that are comparable to non-MLLM metrics. In color, texture, 2D/3D-spatial and numeracy categories, their performances are slightly lower than non-MLLM metrics.","[ 0.00958252  0.00277138 -0.02423096 ...  0.00837708  0.02200317
 -0.01111603]",10,,text,,,,,14,19,792.0,9,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 463, 563, 629]","Conclusion on the evaluation metrics. Based on the human correlation of different evaluation metrics, we draw the conclusion for the optimal evaluation metrics for each category or sub-category, and those optimal metrics are used to benchmark different text-to-image generation methods in the following subsections. (1) For attribute binding, the best evalu- ation metric is BLIP-VQA. (2) For 2D spatial relationships, 3D spatial relationships, and numeracy, the UniDet-based metric performs best. (3) For non-spatial relationships, the best metric is GPT-4V, the second-best metric is Share-CoT (ShareGPT4V with Chain-of-Thought), and the best non-MLLM metric is CLIP. (4) For complex compositions, the best metric is GPT- 4V, the second-best metric is Share-CoT (ShareGPT4V with Chain-of-Thought), and the best non-MLLM metric is 3-in-1.","[ 0.00720215 -0.0243988   0.00210381 ... -0.01319122  0.04492188
 -0.02996826]",11,,text,,,,,14,19,792.0,9,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 653, 512, 664]",D. Discussion about MLLMs as a unified metric,"[ 0.00920105 -0.0055542  -0.02886963 ... -0.02235413  0.01823425
 -0.01200867]",12,,text,,,,,14,19,792.0,9,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 671, 563, 705]","Comparisons among different MLLMs. We compare the human correlation among MiniGPT4, ShareGPT4V and GPT-4V. MiniGPT4 shows inadequate correlation with human","[ 0.01779175  0.04547119 -0.01669312 ...  0.00256348  0.00646591
 -0.01898193]",13,,text,,,,,14,19,792.0,9,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[89, 75, 173, 160]",,"[-0.06384277 -0.00461578  0.01896667 ...  0.01959229 -0.0307312
 -0.02088928]",0,,image,,Text Prompt: a brown backpack and a blue cow Image:,,src/resources/pdf/T2I-CompBench/auto/images/4dcd09a8f401d5f760c858cbbabf8f451f18d97f01d6158b0ef0000671fcdefd.jpg,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[257, 75, 342, 160]",,"[-0.02438354 -0.01107025 -0.01554871 ... -0.00849915  0.00257111
 -0.01190948]",1,,image,,Text Prompt: an oval sink and a rectangular mirror Image:,,src/resources/pdf/T2I-CompBench/auto/images/bdc0f714d35e05f7b712e8df49b3cd560b41ba3365616130d327f0a5d6e39579.jpg,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[425, 78, 509, 162]",,"[ 0.01574707  0.01898193  0.00043106 ... -0.02572632  0.02635193
 -0.00531387]",2,,image,,Rate the matching degree of objects' texture attributes between the Image and Text Prompt:,,src/resources/pdf/T2I-CompBench/auto/images/094863bb25f310266297ad4e9f562135fed61eff3682247eb2c18637f81c9a2f.jpg,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[89, 258, 173, 342]",,"[ 0.01367188 -0.02384949 -0.01271057 ...  0.02348328 -0.02593994
  0.03955078]",3,,image,,Rate the matching degree of objects' color attributes between the Image and Text Prompt:,,src/resources/pdf/T2I-CompBench/auto/images/44f4c91b837b85ab27526768a2bc1e69cfa6e93ba51d43a81933399a78de96b4.jpg,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[257, 258, 340, 341]",,"[-0.02786255  0.00348854  0.02070618 ... -0.00856018 -0.04217529
 -0.03887939]",4,,image,,Text Prompt: a vase on the right of a cat Image:,,src/resources/pdf/T2I-CompBench/auto/images/2c08284b750001805edaf1ceb330e5f8516b08922323996c8796f7a0c7e96697.jpg,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[150, 434, 232, 529]",,"[-0.04660034 -0.02844238  0.01556396 ... -0.00190258 -0.01657104
 -0.00670624]",5,,image,,Text Prompt: A boat is sailing on a lake Image:,,src/resources/pdf/T2I-CompBench/auto/images/ce05b2f76dc48610a53059eaa0f61d86825e7e9ab17f9f6f955b7c1ac6cbc6f2.jpg,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[424, 258, 509, 342]",,"[ 0.00276947  0.02090454 -0.02206421 ...  0.00207901 -0.00197601
 -0.00216484]",6,,image,,"Fig. 5: AMT interface for the image-text alignment evaluation on Pl2easDe ch-ecsk tpheaImtaigeaaln rTextlPraotmiptocarnefuslly!hWie apntisci,p tenthoat itnta-kes apt tahetvieray le srt aeboluta1t0ionships, and comple compositions.",,src/resources/pdf/T2I-CompBench/auto/images/cfe641e45e15c23ad452c38ff2f9433a5fb4dd5d02d22a3aff7c5f79ca7c80fd.jpg,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[305, 435, 385, 529]",,"[-0.02467346 -0.02076721 -0.0224762  ... -0.02008057 -0.0120697
  0.01690674]",7,,image,,Rate the matching degree of objects' generative numeracy between the Image and Text Prompt:,,src/resources/pdf/T2I-CompBench/auto/images/92c455118569bafc2f8a9b0c489222a69d2a34e3df48976d5c4874b2c69ec565.jpg,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 177, 174, 206]","5 - Perfect: all/both objects match their attributes in the text prompt  4 - Good: basic level of alignment  3 - Not okay: merely aligned with the text prompt  P2le-asBeacdh:enckotthaleigInmeadgperoapnedrlTyewxtithPrtohemtpetxctarperfoulmlyp!t  l1ea-sPt oaobro:utal1m0osetciornredlsetvoanptertfo rthmeatteaxstkp(rionm""epat","[ 0.03030396 -0.01593018 -0.0352478  ...  0.00487518 -0.00169754
 -0.01084137]",8,,text,,,,,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[216, 177, 342, 206]",5 - Perfect: all/both objects match their attributes in the text prompt  4 - Good: basic level of alignment  3 - Not okay: merely aligned with the text prompt  2 - Bad: not aligned properly with the text prompt  1 - Poor: almost irrelevant to the text prompt  5 - Perfect: all/both objects match their attributes in the text prompt  4 - Good: basic level of alignment  3 - Not okay: merely aligned with the text prompt  2 - Bad: not aligned properly with the text prompt  1 - Poor: almost irrelevant to the text prompt,"[ 0.02319336 -0.02131653  0.00057507 ... -0.00596237  0.01545715
 -0.00548553]",9,,text,,,,,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[384, 180, 510, 209]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",10,,text,,,,,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[88, 214, 524, 226]","sulFtsiangd.r ec4t a:l asAks frMom wTo irs,nwthoeorbvfioauslcy deid oft follro thehe i TmextaProgmpet:-Atbeoaxt its aililniggonna lamk ent evaluation on attributWe lsbo ichenckdthie rnesgult a(ndcreojelctoallrt,a sfrohmawoprkers, htoeobxviotusulyrdied n)o.t","[ 0.02217102 -0.04220581  0.0241394  ... -0.02478027 -0.0435791
 -0.02708435]",11,,text,,,,,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[49, 347, 201, 352]",Rate the matching degree of objects' spatial layout between the Image and Text Prompt:,"[-0.01629639 -0.01386261 -0.02154541 ...  0.00207138  0.04766846
  0.00246811]",12,,text,,,,,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 360, 157, 388]",5 - Perfect: correct spatial layout  4 - Good: basically correct spatial layout  3 - Not okay: spatial layout not aligned properly with the text  2 - Bad: image not aligned properly with the text  1 - Poor: image almost irrelevant to the text prompt  5 - Perfect: accurate alignment  4 - Good: basic level of alignment  3 - Not okay: action relationship not correct.  2 - Bad: image not aligned properly with the text  1 - Poor: image almost irrelevant to the text   5 - Perfect: accurate alignment  4 - Good: basic level of alignment  3 - Not okay: ignored key parts  2 - Bad: image not aligned properly with the text  1 - Poor: image almost irrelevant to the text,"[-0.00808716 -0.00174999 -0.00970459 ... -0.00909424  0.02662659
 -0.01799011]",13,,text,,,,,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[216, 360, 304, 388]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",14,,text,,,,,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[384, 360, 472, 389]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",15,,text,,,,,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[150, 533, 298, 538]",Rate the matching degree of objects' 3D spatial layout between the Image and Text Prompt:,"[-0.01698303 -0.00865936 -0.01773071 ...  0.02130127  0.04803467
  0.01991272]",16,,text,,,,,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[150, 546, 255, 573]",5 - Perfect: correct spatial layout  4 - Good: basically correct spatial layout  3 - Not okay: spatial layout not aligned properly with the text  2 - Bad: image not aligned properly with the text  1 - Poor: image almost irrelevant to the text prompt  5 - Perfect: accurate alignment  4 - Good: basic level of alignment  3 - Not okay: ignored key parts  2 - Bad: image not aligned properly with the text  1 - Poor: image almost irrelevant to the text,"[-0.00647354 -0.00723267 -0.01495361 ... -0.00987244  0.02084351
 -0.01480865]",17,,text,,,,,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[305, 546, 390, 573]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",18,,text,,,,,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[65, 585, 543, 597]","Fig. 6: AMT interface for the image-text alignment evaluation on 3D-spatial relationships, and generative numeracy.","[-0.00996399 -0.01361084 -0.0107193  ... -0.00958252  0.02568054
 -0.03442383]",19,,text,,,,,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 619, 300, 703]","perception‡. ShareGPT4V and GPT-4V excel in terms of non- spatial and complex categories. In color, texture, 2D/3D-spatial and numeracy categories, their performances are slightly lower than non-MLLM metrics. ShareGPT4V ranks second in non- spatial relationship and complex, ranging between first to third place in attribute categories, third in 2D-spatial, third (in terms of  ) and second (in terms of  ) in 3D-spatial, second in numeracy. GPT-4V ranks first in non-spatial relationship and complex, second in color attribute, third in shape and texture attributes, second in 2D-spatial, second (in terms of ) and third (in terms of ) in 3D-spatial, third in numeracy.","[-0.01818848 -0.01006317  0.00904083 ...  0.009758    0.02400208
  0.01190186]",20,,text,,,,,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 619, 563, 667]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",21,,text,,,,,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 677, 563, 748]","Comparisons of the effectiveness of Chain-of-Thought for MLLMs. Utilizing Chain-of-thought [78] to stimulate MLLM’s evaluation ability, we conduct the comparisons on MiniGPT4 and ShareGPT4V. With CoT, both MiniGPT4 and ShareGPT4V demonstrate improvements across most cate- gories in terms of human correlation, highlighting the signifi-","[ 0.05792236  0.04626465 -0.00616074 ...  0.00217056  0.0287323
 -0.00061131]",22,,text,,,,,23,19,792.0,10,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[47, 69, 563, 139]",,"[-0.0171814   0.0151062  -0.01808167 ...  0.02696228  0.04824829
 -0.01821899]",0,,table,,TABLE IX: Comparisons of evaluation metrics on attribute binding (texture) and 2D-spatial relationship.,,src/resources/pdf/T2I-CompBench/auto/images/2e0b89fd90dd5ff0b3cd0e9c7627b3b65269a1e70837523f75315e24c8432efc.jpg,13,19,792.0,11,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 171, 561, 246]",,"[-0.00672913  0.00114441 -0.01799011 ...  0.03909302  0.04458618
 -0.01050568]",1,,table,,TABLE X: Comparisons of evaluation metrics on 3D-spatial relationship and numeracy task.,,src/resources/pdf/T2I-CompBench/auto/images/c988165c6e83d334a0325d260ef59ecc98f6316f11552dd45df0cc1f8bbd13f0.jpg,13,19,792.0,11,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[49, 272, 561, 351]",,"[ 0.01741028  0.00421524 -0.00437164 ...  0.05181885  0.03686523
 -0.0249939 ]",2,,table,,TABLE XI: Comparisons of evaluation metrics on the non-spatial relationship and complex compositions.,,src/resources/pdf/T2I-CompBench/auto/images/3136b047c31550276b30c4e2a3e290ff017675f7f03cd6ba652ed4d5a82c9359.jpg,13,19,792.0,11,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[46, 406, 563, 527]",,"[ 2.94799805e-02 -2.22473145e-02  2.19879150e-02 ... -1.69982910e-02
  2.34832764e-02 -4.15444374e-05]",3,,table,,,,src/resources/pdf/T2I-CompBench/auto/images/5bdf35f71f032a2e950b7d1f6e70303564e1701dde15016f2174956f952e0e79.jpg,13,19,792.0,11,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[47, 360, 564, 397]",TABLE XII: The correlation between automatic evaluation metrics and human evaluation. Our proposed metrics demonstrate a significant improvement over existing metrics in terms of Kendall’s  and Spearmanr’s . Bold stands for the highest correlation score across all non-MLLM metrics. Red indicates the highest correlation score with MLLM metrics included.,"[ 0.04620361  0.02210999 -0.0186615  ... -0.00354385  0.01965332
 -0.00013494]",4,,text,,,,,13,19,792.0,11,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 549, 168, 560]",cance of effective prompting.,"[ 0.03173828  0.10015869  0.00953674 ...  0.00208855  0.01267242
 -0.03250122]",5,,text,,,,,13,19,792.0,11,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 565, 300, 636]","We benchmark mGPT without Chain-of-Thought in Ta- ble XIV, which shows the additional results of benchmarking on T2I-CompBench  of 6 models with MiniGPT-4 without Chain-of-Thought. Results indicate that MiniGPT-4 evaluation without Chain-of-Thought does not strictly align with human evaluation results.","[ 0.06088257 -0.02200317  0.02183533 ...  0.01293182  0.05114746
 -0.04299927]",6,,text,,,,,13,19,792.0,11,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 641, 300, 748]","Discussion about the stability of MLLMs’ results. We test the stability of the MLLM metric by conducting multiple executions on a consistent set of images. Specifically, we employ ShareGPT4V and GPT-4V to analyze 50 images from Stable v2 color category, repeating the process five times. Default parameters were used, with a temperature of 1.0 for GPT-4V and 0.2 for ShareGPT4V. As shown in Table XVIII, our analysis consistently produces results, with variations remaining within 0.032 across the five executions for GPT-4V and 0.0273 for ShareGPT4V. The average standard deviations are 7.3235 for GPT-4V and 6.8741 for ShareGPT4V.","[-0.01898193 -0.02639771 -0.03134155 ...  0.02178955 -0.00985718
  0.01080322]",7,,text,,,,,13,19,792.0,11,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 548, 563, 571]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",8,,text,,,,,13,19,792.0,11,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 573, 563, 655]","Limitations of MLLMs as a metric. Share-CoT and GPT- 4V have limits despite their strong performance as evaluation metrics. They do not adhere to the prompts of grading guide- lines very well. Our empirical observations show that Share- CoT tends to give less diverse ratings, regardless of the prompt provided. GPT-4V can comprehend the evaluation prompts, but it is less able to convert into exact grades.","[ 0.02619934  0.00409317  0.01596069 ... -0.0191803  -0.00341415
  0.00754166]",9,,text,,,,,13,19,792.0,11,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[312, 673, 473, 685]",E. Benchmarking on Different Methods,"[ 0.00818634 -0.0008359  -0.02102661 ...  0.01407623  0.00627136
 -0.02130127]",10,,text,,,,,13,19,792.0,11,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 689, 562, 724]","The quantitative benchmarking results are reported in Ta- ble VIII-XI and XIII. Qualitative results are shown in appendix Figure 7, Figure 8 and Figure 9.","[ 0.02403259 -0.02458191 -0.02189636 ...  0.01583862  0.01409912
 -0.02877808]",11,,text,,,,,13,19,792.0,11,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 725, 563, 748]","Comparisons across text-to-image models. (1) Stable Diffusion v2 consistently outperforms Stable Diffusion v1-4 in all types of compositional prompts and evaluation metrics. (2) Although Structured Diffusion built upon Stable Diffusion v1- 4 shows great performance improvement in attribute binding as reported in Feng et al. [8], Structured Diffusion built upon Stable Diffusion v2 only brings slight performance gain upon Stable Diffusion v2. It indicates that boosting the performance upon a better baseline of Stable Diffusion v2 is more challeng- ing. (3) Composable Diffusion built upon Stable Diffusion v2 does not work well. A similar phenomenon was also observed in previous work [9] that Composable Diffusion often gener- ates images containing a mixture of the subjects. In addition, Composable Diffusion was designed for concept conjunctions and negations so it is reasonable that it does not perform well in other compositional scenarios. (4) Attend-and-Excite built upon Stable Diffusion v2 improves the performance in attribute binding. (5) Previous methods Composable Diffu- sion [7], Structure Diffusion [8] and Attend-and-Excite [9] are designed for concept conjunction or attribute binding, so they do not result in significant improvements in object relationships. (6) Our proposed approach, GORS, outperforms previous approaches across all types of compositional prompts, as demonstrated by the automatic evaluation, human evalua- tion, and qualitative results. The evaluation results of GORS- unbiased and GORS significantly exceed the baseline Stable v2. Besides, the performances of GORS-unbiased indicate that our proposed approach is insensitive to the reward model used for selecting samples, and that the proposed approach works well as long as high-quality samples are selected. (7) Recent text-to-image models, Stable Diffusion XL [21], Pixart- -ft [20], DALLE 3 [19], Stable Diffusion 3 [18] and FLUX.1 [schnell] [17] improves across all categories compared to previous methods. Notably, DALLE 3 and SD3 achieves the state-of-the-art (SOTA) performance by a significant margin in almost all categories.","[ 0.00613403 -0.00341225 -0.01730347 ...  0.0168457   0.04483032
 -0.04217529]",12,,text,,,,,13,19,792.0,11,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[46, 81, 562, 209]",,"[ 0.00476074 -0.01937866 -0.00791931 ...  0.02603149  0.01676941
 -0.00310326]",0,,table,,TABLE XIII: Benchmarking on all categories with proposed metrics. Bold stands for the best score across 7 models in T2I- Compbench [22]. Red indicates the best score across 10 models in T2I-CompBench .,,src/resources/pdf/T2I-CompBench/auto/images/5eb13419dea4e9ea860336fba6ca7ecc59070889199ac3a0bac3b0468093b9be.jpg,8,19,792.0,12,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[47, 236, 561, 342]",,"[-0.00052071 -0.01042938 -0.00404358 ...  0.01695251  0.02935791
 -0.00508881]",1,,table,,TABLE XIV: mGPT benchmarking on 6 sub-categories in T2I-CompBench++.,,src/resources/pdf/T2I-CompBench/auto/images/5727e6300d593f8c3ab113aaf179164f7e1d3e3ffd724d8be8060dd3ea9158b5.jpg,8,19,792.0,12,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[314, 591, 560, 699]",,"[-0.0096283  -0.01669312 -0.02526855 ... -0.01542664 -0.00260735
 -0.01519012]",2,,table,,"TABLE XV: Performances of our model on attribute binding (color, shape, and texture) for seen and unseen sets.",,src/resources/pdf/T2I-CompBench/auto/images/f27dc91fda1ce2dbc67364c338fa8c3e82163a129f63659541a02385834c49e8.jpg,8,19,792.0,12,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 359, 300, 746]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",3,,text,,,,,8,19,792.0,12,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 363, 562, 387]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",4,,text,,,,,8,19,792.0,12,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 388, 563, 447]","Comparisons across compositionality categories. Accord- ing to the human evaluation results, spatial relationship is the most challenging sub-category for text-to-image models, and attribute binding (shape) is also challenging. Non-spatial relationship is the easiest sub-category.","[ 0.01007843 -0.01551819  0.0102005  ...  0.00619125  0.04821777
 -0.02259827]",5,,text,,,,,8,19,792.0,12,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 448, 563, 554]","Comparisons between seen and unseen splits. We provide the seen and unseen splits for the test set in attribute binding, where the unseen set consists of attribute-object pairs that do not appear in the training set. The unseen split tends to include more uncommon attribute-object combinations than seen split. The performance comparison of seen and unseen splits for attribute binding is shown in Table XV. Our observations reveal that our model exhibits slightly lower performance on the unseen set than the seen set.","[ 0.00403976 -0.05151367  0.00161934 ... -0.00357628  0.00127792
 -0.01913452]",6,,text,,,,,8,19,792.0,12,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 712, 562, 748]","Comparisons of the scalability of our proposed ap- proach. To demonstrate the scalability of our proposed ap- proach, we introduce additional 700 prompts of complex compositions to form an extended training set of 1,400 com- plex prompts. The new prompts are generated with the same methodology . We conduct 6 experiments to train the models with different training set sizes, i.e., 25 prompts, 275 prompts, 350 prompts, 700 prompts, 1050 prompts, and 1400 prompts. The results in Table XVI show the performance of our model grows with the increase of the training set sizes. The results indicate the potential to achieve better performance by scaling up the training set.","[ 0.04882812  0.01117706  0.01256561 ...  0.01571655 -0.02435303
 -0.03515625]",7,,text,,,,,8,19,792.0,12,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[51, 246, 298, 489]",,"[ 0.01021576 -0.03509521  0.00248528 ...  0.01438904  0.0087204
  0.01552582]",0,,image,,Fig. 7: Qualitative comparison between our approach and previous methods.,,src/resources/pdf/T2I-CompBench/auto/images/f797520ea61c6cdba155b7315bfb7bb9fb473775bcda506c9db701257bc84399.jpg,9,19,792.0,13,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[316, 59, 560, 342]",,"[ 0.0044899  -0.03799438 -0.01300049 ...  0.00621796  0.00272942
  0.00590134]",1,,image,,Fig. 8: Qualitative comparison between our approach and previous methods.,,src/resources/pdf/T2I-CompBench/auto/images/cc112750e18585d4ba9fd9702c64a561622825f9569492e80f1c77783c4a0004.jpg,9,19,792.0,13,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[316, 396, 559, 678]",,"[-0.00617599 -0.03271484 -0.01302338 ...  0.00127792  0.00196838
  0.00395203]",2,,image,,Fig. 9: Qualitative comparison between our approach and previous methods.,,src/resources/pdf/T2I-CompBench/auto/images/1fdf7ee5baeac521f5ae30ba4fd78fc7b20421997cd3ab8937e9ee7bed99c0f7.jpg,9,19,792.0,13,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[50, 199, 297, 226]",,"[-0.01791382 -0.05026245  0.02813721 ...  0.03604126 -0.01131439
 -0.00606918]",3,,table,,TABLE XVI: Performances of our model on complex com- positons on the 3-in-1 metric.,,src/resources/pdf/T2I-CompBench/auto/images/38fc18f71059ae7b0d5dc053444c959f9e5b4e006a2dda16cc48128afed1321a.jpg,9,19,792.0,13,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 55, 300, 162]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",4,,text,,,,,9,19,792.0,13,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 532, 300, 698]","Comparisons between original prompts and rephrased detailed prompts. Enhancing prompt details does not signif- icantly improve the generative outcomes, as the challenges primarily lie in the visual content composition instead of the granularity of prompts. While more detailed prompts can improve image quality by providing additional descriptions, they do not necessarily enhance alignment between the gen- erated images and the prompts. To illustrate this, we use GPT-4 to rephrase the prompts to more detailed versions and compare the evaluation results. Specifically, we examine SDXL with attribute binding categories (e.g., color) as an example. The results, shown in Table XVII, indicate that T2I models face similar compositional challenges with both original and detailed prompts.","[-0.0186615  -0.04241943 -0.00181103 ...  0.03790283  0.03335571
 -0.0138092 ]",5,,text,,,,,9,19,792.0,13,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 710, 121, 721]",F. Ablation Study,"[-0.0046463   0.04122925 -0.01101685 ...  0.01406097  0.02754211
 -0.01532745]",6,,text,,,,,9,19,792.0,13,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[47, 725, 299, 748]",We conduct ablation studies on our proposed GORS ap- proach and evaluation metric.,"[ 0.0329895   0.05621338  0.00159454 ...  0.00055552  0.00447083
 -0.01329803]",7,,text,,,,,9,19,792.0,13,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[320, 737, 563, 748]",Finetuning strategy. Our approach finetunes both the,"[ 0.02998352  0.01100922  0.00857544 ... -0.014534   -0.0418396
 -0.02593994]",8,,text,,,,,9,19,792.0,13,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[50, 79, 297, 197]",,"[ 0.026474   -0.01977539 -0.02886963 ...  0.00089169 -0.00540161
 -0.02101135]",0,,table,,TABLE XVII: Comparison of BLIP-VQA scores between original and rephrased detailed prompts.,,src/resources/pdf/T2I-CompBench/auto/images/744afc8f9dc2d467898f378a15ecc9688df6774d723d5ed4194cf8bacf4cd72a.jpg,14,19,792.0,14,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 223, 299, 282]","CLIP text encoder and the U-Net of Stable Diffusion with LoRA [82]. We conduct with the attribute binding (color) sub- category. We investigate the effects of finetuning CLIP only and U-Net only with LoRA. As shown in Table XIX, our model which finetunes both CLIP and U-Net performs better.","[ 0.00302696 -0.00111675  0.00764084 ...  0.03131104  0.01713562
 -0.06106567]",1,,text,,,,,14,19,792.0,14,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 284, 300, 402]","Threshold of selecting samples for finetuning. Our ap- proach fine-tunes Stable Diffusion v2 with the selected sam- ples that align well with the compositional prompts. We manually set a threshold for the alignment score to select samples with higher alignment scores than the threshold for fine-tuning. We experiment with setting the threshold to half of its original value, and setting the threshold to 0 (i.e., use all generated images for finetuning with rewards, without se- lection). Results in Table XIX demonstrate that half threshold and zero threshold will lead to worse performance.","[ 0.02114868  0.007061   -0.01397705 ...  0.03018188  0.02792358
 -0.02139282]",2,,text,,,,,14,19,792.0,14,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 404, 300, 462]","Threshold of UniDet-based metrics. The threshold for the UniDet-based metric is validated via user studies with human correlation, as shown in Table XX. Among various tested thresholds (0.0, 0.25, 0.50, 0.75), the chosen threshold of 0.5 exhibited the highest correlation with human evaluation.","[ 0.0080719   0.0002327  -0.03643799 ...  0.0087738   0.00525284
 -0.03646851]",3,,text,,,,,14,19,792.0,14,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 464, 300, 666]","Qualitative results. We show the qualitative results of the variants in ablation study in Figure 11. When only CLIP is fine-tuned with LoRA, the generated images do not bind attributes to correct objects (for example, Figure 11 Row. 3 Col. 3 and Row. 6 Col. 3). Noticeable improvements are observed in the generated images when U-Net is fine-tuned by LoRA, particularly when both CLIP and U-Net are finetuned together. Furthermore, we delve into the effect of the threshold for selecting images aligned with text prompts for fine-tuning. A higher threshold value enables the selection of images that are highly aligned with text prompts for finetuning, ensuring that only well-aligned examples are incorporated into the finetuning process. In contrast, a lower threshold leads to the inclusion of misaligned images during finetuning, which can degrade the compositional ability of the finetuned text- to-image models (for example, Figure 11 last two columns in Row. 2).","[-0.00502777 -0.0031395   0.00516891 ...  0.01033783  0.02522278
 -0.03320312]",4,,text,,,,,14,19,792.0,14,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 685, 265, 696]",G. Limitation and Potential Negative Social Impacts,"[-0.00305557  0.03102112 -0.00321388 ...  0.01278687  0.01224518
 -0.0328064 ]",5,,text,,,,,14,19,792.0,14,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 701, 299, 748]","One limitation of our work is the absence of a unified metric for all forms of compositionality. Future research can explore the potential of multimodal LLM to develop a unified metric. Our proposed evaluation metrics are not perfect. As shown by the failure cases in Fig. 10, BLIP-VQA may fail in challenging cases, for example, the objects’ shapes are not fully visible in the image, shape’s description is uncommon or the objects are not easy to recognize. The UniDet-based evaluation metric is limited to evaluating 2D spatial relationships and we leave 3D spatial relationships for future study. Researchers need to be aware of the potential negative social impact from the abuse of text-to-image models and the biases of hallucinations from image generators as well as pre-trained multimodal models and multimodal LLMs. Future research should exercise caution when working with generated images and LLM-generated content and devise appropriate prompts to mitigate the impact of hallucinations and bias in those models.","[-0.0005641   0.00487137 -0.00191689 ... -0.00576401  0.01203156
 -0.03237915]",6,,text,,,,,14,19,792.0,14,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 55, 563, 209]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",7,,text,,,,,14,19,792.0,14,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[356, 224, 518, 235]",VII. CONCLUSION AND DISCUSSIONS,"[ 0.04159546  0.02583313 -0.02731323 ... -0.00449753  0.02496338
 -0.01968384]",8,,text,,,,,14,19,792.0,14,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 239, 563, 405]","We propose T2I-CompBench , a comprehensive bench- mark for open-world compositional text-to-image generation, consisting of 8,000 prompts from 4 categories and 8 sub- categories. We propose new evaluation metrics and an im- proved baseline for the benchmark, and validate the effective- ness of the metrics and method by extensive evaluation. We further study the potential and limitations of MLLMs as a unified metrics. Besides, we propose a simple yet effective way GORS to boost the compositionally of text-to-image models. When studying generative models, researchers need to be aware of the potential negative social impact, for example, it might be abused to generate fake news. We also need to be aware of the bias from the image generators and the evaluation metrics based on pretrained multimodal models.","[-0.01869202  0.02099609  0.01311493 ... -0.0098114   0.02433777
 -0.00315857]",9,,text,,,,,14,19,792.0,14,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[389, 420, 486, 430]",ACKNOWLEDGEMENTS,"[ 0.02554321  0.03436279  0.00140095 ... -0.01328278  0.02632141
 -0.02642822]",10,,text,,,,,14,19,792.0,14,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 434, 563, 493]","This work is supported by the National Nature Science Foundation of China (No. 62402406), HKU IDS research Seed Fund, HKU Fintech Academy R&D Funding, HKU Seed Fund for Basic Research, and HKU Seed Fund for Translational and Applied Research.","[ 0.08673096  0.04138184  0.00019825 ... -0.01544189  0.01618958
 -0.02111816]",11,,text,,,,,14,19,792.0,14,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[409, 507, 465, 517]",REFERENCES,"[ 0.00976562  0.04724121 -0.02761841 ... -0.01187134  0.02090454
 -0.00836945]",12,,text,,,,,14,19,792.0,14,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[315, 522, 563, 748]","[1] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High- resolution image synthesis with latent diffusion models,” in CVPR, 2022. [2] J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and T. Salimans, “Cascaded diffusion models for high fidelity image generation,” The Journal of Machine Learning Research, 2022. [3] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans et al., “Photorealistic text-to-image diffusion models with deep language understanding,” in NeurIPS, 2022. [4] P. Dhariwal and A. Nichol, “Diffusion models beat gans on image synthesis,” in NeurIPS, 2021. [5] A. Q. Nichol and P. Dhariwal, “Improved denoising diffusion proba- bilistic models,” ICLR, 2021. [6] H. Chang, H. Zhang, J. Barber, A. Maschinot, J. Lezama, L. Jiang, M.-H. Yang, K. Murphy, W. T. Freeman, M. Rubinstein et al., “Muse: Text-to- image generation via masked generative transformers,” arXiv preprint arXiv:2301.00704, 2023. [7] N. Liu, S. Li, Y. Du, A. Torralba, and J. B. Tenenbaum, “Compositional visual generation with composable diffusion models,” in ECCV, 2022. [8] W. Feng, X. He, T.-J. Fu, V. Jampani, A. Akula, P. Narayana, S. Basu, X. E. Wang, and W. Y. Wang, “Training-free structured diffusion guidance for compositional text-to-image synthesis,” in ICLR, 2023. [9] H. Chefer, Y. Alaluf, Y. Vinker, L. Wolf, and D. Cohen-Or, “Attend-and- excite: Attention-based semantic guidance for text-to-image diffusion models,” in ACM Trans. Graph., 2023. [10] Q. Wu, Y. Liu, H. Zhao, T. Bui, Z. Lin, Y. Zhang, and S. Chang, “Harnessing the spatial-temporal attention of diffusion models for high- fidelity text-to-image synthesis,” in ICCV, 2023. [11] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable visual models from natural language supervision,” in ICML, 2021. [12] J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y. Choi, “Clipscore: A reference-free evaluation metric for image captioning,” arXiv preprint arXiv:2104.08718, 2021. [13] J. Li, D. Li, C. Xiong, and S. Hoi, “Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation,” in ICML, 2022. [14] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language- image pre-training with frozen image encoders and large language models,” arXiv preprint arXiv:2301.12597, 2023. [15] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin, “Sharegpt4v: Improving large multi-modal models with better captions,” arXiv preprint arXiv:2311.12793, 2023. [16] Z. Yang, L. Li, K. Lin, J. Wang, C.-C. Lin, Z. Liu, and L. Wang, “The dawn of lmms: Preliminary explorations with gpt-4v (ision),” arXiv preprint arXiv:2309.17421, vol. 9, no. 1, p. 1, 2023. [17] B. F. Labs, https://github.com/black-forest-labs/flux, 2024. [18] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. M ¨uller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, D. Podell, T. Dockhorn, Z. English, K. Lacey, A. Goodwin, Y. Marek, and R. Rombach, “Scaling rectified flow transformers for high-resolution image synthesis,” 2024. [19] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo et al., “Improving image generation with better captions,” Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, vol. 2, no. 3, p. 8, 2023. [20] J. Chen, J. Yu, C. Ge, L. Yao, E. Xie, Y. Wu, Z. Wang, J. Kwok, P. Luo, H. Lu et al., “Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis,” in ICLR, 2024. [21] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. M¨uller, J. Penna, and R. Rombach, “Sdxl: Improving latent diffusion models for high-resolution image synthesis,” arXiv preprint arXiv:2307.01952, 2023. [22] K. Huang, K. Sun, E. Xie, Z. Li, and X. Liu, “T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation,” NeurIPS, 2024. [23] E. M. Bakr, P. Sun, X. Shen, F. F. Khan, L. E. Li, and M. Elhoseiny, “Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models,” in ICCV, 2023. [24] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee, “Generative adversarial text to image synthesis,” in ICML, 2016. [25] S. E. Reed, Z. Akata, S. Mohan, S. Tenka, B. Schiele, and H. Lee, “Learning what and where to draw,” in NeurIPS, 2016. [26] H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and D. Metaxas, “Stackgan: Text to photo-realistic image synthesis with stacked genera- tive adversarial networks,” in ICCV, 2017. [27] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and X. He, “Attngan: Fine-grained text to image generation with attentional generative adversarial networks,” in CVPR, 2018. [28] M. Zhu, P. Pan, W. Chen, and Y. Yang, “Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis,” in CVPR, 2019. [29] H. Zhang, J. Y. Koh, J. Baldridge, H. Lee, and Y. Yang, “Cross-modal contrastive learning for text-to-image generation,” in CVPR, 2021. [30] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in NeurIPS, 2014. [31] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hierarchical text-conditional image generation with clip latents,” arXiv preprint arXiv:2204.06125, 2022. [32] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen, “Glide: Towards photorealistic image gener- ation and editing with text-guided diffusion models,” in ICML, 2022. [33] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans et al., “Photorealistic text-to-image diffusion models with deep language understanding,” in NeurIPS, 2022. [34] O. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh, and Y. Taig- man, “Make-a-scene: Scene-based text-to-image generation with human priors,” in ECCV, 2022. [35] J. Gu, S. Zhai, Y. Zhang, J. M. Susskind, and N. Jaitly, “Matryoshka diffusion models,” 2023. [36] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, “Zero-shot text-to-image generation,” in ICML, 2021. [37] S. Zhang, X. Yang, Y. Feng, C. Qin, C.-C. Chen, N. Yu, Z. Chen, H. Wang, S. Savarese, S. Ermon et al., “Hive: Harnessing human feed- back for instructional visual editing,” arXiv preprint arXiv:2303.09618, 2023. [38] K. Lee, H. Liu, M. Ryu, O. Watkins, Y. Du, C. Boutilier, P. Abbeel, M. Ghavamzadeh, and S. S. Gu, “Aligning text-to-image models using human feedback,” arXiv preprint arXiv:2302.12192, 2023. [39] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang, “Raft: Reward ranked finetuning for generative foundation model alignment,” arXiv preprint arXiv:2304.06767, 2023. [40] Z. Li, M. R. Min, K. Li, and C. Xu, “Stylet2i: Toward compositional and high-fidelity text-to-image synthesis,” in CVPR, 2022. [41] M. Patel, C. Kim, S. Cheng, C. Baral, and Y. Yang, “Eclipse: A resource- efficient text-to-image prior for image generations,” arXiv preprint arXiv:2312.04655, 2023. [42] X. Liu, T. Hu, W. Wang, K. Kawaguchi, and Y. Yao, “Referee can play: An alternative approach to conditional generation via model inversion,” arXiv preprint arXiv:2402.16305, 2024. [43] D. H. Park, S. Azadi, X. Liu, T. Darrell, and A. Rohrbach, “Benchmark for compositional text-to-image synthesis,” in NeurIPS, 2021. [44] L. Lian, B. Li, A. Yala, and T. Darrell, “Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models,” arXiv preprint arXiv:2305.13655, 2023. [45] M. Chen, I. Laina, and A. Vedaldi, “Training-free layout control with cross-attention guidance,” in WACV, 2024. [46] R. Wang, Z. Chen, C. Chen, J. Ma, H. Lu, and X. Lin, “Compositional text-to-image synthesis with attention map control of diffusion models,” arXiv preprint arXiv:2305.13921, 2023. [47] T. H. S. Meral, E. Simsar, F. Tombari, and P. Yanardag, “Conform: Contrast is all you need for high-fidelity text-to-image diffusion models,” arXiv preprint arXiv:2312.06059, 2023. [48] Y. Kim, J. Lee, J.-H. Kim, J.-W. Ha, and J.-Y. Zhu, “Dense text-to-image generation with attention modulation,” 2023. [49] R. Rassin, E. Hirsch, D. Glickman, S. Ravfogel, Y. Goldberg, and G. Chechik, “Linguistic binding in diffusion models: Enhancing attribute correspondence through attention map alignment,” NeurIPS, 2024. [50] H. Gani, S. F. Bhat, M. Naseer, S. Khan, and P. Wonka, “Llm blueprint: Enabling text-to-image generation with complex and detailed prompts,” arXiv preprint arXiv:2310.10640, 2023. [51] Y. Li, H. Liu, Q. Wu, F. Mu, J. Yang, J. Gao, C. Li, and Y. J. Lee, “Gligen: Open-set grounded text-to-image generation,” in ICCV, 2023. [52] A. Taghipour, M. Ghahremani, M. Bennamoun, A. M. Rekavandi, H. Laga, and F. Boussaid, “Box it to bind it: Unified layout con- trol and attribute binding in t2i diffusion models,” arXiv preprint arXiv:2402.17910, 2024. [53] Z. Wang, E. Xie, A. Li, Z. Wang, X. Liu, and Z. Li, “Divide and conquer: Language models can plan and self-correct for compositional text-to- image generation,” arXiv preprint arXiv:2401.15688, 2024. [54] X. Chen, Y. Liu, Y. Yang, J. Yuan, Q. You, L.-P. Liu, and H. Yang, “Reason out your layout: Evoking the layout master from large language models for text-to-image synthesis,” arXiv preprint arXiv:2311.17126, 2023. [55] L. Yang, Z. Yu, C. Meng, M. Xu, S. Ermon, and B. Cui, “Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms,” arXiv preprint arXiv:2401.11708, 2024. [56] X. Zhang, L. Yang, Y. Cai, Z. Yu, J. Xie, Y. Tian, M. Xu, Y. Tang, Y. Yang, and B. Cui, “Realcompo: Dynamic equilibrium between realism and compositionality improves text-to-image diffusion models,” arXiv preprint arXiv:2402.12908, 2024. [57] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, “The caltech-ucsd birds-200-2011 dataset,” 2011. [58] M.-E. Nilsback and A. Zisserman, “Automated flower classification over a large number of classes,” in ICVGIP, 2008. [59] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in context,” in ECCV, 2014. [60] J. Cho, A. Zala, and M. Bansal, “Dall-eval: Probing the reasoning skills and social biases of text-to-image generation models,” in ICCV, 2023. [61] V. Petsiuk, A. E. Siemenn, S. Surbehera, Z. Chin, K. Tyser, G. Hunter, A. Raghavan, Y. Hicke, B. A. Plummer, O. Kerret et al., “Human evaluation of text-to-image models on a multi-task benchmark,” arXiv preprint arXiv:2211.12112, 2022. [62] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, “Improved techniques for training gans,” in NeurIPS, 2016. [63] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, “Gans trained by a two time-scale update rule converge to a local nash equilibrium,” in NeurIPS, 2017. [64] Y. Lu, X. Yang, X. Li, X. E. Wang, and W. Y. Wang, “Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation,” arXiv preprint arXiv:2305.11116, 2023. [65] Y. Chen, “X-iqe: explainable image quality evaluation for text-to- image generation with visual large language models,” arXiv preprint arXiv:2305.10843, 2023. [66] S. Wen, G. Fang, R. Zhang, P. Gao, H. Dong, and D. Metaxas, “Improving compositional text-to-image generation with large vision- language models,” arXiv preprint arXiv:2310.06311, 2023. [67] J. Xu, X. Liu, Y. Wu, Y. Tong, Q. Li, M. Ding, J. Tang, and Y. Dong, “Imagereward: Learning and evaluating human preferences for text-to- image generation,” 2023. [68] J. Sun, D. Fu, Y. Hu, S. Wang, R. Rassin, D.-C. Juan, D. Alon, C. Herrmann, S. van Steenkiste, R. Krishna et al., “Dreamsync: Aligning text-to-image generation with image understanding feedback,” arXiv preprint arXiv:2311.17946, 2023. [69] Y. Kirstain, A. Polyak, U. Singer, S. Matiana, J. Penna, and O. Levy, “Pick-a-pic: An open dataset of user preferences for text-to-image ge neration,” NeurIPS, 2024. [70] X. Wu, K. Sun, F. Zhu, R. Zhao, and H. Li, “Better aligning text-to- image models with human preference,” 2023. [71] X. Wu, Y. Hao, K. Sun, Y. Chen, F. Zhu, R. Zhao, and H. Li, “Human preference score v2: A solid benchmark for evaluating human pref- erences of text-to-image synthesis,” arXiv preprint arXiv:2306.09341, 2023. [72] Y. Liang, J. He, G. Li, P. Li, A. Klimovskiy, N. Carolan, J. Sun, J. Pont- Tuset, S. Young, F. Yang et al., “Rich human feedback for text-to-image generation,” arXiv preprint arXiv:2312.10240, 2023. [73] M. Ku, T. Li, K. Zhang, Y. Lu, X. Fu, W. Zhuang, and W. Chen, “Im- agenhub: Standardizing the evaluation of conditional image generation models,” arXiv preprint arXiv:2310.01596, 2023. [74] M. Ku, D. Jiang, C. Wei, X. Yue, and W. Chen, “Viescore: Towards explainable metrics for conditional image synthesis evaluation,” arXiv preprint arXiv:2312.14867, 2023. [75] T. Lee, M. Yasunaga, C. Meng, Y. Mai, J. S. Park, A. Gupta, Y. Zhang, D. Narayanan, H. Teufel, M. Bellagente et al., “Holistic evaluation of text-to-image models,” NeurIPS, 2024. [76] OpenAI, https://openai.com/blog/chatgpt/, 2023. [77] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-4: Enhancing vision-language understanding with advanced large language models,” arXiv preprint arXiv:2304.10592, 2023. [78] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., “Chain-of-thought prompting elicits reasoning in large language models,” in NeurIPS, 2022. [79] X. Zhou, V. Koltun, and P. Kr¨ahenbu¨hl, “Simple multi-dataset detection,” in CVPR, 2022. [80] R. Ranftl, A. Bochkovskiy, and V. Koltun, “Vision transformers for dense prediction,” in ICCV, 2021. [81] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4 technical report,” arXiv preprint arXiv:2303.08774, 2023. [82] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, “Lora: Low-rank adaptation of large language models,” arXiv preprint arXiv:2106.09685, 2021. [83] D. Teney, E. Abbasnejad, K. Kafle, R. Shrestha, C. Kanan, and A. Van Den Hengel, “On the value of out-of-distribution testing: An example of goodhart’s law,” Advances in neural information processing systems, vol. 33, pp. 407–417, 2020. [84] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su, J. Zhu, and L. Zhang, “Grounding dino: Marrying dino with grounded pre-training for open-set object detection,” arXiv preprint arXiv:2303.05499, 2023. [85] L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan, L. Zhang, J.-N. Hwang et al., “Grounded language-image pre- training,” in CVPR, 2022. [86] S. Shao, Z. Li, T. Zhang, C. Peng, G. Yu, X. Zhang, J. Li, and J. Sun, “Objects365: A large-scale, high-quality dataset for object detection,” in ICCV, 2019. [87] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont- Tuset, S. Kamali, S. Popov, M. Malloci, A. Kolesnikov et al., “The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale,” in IJCV, 2020. [88] G. Neuhold, T. Ollmann, S. Rota Bulo, and P. Kontschieder, “The mapillary vistas dataset for semantic understanding of street scenes,” in ICCV, 2017. [89] https://github.com/huggingface/diffusers/blob/main/examples/text to image. [90] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,” in ICLR, 2018.","[-0.00240135  0.00437927  0.00910187 ...  0.02636719 -0.0098114
  0.00637436]",13,,text,,,,,14,19,792.0,14,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[53, 120, 559, 219]",,"[-0.00516129 -0.03601074  0.00202179 ... -0.02052307  0.01980591
  0.00855255]",0,,image,,Fig. 10: Failure cases of the evaluation metric BLIP-VQA.,,src/resources/pdf/T2I-CompBench/auto/images/fc91a59a01f4988396e1965a29e310a875c30e36ed9df7d2f92cae76a832e0bd.jpg,6,19,792.0,15,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[314, 270, 559, 600]",,"[ 0.00562286  0.00788879 -0.0227356  ...  0.00144005  0.00467682
 -0.02267456]",1,,image,,Fig. 11: Qualitative comparison of ablation study on fine- tuning strategy and threshold.,,src/resources/pdf/T2I-CompBench/auto/images/01580e2743a5d20def40820fa8f735b2754fe776caeca98c7df3835f850073fc.jpg,6,19,792.0,15,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[122, 68, 489, 102]",,"[ 0.00516129  0.01014709 -0.01963806 ... -0.01390839  0.01641846
 -0.03866577]",2,,table,,TABLE XIX: Ablation studies on fine-tuning strategy and threshold.,,src/resources/pdf/T2I-CompBench/auto/images/a587dbcdf54fb5a38a33a474b95c336b50fdf2cfba1d2ef18e7e941057065fa8.jpg,6,19,792.0,15,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[50, 305, 297, 358]",,"[ 0.05099487 -0.01576233  0.00475693 ... -0.0056839  -0.01507568
  0.00235558]",3,,table,,TABLE XX: Correlation between UniDet-based metric with different thresholds and human evaluation on the category of 2D-spatial relationships.,,src/resources/pdf/T2I-CompBench/auto/images/788f939b2f681c29bcc71b891ac225170162165589a1bbf78d7c8acd60d503a4.jpg,6,19,792.0,15,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[47, 382, 301, 749]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",4,,text,,,,,6,19,792.0,15,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[311, 657, 563, 748]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",5,,text,,,,,6,19,792.0,15,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[47, 54, 301, 747]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,2,19,792.0,16,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[309, 47, 564, 747]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",1,,text,,,,,2,19,792.0,16,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[47, 57, 300, 336]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,1,19,792.0,17,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[85, 413, 260, 709]",,"[ 0.00230598 -0.01872253 -0.00325394 ...  0.00111198  0.0022049
 -0.02706909]",0,,image,,Fig. 12: Qualitative comparisons between original prompts and rephrased detailed prompts.,,src/resources/pdf/T2I-CompBench/auto/images/747eba962e9bcb2b1ca5cbdb0b0c28665ad6871e8cbc45805a17890a6e9e35d0.jpg,7,19,792.0,18,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[351, 77, 525, 651]",,"[-0.02056885 -0.01045227 -0.02137756 ...  0.00322533  0.00262833
 -0.02780151]",1,,image,,and long text prompts.,,src/resources/pdf/T2I-CompBench/auto/images/044314cf2007757ba2d2a5babfc566deb638cb6463628352ebea84300fa6a68f.jpg,7,19,792.0,18,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[152, 56, 196, 65]",APPENDIX,"[ 0.02124023  0.02124023 -0.02232361 ... -0.01487732  0.05291748
 -0.03405762]",2,,text,,,,,7,19,792.0,18,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 72, 300, 190]","Qualitative comparisons between original prompts and rephrased detailed prompts. We observe that enhancing prompt details does not significantly improve the generative outcomes. We use GPT-4 to rephrase the prompts to more detailed versions and compare the evaluation results. The original prompts had an average length of 8.21 words, while the rephrased versions averaged 24.00 words. Qualitative com- parisons, shown in Figure 12, demonstrate that T2I models continue to face compositional challenges, regardless of the level of detail in the prompts.","[ 0.02281189 -0.02081299  0.01406097 ...  0.01346588  0.01110077
 -0.01928711]",3,,text,,,,,7,19,792.0,18,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 192, 300, 274]",Performances of proposed method GORS on short and long text prompts. We compare the performances of the base model (Stable v2) and the GORS model (Stable ) on long text prompts in Figure 13. The results show that GORS model is not overly attuned to short prompts and the ability to handle long text prompts is still preserved in our GORS model.,"[ 0.0340271   0.00451279 -0.01540375 ...  0.02575684  0.0216217
 -0.02183533]",4,,text,,,,,7,19,792.0,18,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 276, 300, 346]","Qualitative results of threshold of UniDet-based metrics. We provide qualitative examples in 2D/3D-spatial relation- ships and generative numeracy, which use UniDet-based met- rics for visualization in Figure 14. The results demonstrate that the chosen threshold effectively distinguishes between good and bad cases.","[-0.016922   -0.0044136  -0.01006317 ...  0.00050211  0.02319336
  0.0068779 ]",5,,text,,,,,7,19,792.0,18,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[48, 347, 299, 394]",Qualitative results of SOTA T2I models. We provide more qualitative examples in non-relationships and multiple ob- jects ) in Figure 15. The results demonstrate that even the SOTA models struggle with complex compositional prompts.,"[ 0.00122452 -0.04833984  0.03579712 ...  0.00681686  0.03356934
  0.00901794]",6,,text,,,,,7,19,792.0,18,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[153, 75, 458, 419]",,"[-0.03973389 -0.04718018 -0.00334167 ...  0.0059433   0.02441406
 -0.02082825]",0,,image,,Fig. 14: Qualitative results of threshold of UniDet-based metrics.,,src/resources/pdf/T2I-CompBench/auto/images/2c5775e1a83872ede941e5b160617d1451e05cc15bfbe27d1fa71ea9b6d59fc3.jpg,4,19,792.0,19,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[78, 518, 530, 694]",,"[-0.00938416 -0.01808167 -0.04122925 ...  0.01030731 -0.02029419
 -0.03277588]",1,,image,,"Fig. 15: Qualitative results of SOTA T2I models (Flux.1, SD3, and DALLE 3) on complex compositional prompts, involving non-spatial relationships and multiple objects  .",,src/resources/pdf/T2I-CompBench/auto/images/964e4b44dc5167b7b2e1a747f3ba8d4de37bbfe763d686f2f3004321317edb31.jpg,4,19,792.0,19,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[78, 488, 298, 513]","A young girl sitting on a giant elephant on a sandy beach at sunset, holding a small red toy car in her hand, while a seagull perches on the elephant's tusk, and a distant ship sails on the horizon.","[-0.02203369 -0.00296402  0.00450516 ... -0.00401306  0.0362854
  0.00321388]",2,,text,,,,,4,19,792.0,19,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[307, 489, 531, 506]","A majestic silver train glides through snowy mountains, while a penguin and a polar bear are enjoying a picnic on the roof.","[ 0.00947571  0.00369263 -0.01997375 ... -0.04214478  0.03591919
  0.01991272]",3,,text,,,,,4,19,792.0,19,612.0,src/resources/pdf/T2I-CompBench.pdf,
"[107, 427, 504, 627]",,"[-0.02566528 -0.00403595 -0.04315186 ...  0.01412964 -0.01895142
  0.01148987]",0,,image,,Figure 1: Selected samples from our best ImageNet  model (FID 3.85),,src/resources/pdf/Diffusion GAN/auto/images/56333a902fb44586e43201356436fe369d0a7e0005b4b972e938cec7df0b3405.jpg,8,44,792.0,1,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[124, 97, 487, 117]",Diffusion Models Beat GANs on Image Synthesis,"[-0.0254364   0.05706787 -0.01879883 ...  0.01345062 -0.00476074
 -0.02774048]",1,,text,,,,,8,44,792.0,1,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[177, 159, 278, 193]",Prafulla Dhariwal∗ OpenAI prafulla@openai.com,"[ 0.0703125   0.04864502 -0.01596069 ...  0.01197052  0.01852417
 -0.06488037]",2,,text,,,,,8,44,792.0,1,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[353, 159, 434, 193]",Alex Nichol∗ OpenAI alex@openai.com,"[ 0.07141113  0.03359985  0.00106812 ...  0.01271057  0.0141449
 -0.03445435]",3,,text,,,,,8,44,792.0,1,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[283, 221, 328, 234]",Abstract,"[-0.01850891  0.02378845 -0.01013184 ...  0.0120697  -0.02732849
  0.00043058]",4,,text,,,,,8,44,792.0,1,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[142, 246, 469, 378]","We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional im- age synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guid- ance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet  , 4.59 on ImageNet  , and 7.72 on ImageNet  , and we match BigGAN-deep even with as few as 25 forward passes per sample, all while main- taining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet  and 3.85 on ImageNet . We release our code at https://github.com/openai/guided-diffusion.","[ 0.00502396  0.06317139 -0.01089478 ...  0.03347778  0.03778076
 -0.02381897]",5,,text,,,,,8,44,792.0,1,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 399, 191, 412]",1 Introduction,"[ 0.02896118  0.05752563 -0.01829529 ...  0.02319336  0.03010559
  0.01599121]",6,,text,,,,,8,44,792.0,1,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 659, 504, 703]","Over the past few years, generative models have gained the ability to generate human-like natural language [6], infinite high-quality synthetic images [5, 28, 51] and highly diverse human speech and music [64, 13]. These models can be used in a variety of ways, such as generating images from text prompts [72, 50] or learning useful feature representations [14, 7]. While these models are already capable of producing realistic images and sound, there is still much room for improvement beyond the current state-of-the-art, and better generative models could have wide-ranging impacts on graphic design, games, music production, and countless other fields.","[ 0.00389099  0.02148438  0.00190449 ...  0.00251961  0.01475525
 -0.01942444]",7,,text,,,,,8,44,792.0,1,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 72, 504, 105]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,9,44,792.0,2,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 111, 505, 166]","GANs [19] currently hold the state-of-the-art on most image generation tasks [5, 68, 28] as measured by sample quality metrics such as FID [23], Inception Score [54] and Precision [32]. However, some of these metrics do not fully capture diversity, and it has been shown that GANs capture less diversity than state-of-the-art likelihood-based models [51, 43, 42]. Furthermore, GANs are often difficult to train, collapsing without carefully selected hyperparameters and regularizers [5, 41, 4].","[-0.02105713 -0.01248169 -0.01311493 ...  0.01638794 -0.02737427
 -0.03164673]",1,,text,,,,,9,44,792.0,2,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 171, 505, 226]","While GANs hold the state-of-the-art, their drawbacks make them difficult to scale and apply to new domains. As a result, much work has been done to achieve GAN-like sample quality with likelihood-based models [51, 25, 42, 9]. While these models capture more diversity and are typically easier to scale and train than GANs, they still fall short in terms of visual sample quality. Furthermore, except for VAEs, sampling from these models is slower than GANs in terms of wall-clock time.","[-0.03616333 -0.02841187  0.02156067 ...  0.00091887 -0.00680923
 -0.04647827]",2,,text,,,,,9,44,792.0,2,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 230, 505, 340]","Diffusion models are a class of likelihood-based models which have recently been shown to produce high-quality images [56, 59, 25] while offering desirable properties such as distribution coverage, a stationary training objective, and easy scalability. These models generate samples by gradually removing noise from a signal, and their training objective can be expressed as a reweighted variational lower-bound [25]. This class of models already holds the state-of-the-art [60] on CIFAR-10 [31], but still lags behind GANs on difficult generation datasets like LSUN and ImageNet. Nichol and Dhariwal [43] found that these models improve reliably with increased compute, and can produce high-quality samples even on the difficult ImageNet dataset using an upsampling stack. However, the FID of this model is still not competitive with BigGAN-deep [5], the current state-of-the-art on this dataset.","[-0.01348114 -0.0037384  -0.00979614 ...  0.00339699 -0.01290894
 -0.0110321 ]",3,,text,,,,,9,44,792.0,2,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 345, 505, 422]","We hypothesize that the gap between diffusion models and GANs stems from at least two factors: first, that the model architectures used by recent GAN literature have been heavily explored and refined; second, that GANs are able to trade off diversity for fidelity, producing high quality samples but not covering the whole distribution. We aim to bring these benefits to diffusion models, first by improving model architecture and then by devising a scheme for trading off diversity for fidelity. With these improvements, we achieve a new state-of-the-art, surpassing GANs on several different metrics and datasets.","[-0.03387451  0.02577209 -0.01916504 ...  0.00485611  0.00684738
 -0.02459717]",4,,text,,,,,9,44,792.0,2,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 427, 506, 570]","The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et al. [25] and the improvements from Nichol and Dhariwal [43] and Song et al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples [61]. Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of- the-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet  and .","[ 0.02923584  0.04653931 -0.02610779 ...  0.01555634  0.02108765
 -0.01082611]",5,,text,,,,,9,44,792.0,2,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 597, 189, 610]",2 Background,"[-0.02877808 -0.00377464 -0.00946045 ...  0.01934814 -0.00137997
 -0.00180531]",6,,text,,,,,9,44,792.0,2,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 628, 504, 651]","In this section, we provide a brief overview of diffusion models. For a more detailed mathematical description, we refer the reader to Appendix B.","[-0.01795959 -0.00368118 -0.01974487 ... -0.01258087  0.01106262
 -0.00744247]",7,,text,,,,,9,44,792.0,2,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 655, 504, 722]","On a high level, diffusion models sample from a distribution by reversing a gradual noising process. In particular, sampling starts with noise  and produces gradually less-noisy samples until reaching a final sample . Each timestep  corresponds to a certain noise level, and  can be thought of as a mixture of a signal  with some noise  where the signal to noise ratio is determined by the timestep . For the remainder of this paper, we assume that the noise  is drawn from a diagonal Gaussian distribution, which works well for natural images and simplifies various derivations.","[ 6.35623932e-04  2.38342285e-02 -5.79452515e-03 ...  2.10723877e-02
 -2.90870667e-05 -9.10949707e-03]",8,,text,,,,,9,44,792.0,2,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 72, 505, 138]","A diffusion model learns to produce a slightly more “denoised”  from  . Ho et al. [25] parameterize this model as a function  which predicts the noise component of a noisy sample . To train these models, each sample in a minibatch is produced by randomly drawing a data sample , a timestep , and noise , which together give rise to a noised sample  (Equation 17). The training objective is then , i.e. a simple mean-squared error loss between the true noise and the predicted noise (Equation 26).","[ 0.00389481  0.02122498  0.01396179 ...  0.00013483 -0.03430176
 -0.00761032]",0,,text,,,,,12,44,792.0,3,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 143, 505, 221]","It is not immediately obvious how to sample from a noise predictor . Recall that diffusion sampling proceeds by repeatedly predicting  from  , starting from . Ho et al. [25] show that, under reasonable assumptions, we can model the distribution  of  given  as a diagonal Gaussian  , where the mean  can be calculated as a function of   (Equation 27). The variance  of this Gaussian distribution can be fixed to a known constant [25] or learned with a separate neural network head [43], and both approaches yield high-quality samples when the total number of diffusion steps  is large enough.","[-0.01113129  0.01908875 -0.02241516 ...  0.01739502 -0.00572586
 -0.00638199]",1,,text,,,,,12,44,792.0,3,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 225, 505, 302]","Ho et al. [25] observe that the simple mean-sqaured error objective, , works better in practice than the actual variational lower bound  that can be derived from interpreting the denoising diffu- sion model as a VAE. They also note that training with this objective and using their corresponding sampling procedure is equivalent to the denoising score matching model from Song and Ermon [58], who use Langevin dynamics to sample from a denoising model trained with multiple noise levels to produce high quality image samples. We often use “diffusion models” as shorthand to refer to both classes of models.","[-0.02934265  0.02104187  0.01834106 ...  0.02272034 -0.01589966
 -0.0031662 ]",2,,text,,,,,12,44,792.0,3,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 315, 192, 326]",2.1 Improvements,"[-0.01543427  0.01070404  0.01609802 ... -0.01384735  0.05474854
 -0.01661682]",3,,text,,,,,12,44,792.0,3,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 335, 505, 368]","Following the breakthrough work of Song and Ermon [58] and Ho et al. [25], several recent papers have proposed improvements to diffusion models. Here we describe a few of these improvements, which we employ for our models.","[ 0.00861359 -0.00126362 -0.04156494 ... -0.01789856  0.02746582
 -0.00171852]",4,,text,,,,,12,44,792.0,3,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 373, 505, 406]","Nichol and Dhariwal [43] find that fixing the variance  to a constant as done in Ho et al. [25] is sub-optimal for sampling with fewer diffusion steps, and propose to parameterize  as a neural network whose output   is interpolated as:","[ 0.0114975   0.02029419  0.01094818 ...  0.00606537 -0.00066376
 -0.01593018]",5,,text,,,,,12,44,792.0,3,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[219, 411, 393, 426]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",6,,text,,,,,12,44,792.0,3,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 437, 505, 503]","Here,  and  (Equation 19) are the variances in Ho et al. [25] corresponding to upper and lower bounds for the reverse process variances. Additionally, Nichol and Dhariwal [43] propose a hybrid objective for training both  and  using the weighted sum . Learning the reverse process variances with their hybrid objective allows sampling with fewer steps without much drop in sample quality. We adopt this objective and parameterization, and use it throughout our experiments.","[ 0.02406311  0.02690125  0.00201607 ... -0.00165749 -0.00233459
 -0.02757263]",7,,text,,,,,12,44,792.0,3,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 508, 505, 574]","Song et al. [57] propose DDIM, which formulates an alternative non-Markovian noising process that has the same forward marginals as DDPM, but allows producing different reverse samplers by changing the variance of the reverse noise. By setting this noise to 0, they provide a way to turn any model  into a deterministic mapping from latents to images, and find that this provides an alternative way to sample with fewer steps. We adopt this sampling approach when using fewer than 50 sampling steps, since Nichol and Dhariwal [43] found it to be beneficial in this regime.","[ 0.01027679 -0.02400208 -0.00790405 ...  0.00706863 -0.00967407
 -0.00741959]",8,,text,,,,,12,44,792.0,3,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 587, 232, 599]",2.2 Sample Quality Metrics,"[-0.03494263  0.01876831 -0.02090454 ...  0.02575684  0.01773071
 -0.03762817]",9,,text,,,,,12,44,792.0,3,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 606, 505, 651]","For comparing sample quality across models, we perform quantitative evaluations using the following metrics. While these metrics are often used in practice and correspond well with human judgement, they are not a perfect proxy, and finding better metrics for sample quality evaluation is still an open problem.","[-0.00902557 -0.00121689 -0.009552   ...  0.00260925  0.00219917
 -0.0461731 ]",10,,text,,,,,12,44,792.0,3,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 655, 505, 722]","Inception Score (IS) was proposed by Salimans et al. [54], and it measures how well a model captures the full ImageNet class distribution while still producing individual samples that are convincing examples of a single class. One drawback of this metric is that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS [3]. To better capture diversity than IS, Fréchet Inception Distance (FID) was proposed by Heusel et al. [23], who argued that it is more consistent with human judgement than Inception Score. FID provides a symmetric measure of the distance between two image distributions in the Inception-V3 [62] latent space. Recently, sFID was proposed by Nash et al. [42] as a version of FID that uses spatial features rather than the standard pooled features. They find that this metric better captures spatial relationships, rewarding image distributions with coherent high-level structure. Finally, Kynkäänniemi et al. [32] proposed Improved Precision and Recall metrics to separately measure sample fidelity as the fraction of model samples which fall into the data manifold (precision), and diversity as the fraction of data samples which fall into the sample manifold (recall).","[-0.05474854 -0.00942993 -0.04006958 ... -0.01126862  0.01050568
 -0.03424072]",11,,text,,,,,12,44,792.0,3,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[132, 70, 480, 181]",,"[-0.00484848 -0.01279449 -0.01734924 ...  0.02256775  0.00749588
 -0.07189941]",0,,table,,,,src/resources/pdf/Diffusion GAN/auto/images/939d62539190914a56f45a1e103c9a80fe96f66e736d960d948f5578368a0981.jpg,9,44,792.0,4,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 209, 505, 298]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",1,,text,,,,,9,44,792.0,4,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 302, 505, 401]","We use FID as our default metric for overall sample quality comparisons as it captures both diversity and fidelity and has been the de facto standard metric for state-of-the-art generative modeling work [27, 28, 5, 25]. We use Precision or IS to measure fidelity, and Recall to measure diversity or distribution coverage. When comparing against other methods, we re-compute these metrics using public samples or models whenever possible. This is for two reasons: first, some papers [27, 28, 25] compare against arbitrary subsets of the training set which are not readily available; and second, subtle implementation differences can affect the resulting FID values [45]. To ensure consistent comparisons, we use the entire training set as the reference batch [23, 5], and evaluate metrics for all models using the same codebase.","[ 0.0069809  -0.00811005 -0.00608826 ...  0.00534439 -0.01131439
 -0.04208374]",2,,text,,,,,9,44,792.0,4,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 417, 267, 430]",3 Architecture Improvements,"[-0.00301552  0.00411606 -0.0114975  ... -0.01452637  0.04953003
 -0.02168274]",3,,text,,,,,9,44,792.0,4,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 441, 503, 464]",In this section we conduct several architecture ablations to find the model architecture that provides the best sample quality for diffusion models.,"[-0.03302002  0.0134964  -0.02928162 ...  0.02828979 -0.01759338
 -0.06109619]",4,,text,,,,,9,44,792.0,4,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 469, 505, 579]","Ho et al. [25] introduced the UNet architecture for diffusion models, which Jolicoeur-Martineau et al. [26] found to substantially improve sample quality over the previous architectures [58, 33] used for denoising score matching. The UNet model uses a stack of residual layers and downsampling convolutions, followed by a stack of residual layers with upsampling colvolutions, with skip con- nections connecting the layers with the same spatial size. In addition, they use a global attention layer at the  resolution with a single head, and add a projection of the timestep embedding into each residual block. Song et al. [60] found that further changes to the UNet architecture improved performance on the CIFAR-10 [31] and CelebA-64 [34] datasets. We show the same result on ImageNet , finding that architecture can indeed give a substantial boost to sample quality on much larger and more diverse datasets at a higher resolution.","[ 0.00317955  0.0193634  -0.04095459 ...  0.02134705 -0.01208496
 -0.03973389]",5,,text,,,,,9,44,792.0,4,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 583, 298, 595]",We explore the following architectural changes:,"[-0.01319885 -0.07177734 -0.01078796 ... -0.00101852 -0.02354431
 -0.02778625]",6,,text,,,,,9,44,792.0,4,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[133, 605, 506, 690]","• Increasing depth versus width, holding model size relatively constant. • Increasing the number of attention heads. • Using attention at  , , and  resolutions rather than only at  . • Using the BigGAN [5] residual block for upsampling and downsampling the activations, following [60]. • Rescaling residual connections with , following [60, 27, 28].","[ 0.00497055 -0.02452087  0.01197052 ...  0.01837158 -0.00408936
 -0.05795288]",7,,text,,,,,9,44,792.0,4,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 700, 505, 722]","For all comparisons in this section, we train models on ImageNet  with batch size 256, and sample using 250 sampling steps. We train models with the above architecture changes and compare them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual connections, all of the other modifications improve performance and have a positive compounding effect. We observe in Figure 2 that while increased depth helps performance, it increases training time and takes longer to reach the same performance as a wider model, so we opt not to use this change in further experiments.","[ 0.01013184 -0.0060997   0.02505493 ...  0.01689148 -0.01881409
 -0.06488037]",8,,text,,,,,9,44,792.0,4,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[121, 221, 490, 398]",,"[-0.00416565 -0.01268005  0.0322876  ...  0.0317688   0.00284767
 -0.07733154]",0,,image,,"Figure 2: Ablation of various architecture changes, showing FID as a function of wall-clock time. FID evaluated over  samples instead of   for efficiency.",,src/resources/pdf/Diffusion GAN/auto/images/2c0bc10fc5f24bbc04285f5311e25c4e2be506c9259d5fc08e56cf1858e5a1e6.jpg,5,44,792.0,5,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[212, 70, 399, 171]",,"[ 0.02696228  0.01049042  0.00414658 ...  0.01288605  0.03399658
 -0.04953003]",1,,table,,,,src/resources/pdf/Diffusion GAN/auto/images/a5c74c057462b5f31a3cd6c531bfd26eeb59f2273439f28216cc6b241c2f27a3.jpg,5,44,792.0,5,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[239, 462, 372, 506]",,"[-0.01114655 -0.00323677  0.03192139 ... -0.03274536 -0.02110291
 -0.0562439 ]",2,,table,,Table 3: Ablating the element-wise operation used when projecting timestep and class embeddings into each residual block. Replacing AdaGN with the Addition  GroupNorm layer from Ho et al. [25] makes FID worse.,,src/resources/pdf/Diffusion GAN/auto/images/31289c8273f02730263efd06a9ae71293432385fd1600c2d5ee0c8c68048e8da.jpg,5,44,792.0,5,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 574, 505, 629]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",3,,text,,,,,5,44,792.0,5,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 634, 505, 722]","We also study other attention configurations that better match the Transformer architecture [66]. To this end, we experimented with either fixing attention heads to a constant, or fixing the number of channels per head. For the rest of the architecture, we use 128 base channels, 2 residual blocks per resolution, multi-resolution attention, and BigGAN up/downsampling, and we train the models for 700K iterations. Table 2 shows our results, indicating that more heads or fewer channels per head improves FID. In Figure 2, we see 64 channels is best for wall-clock time, so we opt to use 64 channels per head as our default. We note that this choice also better matches modern transformer architectures, and is on par with our other configurations in terms of final FID.","[-1.15203857e-02 -1.08489990e-02 -6.33621216e-03 ...  5.63049316e-02
 -5.18560410e-05 -5.31921387e-02]",4,,text,,,,,5,44,792.0,5,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 72, 263, 84]",3.1 Adaptive Group Normalization,"[-0.0019989   0.00506592 -0.03268433 ...  0.01110077  0.02929688
  0.00043249]",0,,text,,,,,14,44,792.0,6,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 92, 505, 159]","We also experiment with a layer [43] that we refer to as adaptive group normalization (AdaGN), which incorporates the timestep and class embedding into each residual block after a group normalization operation [69], similar to adaptive instance norm [27] and FiLM [48]. We define this layer as   , where  is the intermediate activations of the residual block following the first convolution, and  is obtained from a linear projection of the timestep and class embedding.","[-0.00151253 -0.00427246  0.0144577  ...  0.01585388 -0.00287437
 -0.0018301 ]",1,,text,,,,,14,44,792.0,6,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 163, 505, 218]","We had already seen AdaGN improve our earliest diffusion models, and so had it included by default in all our runs. In Table 3, we explicitly ablate this choice, and find that the adaptive group normalization layer indeed improved FID. Both models use 128 base channels and 2 residual blocks per resolution, multi-resolution attention with 64 channels per head, and BigGAN up/downsampling, and were trained for 700K iterations.","[-0.00962067  0.00150394  0.00693893 ...  0.01599121 -0.00135994
 -0.03186035]",2,,text,,,,,14,44,792.0,6,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 223, 505, 268]","In the rest of the paper, we use this final improved model architecture as our default: variable width with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and 8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization for injecting timestep and class embeddings into residual blocks.","[-0.03097534 -0.0168457  -0.02026367 ...  0.03009033  0.00862885
 -0.0262146 ]",3,,text,,,,,14,44,792.0,6,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 282, 226, 297]",4 Classifier Guidance,"[ 0.00448608  0.04592896 -0.01690674 ...  0.00164795  0.02734375
  0.01356506]",4,,text,,,,,14,44,792.0,6,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 307, 505, 373]","In addition to employing well designed architectures, GANs for conditional image synthesis [39, 5] make heavy use of class labels. This often takes the form of class-conditional normalization statistics [16, 11] as well as discriminators with heads that are explicitly designed to behave like classifiers  [40]. As further evidence that class information is crucial to the success of these models, Lucic et al. [36] find that it is helpful to generate synthetic labels when working in a label-limited regime.","[-0.03311157  0.00548172 -0.01459503 ...  0.0158844  -0.02107239
  0.00836945]",5,,text,,,,,14,44,792.0,6,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 378, 505, 456]","Given this observation for GANs, it makes sense to explore different ways to condition diffusion models on class labels. We already incorporate class information into normalization layers (Section 3.1). Here, we explore a different approach: exploiting a classifier  to improve a diffusion generator. Sohl-Dickstein et al. [56] and Song et al. [60] show one way to achieve this, wherein a pre-trained diffusion model can be conditioned using the gradients of a classifier. In particular, we can train a classifier  on noisy images , and then use gradients   to guide the diffusion sampling process towards an arbitrary class label .","[-0.00779343  0.03012085 -0.02445984 ...  0.0056076  -0.04724121
  0.00918579]",6,,text,,,,,14,44,792.0,6,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 460, 505, 505]","In this section, we first review two ways of deriving conditional sampling processes using classifiers. We then describe how we use such classifiers in practice to improve sample quality. We choose the notation  and   for brevity, noting that they refer to separate functions for each timestep  and at training time the models must be conditioned on the input .","[ 0.01123047  0.02459717  0.00830078 ...  0.02949524 -0.01341248
 -0.01235962]",7,,text,,,,,14,44,792.0,6,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 517, 286, 529]",4.1 Conditional Reverse Noising Process,"[ 0.02293396  0.03756714  0.0089798  ...  0.01296997 -0.00424194
 -0.02816772]",8,,text,,,,,14,44,792.0,6,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[105, 537, 504, 560]","We start with a diffusion model with an unconditional reverse noising process  . To condition this on a label  , it suffices to sample each transition2 according to","[ 0.01878357  0.0423584  -0.00714493 ...  0.00597    -0.00749588
 -0.00268745]",9,,text,,,,,14,44,792.0,6,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[219, 565, 392, 579]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",10,,text,,,,,14,44,792.0,6,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 589, 505, 623]","where  is a normalizing constant (proof in Appendix ). It is typically intractable to sample from this distribution exactly, but Sohl-Dickstein et al. [56] show that it can be approximated as a perturbed Gaussian distribution. Here, we review this derivation.","[-0.02357483 -0.00089645 -0.01416779 ... -0.0383606  -0.01522827
 -0.00961304]",11,,text,,,,,14,44,792.0,6,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 627, 504, 650]",Recall that our diffusion model predicts the previous timestep  from timestep  using a Gaussian distribution:,"[-0.02319336  0.01034546 -0.03121948 ... -0.00384331 -0.01132965
 -0.00632858]",12,,text,,,,,14,44,792.0,6,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[200, 654, 410, 695]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",13,,text,,,,,14,44,792.0,6,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[105, 71, 505, 311]",,"[ 0.03013611  0.0357666   0.00496674 ... -0.01403046 -0.01274872
  0.02003479]",0,,table,,,,src/resources/pdf/Diffusion GAN/auto/images/fc48b5308f8b226890b8de4655795f5549a4c74385ba512b7255e180061924b7.jpg,9,44,792.0,7,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 334, 506, 370]","We can assume that  has low curvature compared to . This assumption is reasonable in the limit of infinite diffusion steps, where . In this case, we can approximate using a Taylor expansion around   as","[ 0.03741455  0.01525116 -0.01038361 ...  0.01847839 -0.03604126
 -0.00696945]",1,,text,,,,,9,44,792.0,7,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[167, 377, 443, 406]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",2,,text,,,,,9,44,792.0,7,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 418, 372, 432]","Here, , and  is a constant. This gives","[ 0.02767944  0.02748108 -0.01789856 ... -0.01960754 -0.05523682
 -0.00611496]",3,,text,,,,,9,44,792.0,7,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[131, 438, 479, 527]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",4,,text,,,,,9,44,792.0,7,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 538, 505, 594]","We can safely ignore the constant term , since it corresponds to the normalizing coefficient  in Equation 2. We have thus found that the conditional transition operator can be approximated by a Gaussian similar to the unconditional transition operator, but with its mean shifted by . Algorithm 1 summaries the corresponding sampling algorithm. We include an optional scale factor  for the gradients, which we describe in more detail in Section 4.3.","[ 0.02128601  0.03311157  0.01846313 ... -0.02937317 -0.01182556
 -0.02622986]",5,,text,,,,,9,44,792.0,7,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 609, 268, 622]",4.2 Conditional Sampling for DDIM,"[-0.01494598  0.02726746 -0.01238251 ...  0.025177    0.00485229
 -0.03005981]",6,,text,,,,,9,44,792.0,7,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 631, 505, 687]","The above derivation for conditional sampling is only valid for the stochastic diffusion sampling process, and cannot be applied to deterministic sampling methods like DDIM [57]. To this end, we use a score-based conditioning trick adapted from Song et al. [60], which leverages the connection between diffusion models and score matching [59]. In particular, if we have a model   that predicts the noise added to a sample, then this can be used to derive a score function:","[ 0.00010973  0.01901245 -0.01628113 ...  0.02001953 -0.00248718
 -0.02204895]",7,,text,,,,,9,44,792.0,7,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[233, 695, 378, 721]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",8,,text,,,,,9,44,792.0,7,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[109, 70, 502, 166]",,"[-0.05825806 -0.06933594 -0.03056335 ... -0.00572586 -0.00023603
  0.01004791]",0,,image,,"Figure 3: Samples from an unconditional diffusion model with classifier guidance to condition on the class ""Pembroke Welsh corgi"". Using classifier scale 1.0 (left; FID: 33.0) does not produce convincing samples in this class, whereas classifier scale 10.0 (right; FID: 12.0) produces much more class-consistent images.",,src/resources/pdf/Diffusion GAN/auto/images/5f5265c9b577c6cc8fdd5d0cd36afd388e3099a0056f82276766fcdc34f77ee4.jpg,12,44,792.0,8,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 239, 377, 252]",We can now substitute this into the score function for :,"[ 0.03564453  0.01337433  0.0102005  ... -0.00456619 -0.01022339
 -0.01722717]",1,,text,,,,,12,44,792.0,8,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[173, 254, 439, 297]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",2,,text,,,,,12,44,792.0,8,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[105, 303, 504, 325]","Finally, we can define a new epsilon prediction  which corresponds to the score of the joint distribution:","[ 0.02938843  0.01161194 -0.00729752 ... -0.00180912  0.02455139
 -0.01264191]",3,,text,,,,,12,44,792.0,8,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[215, 327, 396, 342]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",4,,text,,,,,12,44,792.0,8,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 350, 506, 384]","We can then use the exact same sampling procedure as used for regular DDIM, but with the modified noise predictions   instead of . Algorithm 2 summaries the corresponding sampling algorithm.","[-0.01625061 -0.01404572 -0.00327301 ...  0.02441406 -0.02314758
 -0.01795959]",5,,text,,,,,12,44,792.0,8,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 396, 249, 408]",4.3 Scaling Classifier Gradients,"[ 0.06524658  0.05712891  0.00029016 ... -0.01100922  0.02824402
  0.01095581]",6,,text,,,,,12,44,792.0,8,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 416, 505, 482]","To apply classifier guidance to a large scale generative task, we train classification models on ImageNet. Our classifier architecture is simply the downsampling trunk of the UNet model with an attention pool [49] at the 8x8 layer to produce the final output. We train these classifiers on the same noising distribution as the corresponding diffusion model, and also add random crops to reduce overfitting. After training, we incorporate the classifier into the sampling process of the diffusion model using Equation 10, as outlined by Algorithm 1.","[ 0.01173401  0.04013062 -0.01107788 ...  0.03277588  0.00107288
 -0.02204895]",7,,text,,,,,12,44,792.0,8,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 487, 505, 553]","In initial experiments with unconditional ImageNet models, we found it necessary to scale the classifier gradients by a constant factor larger than 1. When using a scale of 1, we observed that the classifier assigned reasonable probabilities (around ) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection. Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly . Figure 3 shows an example of this effect.","[ 0.00042725  0.04608154 -0.0037632  ...  0.00443649 -0.02478027
 -0.01705933]",8,,text,,,,,12,44,792.0,8,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 558, 505, 624]","To understand the effect of scaling classifier gradients, note that , where  is an arbitrary constant. As a result, the conditioning process is still theoretically grounded in a re-normalized classifier distribution proportional to . When  , this distribution becomes sharper than , since larger values are amplified by the exponent. In other words, using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples.","[ 0.02471924  0.03421021 -0.00387764 ...  0.0023365  -0.03305054
 -0.03015137]",9,,text,,,,,12,44,792.0,8,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 628, 505, 695]","In the above derivations, we assumed that the underlying diffusion model was unconditional, modeling . It is also possible to train conditional diffusion models, , and use classifier guidance in the exact same way. Table 4 shows that the sample quality of both unconditional and conditional models can be greatly improved by classifier guidance. We see that, with a high enough scale, the guided unconditional model can get quite close to the FID of an unguided conditional model, although training directly with the class labels still helps. Guiding a conditional model further improves FID.","[ 0.01397705  0.03588867 -0.01050568 ...  0.01499939  0.00294304
 -0.0395813 ]",10,,text,,,,,12,44,792.0,8,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 699, 504, 722]","Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also compare our guidance with the truncation trick from BigGAN in Figure 5. We find that classifier guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up until a certain precision threshold, after which point it cannot achieve better precision.","[-0.01421356  0.03198242 -0.00255966 ... -0.00697327  0.02946472
 -0.01622009]",11,,text,,,,,12,44,792.0,8,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[113, 194, 498, 297]",,"[ 0.00181866  0.01496887 -0.01152802 ... -0.00411987 -0.0276947
 -0.04931641]",0,,image,,Table 4: Effect of classifier guidance on sample quality. Both conditional and unconditional models were trained for 2M iterations on ImageNet  with batch size 256.,,src/resources/pdf/Diffusion GAN/auto/images/2517573d8bf7d9d148b827b3915fb3dd42aa7e6b571aabf9bcedb0436dc15e32.jpg,6,44,792.0,9,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[112, 353, 500, 480]",,"[-0.03149414  0.05142212 -0.01824951 ...  0.02008057 -0.00247002
 -0.05575562]",1,,image,,"Figure 5: Trade-offs when varying truncation for BigGAN-deep and gradient scale for classifier guidance. Models are evaluated on ImageNet . The BigGAN-deep results were produced using the TFHub model [12] at truncation levels [0.1, 0.2, 0.3, ..., 1.0].",,src/resources/pdf/Diffusion GAN/auto/images/90914fefe78dbe588dc9eceedfa78605e78aebff7cd12a181e828b9352ebfa86.jpg,6,44,792.0,9,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[147, 70, 464, 154]",,"[-0.01507568  0.01269531 -0.01844788 ...  0.00354195  0.01565552
 -0.06378174]",2,,table,,,,src/resources/pdf/Diffusion GAN/auto/images/53d3d48ea206ee28dc58ee8cf506f4559c5a411c8d983de3ec7fca386126d860.jpg,6,44,792.0,9,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 544, 505, 621]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",3,,text,,,,,6,44,792.0,9,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 647, 163, 660]",5 Results,"[-0.0395813   0.0307312  -0.02934265 ... -0.03018188 -0.03494263
  0.01105499]",4,,text,,,,,6,44,792.0,9,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 678, 505, 722]","To evaluate our improved model architecture on unconditional image generation, we train separate diffusion models on three LSUN [71] classes: bedroom, horse, and cat. To evaluate classifier guidance, we train conditional diffusion models on the ImageNet [52] dataset at , , and  resolution.","[ 0.02174377  0.02784729 -0.05450439 ...  0.02313232  0.01398468
 -0.01679993]",5,,text,,,,,6,44,792.0,9,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 70, 499, 331]",,"[-0.0116806  -0.03570557 -0.02156067 ...  0.00867462 -0.02792358
 -0.04446411]",0,,table,,,,src/resources/pdf/Diffusion GAN/auto/images/c68c3b8b64524b776de50952d4776ece5af877ce9c42a49a0811040fff8f225c.jpg,8,44,792.0,10,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 334, 505, 400]","Table 5: Sample quality comparison with state-of-the-art generative models for each task. ADM refers to our ablated diffusion model, and ADM-G additionally uses classifier guidance. LSUN diffusion models are sampled using 1000 steps (see Appendix J). ImageNet diffusion models are sampled using 250 steps, except when we use the DDIM sampler with 25 steps. *No BigGAN-deep model was available at this resolution, so we trained our own. †Values are taken from a previous paper, due to lack of public models or samples. ‡Results use two-resolution stacks.","[-0.00608444 -0.00442123 -0.03092957 ...  0.01570129  0.00667191
 -0.04629517]",1,,text,,,,,8,44,792.0,10,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 428, 268, 440]",5.1 State-of-the-art Image Synthesis,"[-0.03286743  0.03765869 -0.00691223 ... -0.01942444  0.00430679
  0.00351334]",2,,text,,,,,8,44,792.0,10,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 448, 505, 514]","Table 5 summarizes our results. Our diffusion models can obtain the best FID on each task, and the best sFID on all but one task. With the improved architecture, we already obtain state-of-the-art image generation on LSUN and ImageNet . For higher resolution ImageNet, we observe that classifier guidance allows our models to substantially outperform the best GANs. These models obtain perceptual quality similar to GANs, while maintaining a higher coverage of the distribution as measured by recall, and can even do so using only 25 diffusion steps.","[ 0.00994873  0.02024841 -0.03567505 ... -0.00108051 -0.00169659
 -0.02813721]",3,,text,,,,,8,44,792.0,10,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 519, 505, 574]","Figure 6 compares random samples from the best BigGAN-deep model to our best diffusion model. While the samples are of similar perceptual quality, the diffusion model contains more modes than the GAN, such as zoomed ostrich heads, single flamingos, different orientations of cheeseburgers, and a tinca fish with no human holding it. We also check our generated samples for nearest neighbors in the Inception-V3 feature space in Appendix C, and we show additional samples in Appendices K-M.","[-0.0668335  -0.01261902 -0.01259613 ...  0.02955627  0.01643372
 -0.0030365 ]",4,,text,,,,,8,44,792.0,10,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 587, 248, 599]",5.2 Comparison to Upsampling,"[-0.0378418   0.00527573  0.03259277 ... -0.00746536  0.04556274
 -0.02398682]",5,,text,,,,,8,44,792.0,10,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 607, 505, 695]","We also compare guidance to using a two-stage upsampling stack. Nichol and Dhariwal [43] and Saharia et al. [53] train two-stage diffusion models by combining a low-resolution diffusion model with a corresponding upsampling diffusion model. In this approach, the upsampling model is trained to upsample images from the training set, and conditions on low-resolution images that are concatenated channel-wise to the model input using a simple interpolation (e.g. bilinear). During sampling, the low-resolution model produces a sample, and then the upsampling model is conditioned on this sample. This greatly improves FID on ImageNet , but does not reach the same performance as state-of-the-art models like BigGAN-deep [43, 53], as seen in Table 5.","[-1.02157593e-02  9.85860825e-05  1.84020996e-02 ...  3.83911133e-02
  1.18408203e-02 -1.22604370e-02]",6,,text,,,,,8,44,792.0,10,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 700, 504, 722]","In Table 6, we show that guidance and upsampling improve sample quality along different axes. While upsampling improves precision while keeping a high recall, guidance provides a knob to trade","[-0.05963135  0.01438904 -0.01309967 ...  0.00115013  0.05099487
 -0.03744507]",7,,text,,,,,8,44,792.0,10,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[110, 70, 501, 324]",,"[-6.81152344e-02  3.63588333e-05  1.16043091e-02 ... -1.04293823e-02
 -7.70950317e-03 -1.02081299e-02]",0,,image,,,,src/resources/pdf/Diffusion GAN/auto/images/f2cb27f48cbf83f489a7b0d27cc083a65e2a3d46cf66c6fd244987f7071405ad.jpg,6,44,792.0,11,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[140, 356, 471, 518]",,"[-0.05743408 -0.02577209 -0.00027132 ...  0.0113678  -0.021698
 -0.05316162]",1,,table,,"Figure 6: Samples from BigGAN-deep with truncation 1.0 (FID 6.95, left) vs samples from our diffusion model with guidance (FID 4.59, middle) and samples from the training set (right).",,src/resources/pdf/Diffusion GAN/auto/images/8260aeebfe7819fdcfeac6685991eadf9a29436fa0e49144af4da94ac04d36db.jpg,6,44,792.0,11,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 521, 506, 576]","Table 6: Comparing our single, upsampling and classifier guided models. For upsampling, we use the upsampling stack from Nichol and Dhariwal [43] combined with our architecture improvements, which we refer to as ADM-U. The base resolution for the two-stage upsampling models is 64 and 128 for the 256 and 512 models, respectively. When combining classifier guidance with upsampling, we only guide the lower resolution model.","[-0.01965332  0.01260376  0.01270294 ...  0.00053787  0.00829315
 -0.00617218]",2,,text,,,,,6,44,792.0,11,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 591, 505, 625]","off diversity for much higher precision. We achieve the best FIDs by using guidance at a lower resolution before upsampling to a higher resolution, indicating that these approaches complement one another.","[-0.05587769  0.02294922 -0.00240135 ...  0.00621796  0.02615356
 -0.05004883]",3,,text,,,,,6,44,792.0,11,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 641, 197, 655]",6 Related Work,"[ 0.02450562  0.01435089  0.00023603 ... -0.01287842  0.02648926
 -0.00793457]",4,,text,,,,,6,44,792.0,11,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 667, 505, 722]","Score based generative models were introduced by Song and Ermon [59] as a way of modeling a data distribution using its gradients, and then sampling using Langevin dynamics [67]. Ho et al. [25] found a connection between this method and diffusion models [56], and achieved excellent sample quality by leveraging this connection. After this breakthrough work, many works followed up with more promising results: Kong et al. [30] and Chen et al. [8] demonstrated that diffusion models work well for audio; Jolicoeur-Martineau et al. [26] found that a GAN-like setup could improve samples from these models; Song et al. [60] explored ways to leverage techniques from stochastic differential equations to improve the sample quality obtained by score-based models; Song et al. [57] and Nichol and Dhariwal [43] proposed methods to improve sampling speed; Nichol and Dhariwal [43] and Saharia et al. [53] demonstrated promising results on the difficult ImageNet generation task using upsampling diffusion models. Also related to diffusion models, and following the work of Sohl-Dickstein et al. [56], Goyal et al. [21] described a technique for learning a model with learned iterative generation steps, and found that it could achieve good image samples when trained with a likelihood objective.","[-0.02682495  0.00045514 -0.00523376 ... -0.010849    0.03277588
 -0.03720093]",5,,text,,,,,6,44,792.0,11,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 72, 505, 171]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,8,44,792.0,12,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 176, 505, 264]","One missing element from previous work on diffusion models is a way to trade off diversity for fidelity. Other generative techniques provide natural levers for this trade-off. Brock et al. [5] introduced the truncation trick for GANs, wherein the latent vector is sampled from a truncated normal distribution. They found that increasing truncation naturally led to a decrease in diversity but an increase in fidelity. More recently, Razavi et al. [51] proposed to use classifier rejection sampling to filter out bad samples from an autoregressive likelihood-based model, and found that this technique improved FID. Most likelihood-based models also allow for low-temperature sampling [1], which provides a natural way to emphasize modes of the data distribution (see Appendix G).","[-0.02680969  0.01335144 -0.00858307 ... -0.00588226  0.00091743
 -0.01512909]",1,,text,,,,,8,44,792.0,12,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 269, 505, 411]","Other likelihood-based models have been shown to produce high-fidelity image samples. VQ-VAE [65] and VQ-VAE-2 [51] are autoregressive models trained on top of quantized latent codes, greatly reducing the computational resources required to train these models on large images. These models produce diverse and high quality images, but still fall short of GANs without expensive rejection sampling and special metrics to compensate for blurriness. DCTransformer [42] is a related method which relies on a more intelligent compression scheme. VAEs are another promising class of likelihood-based models, and recent methods such as NVAE [63] and VDVAE [9] have successfully been applied to difficult image generation domains. Energy-based models are another class of likelihood-based models with a rich history [1, 10, 24]. Sampling from the EBM distribution is challenging, and Xie et al. [70] demonstrate that Langevin dynamics can be used to sample coherent images from these models. Du and Mordatch [15] further improve upon this approach, obtaining high quality images. More recently, Gao et al. [18] incorporate diffusion steps into an energy-based model, and find that doing so improves image samples from these models.","[-0.05960083  0.00069475  0.01766968 ...  0.00473404 -0.00341797
 -0.02035522]",2,,text,,,,,8,44,792.0,12,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 416, 505, 493]","Other works have controlled generative models with a pre-trained classifier. For example, an emerging body of work [17, 47, 2] aims to optimize GAN latent spaces for text prompts using pre-trained CLIP [49] models. More similar to our work, Song et al. [60] uses a classifier to generate class-conditional CIFAR-10 images with a diffusion model. In some cases, classifiers can act as stand-alone generative models. For example, Santurkar et al. [55] demonstrate that a robust image classifier can be used as a stand-alone generative model, and Grathwohl et al. [22] train a model which is jointly a classifier and an energy-based model.","[ 0.01045227  0.04476929 -0.01450348 ... -0.00291824 -0.02047729
  0.00773621]",3,,text,,,,,8,44,792.0,12,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 510, 277, 524]",7 Limitations and Future Work,[0.02488708 0.02838135 0.0241394  ... 0.02110291 0.01765442 0.00036311],4,,text,,,,,8,44,792.0,12,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 536, 505, 613]","While we believe diffusion models are an extremely promising direction for generative modeling, they are still slower than GANs at sampling time due to the use of multiple denoising steps (and therefore forward passes). One promising work in this direction is from Luhman and Luhman [37], who explore a way to distill the DDIM sampling process into a single step model. The samples from the single step model are not yet competitive with GANs, but are much better than previous single-step likelihood-based models. Future work in this direction might be able to completely close the sampling speed gap between diffusion models and GANs without sacrificing image quality.","[-0.02922058 -0.02130127 -0.02137756 ...  0.01597595  0.00510406
 -0.04299927]",5,,text,,,,,8,44,792.0,12,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 618, 505, 673]","Our proposed classifier guidance technique is currently limited to labeled datasets, and we have provided no effective strategy for trading off diversity for fidelity on unlabeled datasets. In the future, our method could be extended to unlabeled data by clustering samples to produce synthetic labels [36] or by training discriminative models to predict when samples are in the true data distribution or from the sampling distribution.","[-0.02862549  0.01849365 -0.0037899  ...  0.01792908 -0.00389099
 -0.00345421]",6,,text,,,,,8,44,792.0,12,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 678, 505, 722]","The effectiveness of classifier guidance demonstrates that we can obtain powerful generative models from the gradients of a classification function. This could be used to condition pre-trained models in a plethora of ways, for example by conditioning an image generator with a text caption using a noisy version of CLIP [49], similar to recent methods that guide GANs using text prompts [17, 47,","[ 0.01696777  0.05728149 -0.01272583 ... -0.00106525  0.01823425
  0.01506805]",7,,text,,,,,8,44,792.0,12,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[105, 73, 505, 95]",2]. It also suggests that large unlabeled datasets could be leveraged in the future to pre-train powerful diffusion models that can later be improved by using a classifier with desirable properties.,"[ 0.01867676  0.04162598 -0.03598022 ...  0.01812744  0.0038166
 -0.02156067]",0,,text,,,,,7,44,792.0,13,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 110, 183, 123]",8 Conclusion,"[-0.00085402  0.03717041 -0.0401001  ... -0.01171112  0.01036072
 -0.01168823]",1,,text,,,,,7,44,792.0,13,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 135, 505, 223]","We have shown that diffusion models, a class of likelihood-based models with a stationary training objective, can obtain better sample quality than state-of-the-art GANs. Our improved architecture is sufficient to achieve this on unconditional image generation tasks, and our classifier guidance technique allows us to do so on class-conditional tasks. In the latter case, we find that the scale of the classifier gradients can be adjusted to trade off diversity for fidelity. These guided diffusion models can reduce the sampling time gap between GANs and diffusion models, although diffusion models still require multiple forward passes during sampling. Finally, by combining guidance with upsampling, we can further improve sample quality on high-resolution conditional image synthesis.","[-0.0199585   0.0350647  -0.01940918 ...  0.021698    0.00725555
 -0.0193634 ]",2,,text,,,,,7,44,792.0,13,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 237, 225, 251]",9 Acknowledgements,"[ 0.01292419  0.04052734  0.00697708 ... -0.01060486  0.01126099
 -0.03335571]",3,,text,,,,,7,44,792.0,13,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 262, 504, 285]","We thank Alec Radford, Mark Chen, Pranav Shyam and Raul Puri for providing feedback on this work.","[ 0.0546875   0.03662109 -0.00217247 ...  0.01664734  0.00590134
 -0.01853943]",4,,text,,,,,7,44,792.0,13,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 300, 164, 313]",References,"[-0.00434494  0.04223633 -0.02407837 ... -0.01483154  0.02075195
  0.00366783]",5,,text,,,,,7,44,792.0,13,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[110, 317, 507, 726]","[1] David Ackley, Geoffrey Hinton, and Terrence Sejnowski. A learning algorithm for boltzmann machines. Cognitive science, 9(1):147-169, 1985. [2] Adverb. The big sleep. https://twitter.com/advadnoun/status/ 1351038053033406468, 2021. [3] Shane Barratt and Rishi Sharma. A note on the inception score. arXiv:1801.01973, 2018. [4] Andrew Brock, Theodore Lim, J. M. Ritchie, and Nick Weston. Neural photo editing with introspective adversarial networks. arXiv:1609.07093, 2016. [5] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv:1809.11096, 2018. [6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. arXiv:2005.14165, 2020. [7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International Conference on Machine Learning, pages 1691–1703. PMLR, 2020. [8] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. arXiv:2009.00713, 2020. [9] Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images. arXiv:2011.10650, 2021. [10] Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz machine. Neural computation, 7(5):889–904, 1995. [11] Harm de Vries, Florian Strub, Jérémie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron Courville. Modulating early visual processing by language. arXiv:1707.00683, 2017. [12] DeepMind. Biggan-deep 128x128 on tensorflow hub. https://tfhub.dev/deepmind/ biggan-deep-128/1, 2018. [13] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: A generative model for music. arXiv:2005.00341, 2020. [14] Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. arXiv:1907.02544, 2019. [15] Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. arXiv:1903.08689, 2019. [16] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic style. arXiv:1610.07629, 2017. [17] Federico A. Galatolo, Mario G. C. A. Cimino, and Gigliola Vaglini. Generating images from caption and vice versa via clip-guided generative latent space search. arXiv:2102.01645, 2021. [18] Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, and Diederik P. Kingma. Learning energy- based models by diffusion recovery likelihood. arXiv:2012.08125, 2020. [19] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv:1406.2661, 2014. [20] Google. Cloud tpus. https://cloud.google.com/tpu/, 2018. [21] Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback: Learning a transition operator as a stochastic recurrent net. arXiv:1711.02282, 2017. [22] Will Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. arXiv:1912.03263, 2019. [23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in Neural Information Processing Systems 30 (NIPS 2017), 2017. [24] Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771–1800, 2002. [25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv:2006.11239, 2020. [26] Alexia Jolicoeur-Martineau, Rémi Piché-Taillefer, Rémi Tachet des Combes, and Ioan- nis Mitliagkas. Adversarial score matching and improved sampling for image generation. arXiv:2009.05475, 2020. [27] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. arXiv:arXiv:1812.04948, 2019. [28] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. arXiv:1912.04958, 2019. [29] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014. [30] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. arXiv:2009.09761, 2020. [31] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 (Canadian Institute for Advanced Research), 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html. [32] Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. arXiv:1904.06991, 2019. [33] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. Refinenet: Multi-path refinement networks for high-resolution semantic segmentation. arXiv:1611.06612, 2016. [34] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015. [35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv:1711.05101, 2017. [36] Mario Lucic, Michael Tschannen, Marvin Ritter, Xiaohua Zhai, Olivier Bachem, and Sylvain Gelly. High-fidelity image generation with fewer labels. arXiv:1903.02271, 2019. [37] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv:2101.02388, 2021. [38] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. arXiv:1710.03740, 2017. [39] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv:1411.1784, 2014. [40] Takeru Miyato and Masanori Koyama. cgans with projection discriminator. arXiv:1802.05637, 2018. [41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv:1802.05957, 2018. [42] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W. Battaglia. Generating images with sparse representations. arXiv:2103.03841, 2021. [43] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. arXiv:2102.09672, 2021. [44] NVIDIA. Stylegan2. https://github.com/NVlabs/stylegan2, 2019. [45] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On buggy resizing libraries and surprising subtleties in fid calculation. arXiv:2104.11222, 2021. [46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. arXiv:1912.01703, 2019. [47] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of stylegan imagery. arXiv:2103.17249, 2021. [48] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. arXiv:1709.07871, 2017. [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. arXiv:2103.00020, 2021. [50] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. arXiv:2102.12092, 2021. [51] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. arXiv:1906.00446, 2019. [52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. arXiv:1409.0575, 2014. [53] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. arXiv:arXiv:2104.07636, 2021. [54] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. arXiv:1606.03498, 2016. [55] Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Image synthesis with a single (robust) classifier. arXiv:1906.09453, 2019. [56] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. arXiv:1503.03585, 2015. [57] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv:2010.02502, 2020. [58] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. arXiv:2006.09011, 2020. [59] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. arXiv:arXiv:1907.05600, 2020. [60] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv:2011.13456, 2020. [61] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Good- fellow, and Rob Fergus. Intriguing properties of neural networks. arXiv:1312.6199, 2013. [62] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. arXiv:1512.00567, 2015. [63] Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. arXiv:2007.03898, 2020. [64] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv:1609.03499, 2016. [65] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. arXiv:1711.00937, 2017. [66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv:1706.03762, 2017. [67] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 681–688. Citeseer, 2011. [68] Yan Wu, Jeff Donahue, David Balduzzi, Karen Simonyan, and Timothy Lillicrap. Logan: Latent optimisation for generative adversarial networks. arXiv:1912.00953, 2019. [69] Yuxin Wu and Kaiming He. Group normalization. arXiv:1803.08494, 2018. [70] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Ying Nian Wu. A theory of generative convnet. arXiv:1602.03264, 2016. [71] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv:1506.03365, 2015. [72] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. arXiv:1612.03242, 2016. [73] Ligeng Zhu. Thop. https://github.com/Lyken17/pytorch-OpCounter, 2018.","[-0.00170231  0.02558899  0.01803589 ...  0.01850891 -0.0150528
 -0.00978088]",6,,text,,,,,7,44,792.0,13,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[105, 38, 507, 727]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,1,44,792.0,14,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[104, 37, 507, 732]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,1,44,792.0,15,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[104, 20, 507, 695]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,1,44,792.0,16,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[151, 311, 459, 516]",,"[ 0.01145172 -0.04193115  0.03109741 ... -0.00971985 -0.00216866
 -0.0249176 ]",0,,table,,,,src/resources/pdf/Diffusion GAN/auto/images/8b948af58297359935b2a3cbac410a4537438c4f2c779eb59ba40943a6982c1a.jpg,9,44,792.0,17,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[212, 631, 398, 704]",,"[-0.02011108 -0.0092392  -0.02001953 ... -0.03500366 -0.01602173
 -0.04385376]",1,,table,,,,src/resources/pdf/Diffusion GAN/auto/images/d9350319ece4b1dc5cff6a03d7280ac82ce9e2f53d62845c9835afc74ff68bb5.jpg,9,44,792.0,17,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 70, 280, 85]",A Computational Requirements,"[ 0.0214386   0.01031494  0.00609589 ... -0.00623703  0.00563431
 -0.01647949]",2,,text,,,,,9,44,792.0,17,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 96, 505, 140]","Compute is essential to modern machine learning applications, and more compute typically yields better results. It is thus important to compare our method’s compute requirements to competing methods. In this section, we demonstrate that we can achieve results better than StyleGAN2 and BigGAN-deep with the same or lower compute budget.","[-0.00522232  0.00762177 -0.02137756 ...  0.00830841  0.00249672
 -0.0552063 ]",3,,text,,,,,9,44,792.0,17,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 154, 185, 166]",A.1 Throughput,"[ 0.02536011  0.05770874 -0.03991699 ...  0.00711823 -0.00089359
 -0.01579285]",4,,text,,,,,9,44,792.0,17,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 174, 505, 229]","We first benchmark the throughput of our models in Table 7. For the theoretical throughput, we measure the theoretical FLOPs for our model using THOP [73], and assume   utilization of an NVIDIA Tesla V100 (120 TFLOPs), while for the actual throughput we use measured wall-clock time. We include communication time across two machines whenever our training batch size doesn’t fit on a single machine, where each of our machines has 8 V100s.","[ 0.05758667  0.01585388  0.00229645 ...  0.0229187  -0.03979492
 -0.00051594]",5,,text,,,,,9,44,792.0,17,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 234, 506, 300]","We find that a naive implementation of our models in PyTorch 1.7 is very inefficient, utilizing only  of the hardware. We also benchmark our optimized version, which use larger per-GPU batch sizes, fused GroupNorm-Swish and fused Adam CUDA ops. For our ImageNet  model in particular, we find that we can increase the per-GPU batch size from 4 to 32 while still fitting in GPU memory, and this makes a large utilization difference. Our implementation is still far from optimal, and further optimizations should allow us to reach higher levels of utilization.",[0.01785278 0.02648926 0.03375244 ... 0.01329803 0.0246582  0.01661682],6,,text,,,,,9,44,792.0,17,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 544, 196, 556]",A.2 Early stopping,"[ 0.01366425  0.05218506  0.00767136 ... -0.01306915  0.00875092
 -0.0249176 ]",7,,text,,,,,9,44,792.0,17,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 565, 504, 621]","In addition, we can train for many fewer iterations while maintaining sample quality superior to BigGAN-deep. Table 8 and 9 evaluate our ImageNet  and  models throughout training. We can see that the ImageNet   model beats BigGAN-deep’s FID (6.02) after 500K training iterations, only one eighth of the way through training. Similarly, the ImageNet model beats BigGAN-deep after 750K iterations, roughly a third of the way through training.","[-0.01394653  0.01620483 -0.00278473 ...  0.00891113 -0.01292419
 -0.03945923]",8,,text,,,,,9,44,792.0,17,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[210, 70, 401, 154]",,"[-0.02612305 -0.00146866 -0.02444458 ... -0.03579712 -0.02514648
 -0.04202271]",0,,table,,,,src/resources/pdf/Diffusion GAN/auto/images/cc742a62805b5989dee86dc2d80aa8b990b21928e97189530c4a815e63c9f56c.jpg,4,44,792.0,18,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[114, 290, 497, 621]",,"[-0.0114975  -0.0280304  -0.02305603 ... -0.01143646 -0.01376343
 -0.04937744]",1,,table,,,,src/resources/pdf/Diffusion GAN/auto/images/2e7c2593e39ff6a16951c74ad4868c61a6aad516955a5e5921f856783da0dbce.jpg,4,44,792.0,18,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 182, 224, 194]",A.3 Compute comparison,"[ 0.02438354  0.03872681  0.01087189 ... -0.00946808  0.02461243
 -0.03482056]",2,,text,,,,,4,44,792.0,18,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 202, 506, 280]","Finally, in Table 10 we compare the compute of our models with StyleGAN2 and BigGAN-deep, and show we can obtain better FIDs with a similar compute budget. For BigGAN-deep, Brock et al. [5] do not explicitly describe the compute requirements for training their models, but rather provide rough estimates in terms of days on a Google TPUv3 pod [20]. We convert their TPU-v3 estimates to V100 days according to 2 TPU  day. For StyleGAN2, we use the reported throughput of 25M images over 32 days 13 hour on one V100 for config-f [44]. We note that our classifier training is relatively lightweight compared to training the generative model.","[ 2.65836716e-05 -3.11279297e-03 -6.83212280e-03 ...  1.54571533e-02
  1.66034698e-03 -2.49481201e-02]",3,,text,,,,,4,44,792.0,18,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 70, 290, 85]",B Detailed Formulation of DDPM,"[ 0.0171051   0.00133038 -0.02056885 ... -0.00733566  0.01792908
 -0.01117706]",0,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 95, 506, 140]","Here, we provide a detailed review of the formulation of Gaussian diffusion models from Ho et al. [25]. We start by defining our data distribution  and a Markovian noising process  which gradually adds noise to the data to produce noised samples  through . In particular, each step of the noising process adds Gaussian noise according to some variance schedule given by :","[-0.02955627  0.01143646  0.03335571 ... -0.02085876  0.00705719
  0.01448059]",1,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[222, 142, 389, 158]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",2,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 165, 505, 191]","Ho et al. [25] note that we need not apply  repeatedly to sample from . Instead,  can be expressed as a Gaussian distribution. With   and","[ 0.00637054  0.00014424 -0.00965881 ... -0.01356506  0.0087738
 -0.01794434]",3,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[212, 193, 399, 225]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",4,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[105, 232, 505, 255]","Here,  tells us the variance of the noise for an arbitrary timestep, and we could equivalently use this to define the noise schedule instead of .","[ 0.02478027 -0.00740051  0.04034424 ... -0.0111618  -0.04815674
  0.01047516]",5,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 259, 504, 284]","Using Bayes theorem, one finds that the posterior   is also a Gaussian with mean  and variance  defined as follows:","[-0.02275085 -0.00019622  0.01565552 ... -0.02142334 -0.01383209
  0.02792358]",6,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[199, 299, 412, 370]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",7,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 375, 505, 453]","If we wish to sample from the data distribution , we can first sample from  and then sample reverse steps  until we reach . Under reasonable settings for  and , the distribution  is nearly an isotropic Gaussian distribution, so sampling  is trivial. All that is left is to approximate  using a neural network, since it cannot be computed exactly when the data distribution is unknown. To this end, Sohl-Dickstein et al. [56] note that  approaches a diagonal Gaussian distribution as and correspondingly , so it is sufficient to train a neural network to predict a mean  and a diagonal covariance matrix  :","[-0.00388718  0.01878357  0.01412201 ...  0.02839661 -0.00735855
  0.00501633]",8,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[213, 456, 398, 470]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",9,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[105, 477, 504, 501]","To train this model such that   learns the true data distribution , we can optimize the following variational lower-bound  for :","[ 0.00314713  0.02198792  0.01128387 ... -0.01109314  0.00105667
 -0.01317596]",10,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[212, 502, 398, 560]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",11,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 566, 504, 610]","While the above objective is well-justified, Ho et al. [25] found that a different objective produces better samples in practice. In particular, they do not directly parameterize  as a neural network, but instead train a model  to predict  from Equation 17. This simplified objective is defined as follows:","[ 0.00742722  0.00719452  0.00110149 ...  0.01158142  0.00793457
 -0.01062012]",12,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[195, 612, 416, 628]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",13,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 636, 410, 648]","During sampling, we can use substitution to derive  from :","[-0.00162029  0.02781677 -0.00122643 ...  0.00984955  0.00304031
 -0.00549698]",14,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[215, 650, 395, 678]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",15,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 686, 505, 722]","Note that  does not provide any learning signal for . Ho et al. [25] find that instead of learning , they can fix it to a constant, choosing either   or . These values correspond to upper and lower bounds for the true reverse step variance.","[ 0.00620651  0.03149414  0.02670288 ... -0.00518799 -0.01763916
 -0.03552246]",16,,text,,,,,17,44,792.0,19,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[127, 102, 484, 316]",,"[-0.03894043 -0.01159668  0.02180481 ...  0.00218964  0.00088882
 -0.01338196]",0,,image,,"Figure 7: Nearest neighbors for samples from a classifier guided model on ImageNet . For each image, the top row is a sample, and the remaining rows are the top 3 nearest neighbors from the dataset. The top samples were generated with classifier scale 1 and 250 diffusion sampling steps (FID 4.59). The bottom samples were generated with classifier scale 2.5 and 25 DDIM steps (FID 5.44).",,src/resources/pdf/Diffusion GAN/auto/images/0ea5770975c74836b2692d365e2a09d6d409ae7fc3206f133a98e465fd3a5bb6.jpg,5,44,792.0,20,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[127, 483, 484, 662]",,"[-0.01985168 -0.00245667 -0.0216217  ... -0.00439453  0.01024628
 -0.02511597]",1,,image,,"Figure 8: Samples when increasing the classifier scale from 0.0 (left) to 5.5 (right). Each row corresponds to a fixed noise seed. We observe that the classifier drastically changes some images, while leaving others relatively unaffected.",,src/resources/pdf/Diffusion GAN/auto/images/b833d1cff6e44ddb7a14e896975d856d39d2b1131ea752c87066e9ca9e0d3c57.jpg,5,44,792.0,20,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 71, 287, 85]",C Nearest Neighbors for Samples,"[ 0.01307678  0.01786804 -0.03083801 ... -0.0030365   0.03137207
  0.01802063]",2,,text,,,,,5,44,792.0,20,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 383, 505, 438]","Our models achieve their best FID when using a classifier to reduce the diversity of the generations. One might fear that such a process could cause the model to recall existing images from the training dataset, especially as the classifier scale is increased. To test this, we looked at the nearest neighbors (in InceptionV3 [62] feature space) for a handful of samples. Figure 7 shows our results, revealing that the samples are indeed unique and not stored in the training set.","[-0.01169586  0.00348663 -0.00570297 ...  0.00021565 -0.00867462
 -0.00366974]",3,,text,,,,,5,44,792.0,20,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 453, 316, 467]",D Effect of Varying the Classifier Scale,"[ 0.02433777  0.05249023  0.00812531 ... -0.00344086 -0.03500366
 -0.00434494]",4,,text,,,,,5,44,792.0,20,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[110, 102, 501, 386]",,"[-0.01428223 -0.03924561 -0.01561737 ...  0.00239182 -0.01173401
 -0.01112366]",0,,image,,Figure 9: Samples from StyleGAN2 (or StyleGAN for bedrooms) with truncation 1.0 (left) vs samples from our diffusion models (middle) and samples from the training set (right).,,src/resources/pdf/Diffusion GAN/auto/images/8e31e6422bac35bd2140247e52de7298b074fd11cd1599baf72062d9297d0c35.jpg,2,44,792.0,21,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 71, 275, 85]",E LSUN Diversity Comparison,"[-0.02668762 -0.01456451 -0.02078247 ... -0.00395584 -0.0062561
 -0.02294922]",1,,text,,,,,2,44,792.0,21,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[136, 378, 476, 663]",,"[-0.03384399 -0.0045433  -0.0350647  ... -0.01032257 -0.00849152
 -0.02981567]",0,,image,,Figure 10a: DDIM latent reconstructions and interpolations on real images with no classifier guidance.,,src/resources/pdf/Diffusion GAN/auto/images/54013e325679d027a4673997e582f88aee3212bb2b9cf591bc7868f1b3f3952c.jpg,9,44,792.0,22,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 70, 391, 85]",F Interpolating Between Dataset Images Using DDIM,"[-0.01070404  0.03768921 -0.02410889 ...  0.0121994  -0.00397873
 -0.00880432]",1,,text,,,,,9,44,792.0,22,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 95, 505, 140]","The DDIM [57] sampling process is deterministic given the initial noise , thus giving rise to an implicit latent space. It corresponds to integrating an ODE in the forward direction, and we can run the process in reverse to get the latents that produce a given real image. Here, we experiment with encoding real images into this latent space and then interpolating between them.","[-0.01580811  0.00551224 -0.01119232 ...  0.01371765 -0.01435852
 -0.01809692]",2,,text,,,,,9,44,792.0,22,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 144, 330, 156]",Equation 13 for the generative pass in DDIM looks like,"[-0.01183319 -0.00360489 -0.0023613  ...  0.00836182 -0.02697754
 -0.02127075]",3,,text,,,,,9,44,792.0,22,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[120, 161, 490, 183]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",4,,text,,,,,9,44,792.0,22,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 192, 504, 215]","Thus, in the limit of small steps, we can expect the reversal of this ODE in the forward direction looks like","[ 0.01242828  0.01664734 -0.00602722 ...  0.00133896 -0.00887299
 -0.01187134]",5,,text,,,,,9,44,792.0,22,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[121, 218, 491, 240]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",6,,text,,,,,9,44,792.0,22,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 250, 505, 307]","We found that this reverse ODE approximation gives latents with reasonable reconstructions, even with as few as 250 reverse steps. However, we noticed some noise artifacts when reversing all 250 steps, and find that reversing the first 249 steps gives much better reconstructions. To interpolate the latents, class embeddings, and classifier log probabilities, we use   where sweeps linearly from 0 to π2 .","[ 0.0165863   0.0406189   0.04226685 ... -0.00067854  0.00349617
  0.01654053]",7,,text,,,,,9,44,792.0,22,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 310, 505, 366]","Figures   through  show DDIM latent space interpolations on a class-conditional model, while varying the classifier scale. The left and rightmost images are ground truth dataset examples, and between them are reconstructed interpolations in DDIM latent space (including both endpoints). We see that the model with no guidance has almost perfect reconstructions due to its high recall, whereas raising the guidance scale to 2.5 only finds approximately similar reconstructions.","[-0.0539856   0.00778961 -0.01374054 ...  0.02526855  0.03604126
  0.00794983]",8,,text,,,,,9,44,792.0,22,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[136, 87, 475, 374]",,"[-0.0350647   0.02197266 -0.03155518 ... -0.01663208 -0.01216888
 -0.02946472]",0,,image,,Figure 10b: DDIM latent reconstructions and interpolations on real images with classifier scale 1.0.,,src/resources/pdf/Diffusion GAN/auto/images/a3bf6e8b88bdf533cb5f7f70918b4d6b397c647b852bb5fbf640a573962761de.jpg,2,44,792.0,23,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[136, 396, 475, 683]",,"[-0.01480103  0.01444244 -0.04446411 ... -0.00615311 -0.00094318
 -0.01679993]",1,,image,,Figure 10c: DDIM latent reconstructions and interpolations on real images with classifier scale 2.5.,,src/resources/pdf/Diffusion GAN/auto/images/023cff8c67d58834668c659bc6991386be5f40d60f96dc2ebb7c9756511f13d2.jpg,2,44,792.0,23,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[114, 266, 497, 354]",,"[ 0.01476288  0.00986481 -0.01049805 ...  0.00265312  0.00076532
 -0.04437256]",0,,image,,Figure 11: The effect of changing temperature for an ImageNet  model.,,src/resources/pdf/Diffusion GAN/auto/images/959e0d7beaf96da5260c36ca81eee91be9c0eb8959253944918cd480f038bba6.jpg,5,44,792.0,24,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[114, 396, 497, 576]",,"[-0.07098389  0.01317596  0.00050688 ...  0.00100136 -0.00109386
 -0.02217102]",1,,image,,Figure 12: Samples at temperature 0.98 with epsilon scaling (left) and noise scaling (right).,,src/resources/pdf/Diffusion GAN/auto/images/96222ac9dba3432901f00566e20a150994ed10ea6973db19ba5e1c2ab19e6791.jpg,5,44,792.0,24,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 71, 295, 85]",G Reduced Temperature Sampling,"[ 0.00347328  0.04244995  0.01628113 ...  0.00808716  0.00991058
 -0.03631592]",2,,text,,,,,5,44,792.0,24,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 95, 506, 184]","We achieved our best ImageNet samples by reducing the diversity of our models using classifier guidance. For many classes of generative models, there is a much simpler way to reduce diversity: reducing the temperature [1]. The temperature parameter  is typically setup so that  corre- sponds to standard sampling, and  focuses more on high-density samples. We experimented with two ways of implementing this for diffusion models: first, by scaling the Gaussian noise used for each transition by , and second by dividing  by . The latter implementation makes sense when thinking about  as a re-scaled score function (see Section 4.2), and scaling up the score function is similar to scaling up classifier gradients.","[-0.00728607  0.02285767  0.00445175 ...  0.03036499  0.00979614
 -0.00900269]",3,,text,,,,,5,44,792.0,24,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 189, 506, 255]","To measure how temperature scaling affects samples, we experimented with our ImageNet model, evaluating FID, Precision, and Recall across different temperatures (Figure 11). We find that two techniques behave similarly, and neither technique provides any substantial improvement in our evaluation metrics. We also find that low temperatures have both low precision and low recall, indicating that the model is not focusing on modes of the real data distribution. Figure 12 highlights this effect, indicating that reducing temperature produces blurry, smooth images.","[-0.02732849  0.00466537  0.00083971 ...  0.02076721 -0.00951385
 -0.03088379]",4,,text,,,,,5,44,792.0,24,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 70, 281, 85]",H Conditional Diffusion Process,"[ 0.03085327  0.05090332 -0.01681519 ...  0.02267456 -0.01540375
 -0.01647949]",0,,text,,,,,8,44,792.0,25,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 102, 505, 136]","In this section, we show that conditional sampling can be achieved with a transition operator proportional to  , where   approximates   and  approximates the label distribution for a noised sample .","[ 0.00424194  0.03134155  0.02915955 ... -0.0112381  -0.0116806
 -0.02514648]",1,,text,,,,,8,44,792.0,25,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[103, 140, 504, 163]","We start by defining a conditional Markovian noising process  similar to , and assume that is a known and readily available label distribution for each sample.","[ 0.00291252  0.01429749  0.03442383 ...  0.01429749 -0.02931213
  0.00044727]",2,,text,,,,,8,44,792.0,25,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[219, 177, 391, 255]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",3,,text,,,,,8,44,792.0,25,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 275, 506, 309]","While we defined the noising process  conditioned on , we can prove that  behaves exactly like  when not conditioned on . Along these lines, we first derive the unconditional noising operator :","[ 0.02706909  0.03741455  0.00573349 ... -0.0052948   0.00502014
 -0.00047421]",4,,text,,,,,8,44,792.0,25,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[222, 324, 389, 465]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",5,,text,,,,,8,44,792.0,25,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 483, 369, 496]","Following similar logic, we find the joint distribution :","[-0.00973511 -0.02676392 -0.01734924 ...  0.00980377  0.01198578
  0.00859833]",6,,text,,,,,8,44,792.0,25,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[214, 512, 398, 722]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",7,,text,,,,,8,44,792.0,25,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 72, 289, 84]","Using Equation 44, we can now derive :","[-0.00281525  0.00492477 -0.02024841 ... -0.01261902 -0.01730347
 -0.00845337]",0,,text,,,,,8,44,792.0,26,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[213, 90, 399, 219]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",1,,text,,,,,8,44,792.0,26,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 241, 505, 265]","Using the identities  and , it is trivial to show via Bayes rule that the unconditional reverse process  .","[ 0.01374817  0.02653503 -0.01620483 ...  0.00508881  0.02397156
 -0.01719666]",2,,text,,,,,8,44,792.0,26,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 268, 504, 302]","One observation about  is that it gives rise to a noisy classification function, . We can show that this classification distribution does not depend on  (a noisier version of ), a fact which we will later use:","[ 0.01672363  0.00073242  0.00937653 ... -0.01940918  0.01306915
  0.0350647 ]",3,,text,,,,,8,44,792.0,26,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[220, 306, 390, 376]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",4,,text,,,,,8,44,792.0,26,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 399, 311, 410]",We can now derive the conditional reverse process:,"[ 0.00595474  0.02967834 -0.03869629 ...  0.01708984 -0.01186371
 -0.02055359]",5,,text,,,,,8,44,792.0,26,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[205, 417, 406, 582]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",6,,text,,,,,8,44,792.0,26,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 605, 505, 661]","The  term can be treated as a constant since it does not depend on  . We thus want to sample from the distribution  where  is a normalizing constant. We already have a neural network approximation of , called , so all that is left is an approximation of . This can be obtained by training a classifier  on noised images derived by sampling from .","[ 0.00801849  0.03991699 -0.00647736 ... -0.0020504  -0.02566528
  0.02015686]",7,,text,,,,,8,44,792.0,26,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[113, 319, 496, 482]",,"[-0.00828552 -0.05584717 -0.00884247 ...  0.02505493 -0.04367065
 -0.02334595]",0,,table,,,,src/resources/pdf/Diffusion GAN/auto/images/6432221af4082454a439be32dc808f41bf9cc3d44062e52fe8330c749e5d6bb7.jpg,7,44,792.0,27,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[128, 518, 482, 681]",,"[-0.02388    -0.04223633 -0.00904846 ...  0.02532959 -0.02796936
 -0.02061462]",1,,table,,,,src/resources/pdf/Diffusion GAN/auto/images/2d5144c0e0a84d4e7b0db1708958cfb2a91dcb4d0599ea560c341fed721b094a.jpg,7,44,792.0,27,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 71, 215, 85]",I Hyperparameters,"[-0.00320053  0.06018066 -0.00880432 ...  0.02546692  0.02120972
  0.00222969]",2,,text,,,,,7,44,792.0,27,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 96, 505, 140]","When choosing optimal classifier scales for our sampler, we swept over   for ImageNet  and ImageNet , and   for ImageNet . For DDIM, we swept over values [0.5, 0.75, 1.0, 1.25, 2] for ImageNet  ,  for ImageNet , and  for ImageNet .","[ 0.013237    0.00848389  0.01327515 ...  0.0043869  -0.00732803
 -0.0055275 ]",3,,text,,,,,7,44,792.0,27,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 144, 505, 211]","Hyperparameters for training the diffusion and classification models are in Table 11 and Table 12 respectively. Hyperparameters for guided sampling are in Table 14. Hyperparameters used to train upsampling models are in Table 13. We train all of our models using Adam [29] or AdamW [35] with  and . We train in 16-bit precision using loss-scaling [38], but maintain 32-bit weights, EMA, and optimizer state. We use an EMA rate of 0.9999 for all experiments. We use PyTorch [46], and train on NVIDIA Tesla V100s.",[0.03442383 0.02790833 0.03161621 ... 0.02325439 0.01582336 0.02384949],4,,text,,,,,7,44,792.0,27,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 215, 505, 271]","For all architecture ablations, we train with batch size 256, and sample using 250 sampling steps. For our attention heads ablations, we use 128 base channels, 2 residual blocks per resolution, multi- resolution attention, and BigGAN up/downsampling, and we train the models for 700K iterations. By default, all of our experiments use adaptive group normalization, except when explicitly ablating for it.","[-0.01509857 -0.01370239 -0.00655365 ...  0.04656982  0.0110321
 -0.01934814]",5,,text,,,,,7,44,792.0,27,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 276, 505, 309]","When sampling with 1000 timesteps, we use the same noise schedule as for training. On ImageNet, we use the uniform stride from Nichol and Dhariwal [43] for 250 step samples and the slightly different uniform stride from Song et al. [57] for 25 step DDIM.","[ 0.00867462 -0.00706482  0.00931549 ...  0.03225708 -0.02160645
  0.00349808]",6,,text,,,,,7,44,792.0,27,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[165, 173, 445, 335]",,"[-0.02133179 -0.02949524 -0.0023613  ...  0.03408813 -0.03173828
 -0.00984955]",0,,table,,,,src/resources/pdf/Diffusion GAN/auto/images/111b7227ed063c2e095ee3a0e3f22ea6aae73d09385dbaad90156852aee92458.jpg,2,44,792.0,28,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[115, 566, 495, 609]",,"[-0.02116394 -0.00766373  0.0048027  ...  0.03219604 -0.01023102
 -0.00455093]",1,,table,,Table 14: Hyperparameters for classifier-guided sampling.,,src/resources/pdf/Diffusion GAN/auto/images/b17c37ff3baafbe8418e39a36c1403d0a3660d432dcb7fac58bc141c04cab483.jpg,2,44,792.0,28,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[250, 209, 360, 313]",,"[-0.01450348 -0.02264404  0.02674866 ...  0.00580978 -0.00965881
 -0.04876709]",0,,table,,,,src/resources/pdf/Diffusion GAN/auto/images/266fabbe6df90b6f56d2a7ff76515d96775d3a1aad60d92ba0f0a124bfd10451.jpg,6,44,792.0,29,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[205, 396, 405, 568]",,"[-0.0073967  -0.05111694 -0.0194397  ... -0.01121521 -0.01800537
 -0.01968384]",1,,table,,,,src/resources/pdf/Diffusion GAN/auto/images/e8fb428b33d16114d4c9fb0867ff1e153afac0688abdcb039ae1c6d7da660586.jpg,6,44,792.0,29,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 70, 322, 85]",J Using Fewer Sampling Steps on LSUN,"[ 0.01142883  0.02163696 -0.01681519 ...  0.00447083  0.01053619
 -0.01066589]",2,,text,,,,,6,44,792.0,29,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 95, 505, 162]","We initially found that our LSUN models achieved much better results when sampling with 1000 steps rather than 250 steps, contrary to previous results from Nichol and Dhariwal [43]. To address this, we conducted a sweep over sampling-time noise schedules, finding that an improved schedule can largely close the gap. We swept over schedules on LSUN bedrooms, and selected the schedule with the best FID for use on the other two datasets. Table 15 details the findings of this sweep, and Table 16 applies this schedule to three LSUN datasets.","[ 0.00445175 -0.05148315 -0.0096817  ...  0.01329803  0.0001173
 -0.02990723]",3,,text,,,,,6,44,792.0,29,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 167, 505, 200]","While sweeping over sampling schedules is not as expensive as re-training models from scratch, it does require a significant amount of sampling compute. As a result, we did not conduct an exhaustive sweep, and superior schedules are likely to exist.","[ 0.02500916 -0.00494385  0.02909851 ... -0.00174713  0.01011658
 -0.01907349]",4,,text,,,,,6,44,792.0,29,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 316, 506, 371]","Table 15: Results of sweeping over 250 step sampling schedules on LSUN bedrooms. The schedule is expressed as a sequence of five integers, where each integer is the number of steps allocated to one fifth of the diffusion process. The first integer corresponding to   and the last to . Thus, 50, 50, 50, 50, 50 is a uniform schedule, and  is a schedule where all timesteps are spent near .","[-0.0304718  -0.06008911 -0.00091934 ... -0.00117588 -0.01026917
  0.00680923]",5,,text,,,,,6,44,792.0,29,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 100, 504, 697]",,"[-0.01792908 -0.01460266 -0.02111816 ...  0.02003479 -0.03665161
  0.00218391]",0,,image,,"Figure 13: Samples from our best  model (FID: 3.85). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball.",,src/resources/pdf/Diffusion GAN/auto/images/b03f2c95054d9b682cbeac36e0095fb45889be8179f0c2f0a4b2e0ccb003bd7a.jpg,2,44,792.0,30,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 71, 301, 85]",K Samples from ImageNet 512 512,"[-0.05456543  0.06246948 -0.01789856 ... -0.00232124 -0.01902771
 -0.01106262]",1,,text,,,,,2,44,792.0,30,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 71, 505, 668]",,"[-0.03805542 -0.0194397  -0.00492477 ... -0.00032687 -0.0335083
 -0.00361824]",0,,image,,"Figure 14: Samples from our best  model (FID: 3.85). Classes are 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric.",,src/resources/pdf/Diffusion GAN/auto/images/0b405d9d24064178c695f46887ea5831f2cf76616f9e8033951c625582296283.jpg,1,44,792.0,31,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 74, 505, 667]",,"[-0.04794312 -0.02973938 -0.00144291 ... -0.01496887 -0.00626755
 -0.00131226]",0,,image,,"Figure 15: Difficult class samples from our best  model (FID: 3.85). Classes are 432: bassoon, 468: cab, 424: barbershop, 444: bicycle-built-for-two, 981: ballplayer, 550: espresso maker.",,src/resources/pdf/Diffusion GAN/auto/images/6a2cf48457c9673f1f9f1d7e31ab243c2c4ddacbb7e4ff1797bdab12716d5626.jpg,1,44,792.0,32,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 78, 504, 672]",,"[-0.02272034 -0.01007843 -0.02172852 ...  0.00991058 -0.04013062
  0.012146  ]",0,,image,,"Figure 16: Samples from our guided  model using 250 steps with classifier scale 4.0 (FID 7.72). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball.",,src/resources/pdf/Diffusion GAN/auto/images/bf61da5cc666b8e233aeecb7dcea1492e7505a0c8191f4b5682232546a341230.jpg,1,44,792.0,33,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[108, 81, 504, 672]",,"[-0.03167725 -0.01741028 -0.012146   ...  0.00652313 -0.0328064
 -0.00524139]",0,,image,,"Figure 17: Samples from our guided  model using 250 steps with classifier scale 4.0 (FID 7.72). Classes are 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric.",,src/resources/pdf/Diffusion GAN/auto/images/a2ae52304426eb258530420702153f484b7a6397763cb7a190bab780098affd4.jpg,1,44,792.0,34,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 222, 505, 622]",,"[-0.00366592 -0.03305054 -0.01574707 ...  0.01239777 -0.01064301
 -0.0059166 ]",0,,image,,Figure 18: Random samples from our best ImageNet  model (FID 3.85).,,src/resources/pdf/Diffusion GAN/auto/images/16f117fa23ed9b2287748460cf40b229460b1cf499c9edd1d7fbe9025a054cb3.jpg,1,44,792.0,35,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 219, 505, 615]",,"[-0.03866577  0.00540924 -0.00617599 ...  0.0051651  -0.00869751
  0.01527405]",0,,image,,Figure 19: Random samples from our guided  model using 250 steps with classifier scale 4.0 (FID 7.72).,,src/resources/pdf/Diffusion GAN/auto/images/c0bf39493f40e25d7a0cc190236b1ecb45ff9b5ea203ae0f6256c1a3b6c71a07.jpg,1,44,792.0,36,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 101, 505, 696]",,"[-0.03744507 -0.00765991 -0.00512314 ... -0.00820923 -0.03973389
 -0.02180481]",0,,image,,"Figure 20: Samples using our best  model (FID 3.94). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball, 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric",,src/resources/pdf/Diffusion GAN/auto/images/5f2ddeb953e796dbc29678a70419b811c6ba41b156255e57fa94b8d84efc8520.jpg,2,44,792.0,37,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 71, 300, 85]",L Samples from ImageNet,"[-0.0508728   0.04971313 -0.02670288 ...  0.01520538 -0.01316833
 -0.03164673]",1,,text,,,,,2,44,792.0,37,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 77, 505, 668]",,"[-0.03839111 -0.0181427  -0.01204681 ... -0.00342369 -0.05569458
 -0.00114918]",0,,image,,"Figure 21: Samples from our guided  model using 250 steps with classifier scale 1.0 (FID 4.59). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball, 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric",,src/resources/pdf/Diffusion GAN/auto/images/9a3b1774bd81d2a202def58c1bbad9031723355c05cd3e0052d66455a5ce19fd.jpg,1,44,792.0,38,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 79, 504, 668]",,"[-0.03083801 -0.0206604  -0.00759125 ...  0.00664902 -0.03811646
  0.00480652]",0,,image,,"Figure 22: Samples from our guided  model using 25 DDIM steps with classifier scale 2.5 (FID 5.44). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball, 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric",,src/resources/pdf/Diffusion GAN/auto/images/5e54a6eba5cde41ee887471d0e58ede60b27da1cc76b4b7a88f00e1ea709d7ab.jpg,1,44,792.0,39,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 222, 505, 622]",,"[-0.02514648 -0.01849365  0.00254059 ...  0.01431274 -0.02049255
  0.00380898]",0,,image,,Figure 23: Random samples from our best  model (FID 3.94).,,src/resources/pdf/Diffusion GAN/auto/images/d4b4422c70b7fa1dc60562f276963d4158095bcf4eabfc5f2113243fdaf6cb1e.jpg,1,44,792.0,40,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 218, 505, 615]",,"[-0.03111267 -0.02194214  0.00965118 ... -0.01259613 -0.01622009
  0.01829529]",0,,image,,Figure 24: Random samples from our guided  model using 250 steps with classifier scale 1.0 (FID 4.59).,,src/resources/pdf/Diffusion GAN/auto/images/8644c0aa0231ab3699c0853e6f12c1852514db9889c1a96dfa57425ee18d9039.jpg,1,44,792.0,41,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 101, 504, 499]",,"[-0.03237915 -0.06787109 -0.00183201 ...  0.02119446 -0.02372742
 -0.00938416]",0,,image,,Figure 25: Random samples from our LSUN bedroom model using 1000 sampling steps. (FID 1.90),,src/resources/pdf/Diffusion GAN/auto/images/4e088053c3ef02af379c182f9a43a226ddb97525a2139fb76d9847b5699466eb.jpg,1,44,792.0,42,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[107, 222, 504, 622]",,"[-0.05728149 -0.05535889  0.00810242 ... -0.00926971 -0.03250122
 -0.03295898]",0,,image,,Figure 26: Random samples from our LSUN horse model using 1000 sampling steps. (FID 2.57),,src/resources/pdf/Diffusion GAN/auto/images/13841a554ac9f151099b704b4190429593fd260df43f4d6e82d71f97bca057df.jpg,1,44,792.0,43,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[106, 222, 505, 622]",,"[-0.03924561 -0.05374146  0.00718307 ...  0.0163269  -0.01751709
  0.01957703]",0,,image,,Figure 27: Random samples from our LSUN cat model using 1000 sampling steps. (FID 5.57),,src/resources/pdf/Diffusion GAN/auto/images/4d04607e0bd5ec49820a115487fbbc93fb8252318babad7d7f3a6387ab8e1319.jpg,1,44,792.0,44,612.0,src/resources/pdf/Diffusion GAN.pdf,
"[100, 117, 519, 133]",2024 中国算力大会在郑州开幕，开幕式上，中国算力服务平台(河南)正式启动,"[ 0.01672363  0.04721069 -0.02452087 ... -0.00591278  0.01169586
 -0.00844574]",0,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[178, 148, 417, 169]",22 个签约项目总金额超 230 亿元,"[ 0.02362061  0.00986481  0.01558685 ... -0.00174713  0.0272522
 -0.01436615]",1,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[182, 184, 414, 199]",顶端新闻· 河南商报记者 丁亚菲 郭丁然 算力，作为新质生产力的代表，正在加速推动新一轮科技革命和产业变革。,"[ 0.05575562  0.06842041 -0.00413513 ... -0.01126099  0.01154327
 -0.00876617]",2,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[95, 215, 441, 229]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",3,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[77, 231, 519, 260]",9 月27 日~29 日，我国算力产业领域的顶级盛会，2024 中国算力大会在河南郑州举行。开幕 式上，中国算力服务平台（河南）正式启动。,"[ 0.02783203  0.03610229 -0.01934814 ...  0.01487732  0.02029419
 -0.0163269 ]",4,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[98, 262, 289, 276]",郑州规划算力全部建成后将位居全国前列,"[-0.00409317  0.02206421 -0.01174927 ... -0.00917053  0.01753235
 -0.00345802]",5,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[76, 278, 519, 323]",工业和信息化部总工程师赵志国出席 9 月 28 日举行的大会开幕式并致辞。他介绍，在算力 领域，产业发展提质增效向新，全国在用算力中心机架总规模超过 830 万标准机架，算力总规模 达 246EFLOPS（指每秒百亿亿次浮点运算次数，是算力指标之一），位居世界前列。,"[ 0.02957153  0.0657959   0.00829315 ...  0.01357269  0.02137756
 -0.04324341]",6,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[76, 325, 519, 385]",开幕式上，包括摩尔线程智能科技（北京）有限责任公司、华润数科控股有限公司、软通动 力信息技术（集团）股份有限公司、苏州艾利特机器人有限公司、深圳市超算力量科技有限公司、 上海商汤科技开发有限公司、华为技术有限公司、新华三技术有限公司等多家知名企业与河南多 地进行了重点项目签约。,"[ 0.02783203  0.04150391  0.02729797 ... -0.00933075  0.01395416
 -0.00417709]",7,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[74, 387, 519, 417]",会上，2024 中国算力大会签约项目 22 个，总金额 231.2 亿元。其中，框架协议 8 个，金额 90 亿元；合作协议 9 个，金额 99.2 亿元；战略协议 5 个，金额 42 亿元。,"[ 0.02629089  0.04537964 -0.00026965 ... -0.00391388  0.02839661
 -0.03982544]",8,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[76, 418, 519, 510]",顶端新闻· 河南商报记者在大会现场获悉，郑州算力基础设施加快完善，建成各类数据中心 38 个，全市算力规模约 6200P，高性能算力占比近四成，算力发展综合评价排名全国城市第 14 位。郑州规划算力规模 57EFLOPS，全部建成后算力排名将位居全国前列。算力产业链群不断壮 大，郑州市算法模型领域企业创新团队近 100 个，成功引育超聚变、中原鲲鹏生态创新中心、新 华三、阿帕斯等一批龙头企业。算力应用场景持续拓展，以“ 人工智能  赋能千行百业，建成 国内首条商业化自动驾驶公交线，智能网联车辆累计运营超过 200 万公里。,"[ 0.01730347  0.04971313 -0.01739502 ... -0.00839996  0.02970886
 -0.02145386]",9,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[97, 512, 310, 526]",河南算力产业基金发布，目标总规模 100 亿元 大会现场，河南算力产业基金正式发布。,[0.01661682 0.06536865 0.0276947  ... 0.00666809 0.0199585  0.02766418],10,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[98, 528, 282, 541]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",11,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[74, 543, 518, 573]",顶端新闻· 河南商报记者在现场了解到，河南算力产业基金由河南投资集团旗下河南创新投 资集团发起设立，该基金目标总规模 100 亿元。,"[ 0.03775024  0.05999756  0.00972748 ... -0.00384903  0.01591492
  0.03375244]",12,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[76, 574, 519, 619]",该基金由河南投资集团作为基石出资人，河南创新投资集团作为基金管理人，联合国家级基 金、省级引导基金、地市国有平台产业龙头和知名机构共同发起设立。河南算力产业基金是构建 涵盖直投基金、专项并购基金和产业母基金的基金群。,"[ 0.04998779  0.03515625 -0.00256157 ... -0.00072765  0.01693726
  0.04589844]",13,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[77, 621, 519, 666]",河南算力产业基金将聚焦算力产业链布局，重点投向数字基础设施、算力软硬件、算力运营、 人工智能大模型及垂类应用、具身智能机器人（就是外形类人的具有实时感知和交互能力的机器 人）等前沿科技领域企业。,"[ 0.02072144  0.05187988  0.01634216 ... -0.0254364   0.0480957
 -0.02809143]",14,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[98, 668, 257, 681]",人工智能与智算产业创新发展论坛 河南实施“   战略，全面推动数据工作创新发展,"[ 0.02919006  0.03939819 -0.00473785 ... -0.00203133  0.02285767
  0.0063858 ]",15,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[96, 683, 341, 697]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",16,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[76, 699, 520, 760]",27 日上午，在中国算力大会的会中会——人工智能与智算产业创新发展论坛举行，工业和信 息化部新闻宣传中心（人民邮电报社）总编辑王保平在致辞中表示，随着人工智能创新业态加速 产业化，将有越来越多的行业企业和机构实现智能化升级，让人工智能“   出无限可能。河南 省数据局党组书记、局长郑华卿就河南在数据要素市场化配置和改革要素流动方面的积极探索与,"[ 0.04391479  0.0682373  -0.00626755 ...  0.00592422  0.03900146
 -0.00634384]",17,,text,,,,,18,2,842.0,1,595.219970703125,src/resources/pdf/22个签约.pdf,
"[77, 72, 270, 86]",实践进行了汇报，并向参会企业发出邀请。,"[ 0.05700684  0.01399231 -0.00619507 ...  0.02105713  0.03491211
  0.01438141]",0,,text,,,,,13,2,842.0,2,595.219970703125,src/resources/pdf/22个签约.pdf,
"[97, 88, 349, 101]",“ 河南正在依托自身优势，积极融入算力产业大格局”,"[ 0.03448486  0.04559326 -0.01515961 ... -0.01611328  0.03482056
  0.02122498]",1,,text,,,,,13,2,842.0,2,595.219970703125,src/resources/pdf/22个签约.pdf,
"[77, 103, 518, 133]",据王保平介绍，根据中央网信办统计，截至今年 8 月，我国完成备案并上线，能为公众提供 服务的生成式人工智能服务大模型已达 190 多个。,"[ 0.00385666  0.0375061  -0.01901245 ...  0.00115395  0.0383606
 -0.05755615]",2,,text,,,,,13,2,842.0,2,595.219970703125,src/resources/pdf/22个签约.pdf,
"[75, 134, 518, 164]",人工智能的快速发展，离不开算力的强力支撑。当前，由于大模型、AIGC 等人工智能应用 的快速增长，算力总需求，特别是智算的需求呈现出急剧扩大的趋势。,"[ 0.05426025  0.05917358 -0.02709961 ... -0.00952148  0.02534485
 -0.05828857]",3,,text,,,,,13,2,842.0,2,595.219970703125,src/resources/pdf/22个签约.pdf,
"[76, 165, 519, 195]",目前，全国一体化算力体系正在加速推进，河南也将深度融入全国一体化算力体系建设，今 年计划投资 568 亿元，用于支持制算中心硬件设施建设、技术研发、应用推广等事项。,"[ 0.01463318  0.04888916 -0.01410675 ... -0.01398468  0.00930023
  0.00738907]",4,,text,,,,,13,2,842.0,2,595.219970703125,src/resources/pdf/22个签约.pdf,
"[76, 197, 518, 242]",河南交通便利，是连接华北、华东和西南地区的重要枢纽，具有得天独厚的区位优势。在经 济发展、工业升级、科技创新等方面，具有巨大的潜力和发展空间。“ 河南正在依托自身优势， 加速人工智能创新突破，积极融入算力产业大格局。” 王保平表示。,"[ 0.0586853   0.05963135  0.00335693 ... -0.01799011  0.03713989
  0.00052261]",5,,text,,,,,13,2,842.0,2,595.219970703125,src/resources/pdf/22个签约.pdf,
"[95, 244, 378, 258]",“ 我们会当好大家的服务员，非常欢迎大家来河南投资兴业”,"[ 0.01547241  0.05831909  0.01089478 ... -0.00478745  0.02633667
  0.04544067]",6,,text,,,,,13,2,842.0,2,595.219970703125,src/resources/pdf/22个签约.pdf,
"[77, 259, 520, 289]",郑华卿表示，希望专家和企业家们能够针对河南数据要素市场建设和数据产业发展，提出宝 贵意见和建议，“ 我们会当好大家的服务员，非常欢迎大家来河南投资兴业”。,"[ 0.01045227  0.04959106 -0.02023315 ...  0.00117302  0.03762817
  0.05725098]",7,,text,,,,,13,2,842.0,2,595.219970703125,src/resources/pdf/22个签约.pdf,
"[76, 290, 519, 335]",据悉，今年 1 月，河南省数据局挂牌成立以来，始终按照国家数据局的要求，致力于完成河 南数据工作框架方案的设计和编制工作。为此，河南省数据局提出了“   战略框架，以全面 推动数据工作的创新发展。,"[-0.00373268 -0.01945496  0.00554276 ... -0.00048041  0.02429199
  0.04541016]",8,,text,,,,,13,2,842.0,2,595.219970703125,src/resources/pdf/22个签约.pdf,
"[76, 337, 519, 367]",“   即一条主线，指的是数据要素的市场化配置改革。数据要素要流动起来，通过交易才 能产生价值。,"[ 0.00263596  0.00118637 -0.03881836 ...  0.00090265  0.02677917
 -0.01098633]",9,,text,,,,,13,2,842.0,2,595.219970703125,src/resources/pdf/22个签约.pdf,
"[76, 368, 519, 413]",“   代表三大建设，即数字河南、数字经济和数字社会建设。河南省数据局承担着完成这 三大领域规划及组织实施的重任。数字河南是国家数字中国战略的重要组成部分，数字经济涵盖 了产业数字化的方面，而数字社会则主要涉及民生领域的数字化改造。,"[ 0.01683044 -0.0124588  -0.01925659 ...  0.00766373  0.05780029
  0.05908203]",10,,text,,,,,13,2,842.0,2,595.219970703125,src/resources/pdf/22个签约.pdf,
"[96, 415, 510, 429]",“   是五大举措，包括政策规划、试点示范、项目带动、场景建设和数字基础设施建设。,"[ 0.02381897  0.01795959 -0.00426483 ...  0.00162411  0.05532837
  0.00616837]",11,,text,,,,,13,2,842.0,2,595.219970703125,src/resources/pdf/22个签约.pdf,
"[78, 431, 518, 461]",“   是指八项具体工作，包括数据基础制度、释放数据要素价值、数据基础设施建设、数 字产业化、产业数字化、数字科技和人才、智慧城市以及数据安全保障。,"[ 0.01864624 -0.00049639 -0.01856995 ...  0.03012085  0.04089355
  0.01123047]",12,,text,,,,,13,2,842.0,2,595.219970703125,src/resources/pdf/22个签约.pdf,
"[107, 466, 505, 703]",,"[-0.06646729 -0.01773071 -0.00894165 ... -0.0057869  -0.00957489
 -0.04055786]",0,,image,,Figure 1: Generated samples on CelebA-HQ  (left) and unconditional CIFAR10 (right),,src/resources/pdf/diffusion/auto/images/4af6d9fe5a77468d093254c5f88e4ca10d7c6bcca95899fd5b59c0a550e806f7.jpg,10,25,792.0,1,612.0,src/resources/pdf/diffusion.pdf,
"[153, 97, 459, 117]",Denoising Diffusion Probabilistic Models,"[ 0.0005064   0.05978394 -0.01420593 ...  0.01313019  0.01571655
 -0.00393677]",1,,text,,,,,10,25,792.0,1,612.0,src/resources/pdf/diffusion.pdf,
"[119, 159, 240, 193]",Jonathan Ho UC Berkeley jonathanho@berkeley.edu,"[ 0.04116821  0.06970215  0.00240326 ...  0.01316833 -0.00801849
 -0.01721191]",2,,text,,,,,10,25,792.0,1,612.0,src/resources/pdf/diffusion.pdf,
"[258, 160, 353, 193]",Ajay Jain UC Berkeley ajayj@berkeley.edu,"[ 0.04742432  0.06787109  0.00966644 ... -0.00259781 -0.0017767
  0.00775909]",3,,text,,,,,10,25,792.0,1,612.0,src/resources/pdf/diffusion.pdf,
"[369, 159, 492, 193]",Pieter Abbeel UC Berkeley pabbeel@cs.berkeley.edu,"[ 0.03994751  0.05856323 -0.00034904 ... -0.00284386  0.008461
 -0.03097534]",4,,text,,,,,10,25,792.0,1,612.0,src/resources/pdf/diffusion.pdf,
"[283, 221, 328, 234]",Abstract,"[-0.01850891  0.02378845 -0.01013184 ...  0.0120697  -0.02732849
  0.00043058]",5,,text,,,,,10,25,792.0,1,612.0,src/resources/pdf/diffusion.pdf,
"[143, 246, 470, 356]","We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models nat- urally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our imple- mentation is available at https://github.com/hojonathanho/diffusion.","[-0.03018188  0.04507446 -0.00867462 ...  0.02096558  0.01152039
 -0.01186371]",6,,text,,,,,10,25,792.0,1,612.0,src/resources/pdf/diffusion.pdf,
"[107, 376, 190, 390]",1 Introduction,"[ 0.02896118  0.05752563 -0.01829529 ...  0.02319336  0.03010559
  0.01599121]",7,,text,,,,,10,25,792.0,1,612.0,src/resources/pdf/diffusion.pdf,
"[107, 402, 505, 458]","Deep generative models of all kinds have recently exhibited high quality samples in a wide variety of data modalities. Generative adversarial networks (GANs), autoregressive models, flows, and variational autoencoders (VAEs) have synthesized striking image and audio samples [14, 27, 3, 58, 38, 25, 10, 32, 44, 57, 26, 33, 45], and there have been remarkable advances in energy-based modeling and score matching that have produced images comparable to those of GANs [11, 55].","[-0.03399658  0.02424622  0.03317261 ...  0.01612854  0.02127075
 -0.02000427]",8,,text,,,,,10,25,792.0,1,612.0,src/resources/pdf/diffusion.pdf,
"[106, 731, 459, 742]","34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.","[ 0.01979065  0.03741455  0.00015306 ... -0.00748825  0.05938721
  0.00162506]",9,,text,,,,,10,25,792.0,1,612.0,src/resources/pdf/diffusion.pdf,
"[155, 69, 456, 117]",,"[ 0.0282135  -0.02841187 -0.0065918  ... -0.00167942 -0.0335083
 -0.0254364 ]",0,,image,,Figure 2: The directed graphical model considered in this work.,,src/resources/pdf/diffusion/auto/images/cb3eca3a9fbbd1f7557d1c5e640396c28fe0000e39bb6fbb755230a0a5165246.jpg,13,25,792.0,2,612.0,src/resources/pdf/diffusion.pdf,
"[106, 140, 505, 218]","This paper presents progress in diffusion probabilistic models [53]. A diffusion probabilistic model (which we will call a “diffusion model” for brevity) is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time. Transitions of this chain are learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the data in the opposite direction of sampling until signal is destroyed. When the diffusion consists of small amounts of Gaussian noise, it is sufficient to set the sampling chain transitions to conditional Gaussians too, allowing for a particularly simple neural network parameterization.","[-0.0227356   0.02626038  0.02503967 ...  0.01049042  0.0136795
  0.00148678]",1,,text,,,,,13,25,792.0,2,612.0,src/resources/pdf/diffusion.pdf,
"[107, 222, 505, 310]","Diffusion models are straightforward to define and efficient to train, but to the best of our knowledge, there has been no demonstration that they are capable of generating high quality samples. We show that diffusion models actually are capable of generating high quality samples, sometimes better than the published results on other types of generative models (Section 4). In addition, we show that a certain parameterization of diffusion models reveals an equivalence with denoising score matching over multiple noise levels during training and with annealed Langevin dynamics during sampling (Section 3.2) [55, 61]. We obtained our best sample quality results using this parameterization (Section 4.2), so we consider this equivalence to be one of our primary contributions.","[-0.01795959  0.03442383  0.01133728 ...  0.03765869  0.00172997
 -0.02241516]",2,,text,,,,,13,25,792.0,2,612.0,src/resources/pdf/diffusion.pdf,
"[107, 315, 505, 403]","Despite their sample quality, our models do not have competitive log likelihoods compared to other likelihood-based models (our models do, however, have log likelihoods better than the large estimates annealed importance sampling has been reported to produce for energy based models and score matching [11, 55]). We find that the majority of our models’ lossless codelengths are consumed to describe imperceptible image details (Section 4.3). We present a more refined analysis of this phenomenon in the language of lossy compression, and we show that the sampling procedure of diffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit ordering that vastly generalizes what is normally possible with autoregressive models.","[-0.04150391  0.02510071 -0.01415253 ...  0.03128052  0.01078033
 -0.01657104]",3,,text,,,,,13,25,792.0,2,612.0,src/resources/pdf/diffusion.pdf,
"[107, 417, 189, 431]",2 Background,"[-0.02877808 -0.00377464 -0.00946045 ...  0.01934814 -0.00137997
 -0.00180531]",4,,text,,,,,13,25,792.0,2,612.0,src/resources/pdf/diffusion.pdf,
"[107, 441, 505, 486]","Diffusion models [53] are latent variable models of the form  , where  are latents of the same dimensionality as the data . The joint distribution  is called the reverse process, and it is defined as a Markov chain with learned Gaussian transitions starting at :","[-0.02285767  0.03216553  0.00651169 ... -0.00592041  0.00288582
  0.00737381]",5,,text,,,,,13,25,792.0,2,612.0,src/resources/pdf/diffusion.pdf,
"[122, 489, 477, 522]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",6,,text,,,,,13,25,792.0,2,612.0,src/resources/pdf/diffusion.pdf,
"[108, 524, 505, 557]","What distinguishes diffusion models from other types of latent variable models is that the approximate posterior , called the forward process or diffusion process, is fixed to a Markov chain that gradually adds Gaussian noise to the data according to a variance schedule :","[-0.02766418 -0.00287247  0.02583313 ... -0.01222229  0.01437378
  0.01060486]",7,,text,,,,,13,25,792.0,2,612.0,src/resources/pdf/diffusion.pdf,
"[149, 559, 462, 593]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",8,,text,,,,,13,25,792.0,2,612.0,src/resources/pdf/diffusion.pdf,
"[106, 599, 472, 612]",Training is performed by optimizing the usual variational bound on negative log likelihood:,"[ 0.01646423  0.03842163  0.03494263 ... -0.00856018  0.01773071
 -0.0007906 ]",9,,text,,,,,13,25,792.0,2,612.0,src/resources/pdf/diffusion.pdf,
"[110, 612, 490, 644]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",10,,text,,,,,13,25,792.0,2,612.0,src/resources/pdf/diffusion.pdf,
"[106, 646, 505, 704]","The forward process variances  can be learned by reparameterization [33] or held constant as hyperparameters, and expressiveness of the reverse process is ensured in part by the choice of Gaussian conditionals in , because both processes have the same functional form when  are small [53]. A notable property of the forward process is that it admits sampling  at an arbitrary timestep   in closed form: using the notation  and , we have","[ 0.00695419  0.01068878  0.00546646 ...  0.01933289  0.00361061
 -0.01148224]",11,,text,,,,,13,25,792.0,2,612.0,src/resources/pdf/diffusion.pdf,
"[228, 706, 383, 720]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",12,,text,,,,,13,25,792.0,2,612.0,src/resources/pdf/diffusion.pdf,
"[105, 72, 505, 95]",Efficient training is therefore possible by optimizing random terms of  with stochastic gradient descent. Further improvements come from variance reduction by rewriting  (3) as:,"[ 0.00249863  0.03744507  0.03125    ... -0.00918579  0.0112381
  0.01239014]",0,,text,,,,,15,25,792.0,3,612.0,src/resources/pdf/diffusion.pdf,
"[115, 96, 483, 133]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",1,,text,,,,,15,25,792.0,3,612.0,src/resources/pdf/diffusion.pdf,
"[108, 136, 504, 169]","(See Appendix A for details. The labels on the terms are used in Section 3.) Equation (5) uses KL divergence to directly compare  against forward process posteriors, which are tractable when conditioned on :","[ 0.03979492  0.00642776  0.02355957 ... -0.00010324 -0.00156307
 -0.00876617]",2,,text,,,,,15,25,792.0,3,612.0,src/resources/pdf/diffusion.pdf,
"[137, 169, 477, 215]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",3,,text,,,,,15,25,792.0,3,612.0,src/resources/pdf/diffusion.pdf,
"[106, 216, 506, 250]","Consequently, all KL divergences in Eq. (5) are comparisons between Gaussians, so they can be calculated in a Rao-Blackwellized fashion with closed form expressions instead of high variance Monte Carlo estimates.","[ 0.0076561   0.0075531   0.00595856 ... -0.03329468  0.02348328
 -0.02375793]",4,,text,,,,,15,25,792.0,3,612.0,src/resources/pdf/diffusion.pdf,
"[106, 264, 357, 279]",3 Diffusion models and denoising autoencoders,"[-0.00101566  0.07080078  0.0091629  ...  0.00699615  0.01408386
  0.0165863 ]",5,,text,,,,,15,25,792.0,3,612.0,src/resources/pdf/diffusion.pdf,
"[106, 288, 505, 366]","Diffusion models might appear to be a restricted class of latent variable models, but they allow a large number of degrees of freedom in implementation. One must choose the variances  of the forward process and the model architecture and Gaussian distribution parameterization of the reverse process. To guide our choices, we establish a new explicit connection between diffusion models and denoising score matching (Section 3.2) that leads to a simplified, weighted variational bound objective for diffusion models (Section 3.4). Ultimately, our model design is justified by simplicity and empirical results (Section 4). Our discussion is categorized by the terms of Eq. (5).","[-0.02194214  0.02941895  0.01889038 ...  0.01557922  0.01423645
  0.00658417]",6,,text,,,,,15,25,792.0,3,612.0,src/resources/pdf/diffusion.pdf,
"[107, 378, 235, 390]",3.1 Forward process and,"[ 0.04821777  0.0461731  -0.02232361 ...  0.02919006  0.05084229
 -0.03234863]",7,,text,,,,,15,25,792.0,3,612.0,src/resources/pdf/diffusion.pdf,
"[108, 398, 505, 432]","We ignore the fact that the forward process variances  are learnable by reparameterization and instead fix them to constants (see Section 4 for details). Thus, in our implementation, the approximate posterior  has no learnable parameters, so is a constant during training and can be ignored.","[ 0.01893616 -0.01296997  0.02987671 ...  0.00926971 -0.01418304
 -0.0194397 ]",8,,text,,,,,15,25,792.0,3,612.0,src/resources/pdf/diffusion.pdf,
"[107, 444, 247, 456]",3.2 Reverse process and,"[-0.00235748  0.05700684 -0.01609802 ...  0.03121948  0.03341675
 -0.02767944]",9,,text,,,,,15,25,792.0,3,612.0,src/resources/pdf/diffusion.pdf,
"[106, 464, 505, 534]","Now we discuss our choices in  for  . First, we set   to untrained time dependent constants. Experimentally, both  and  had similar results. The first choice is optimal for , and the second is optimal for  deterministically set to one point. These are the two extreme choices corresponding to upper and lower bounds on reverse process entropy for data with coordinatewise unit variance [53].","[-0.01239777  0.03433228  0.01879883 ... -0.01499176 -0.01158142
 -0.01856995]",10,,text,,,,,15,25,792.0,3,612.0,src/resources/pdf/diffusion.pdf,
"[106, 538, 504, 562]","Second, to represent the mean , we propose a specific parameterization motivated by the following analysis of . With , we can write:","[ 0.009758   -0.01157379 -0.00676346 ...  0.00736618 -0.00277328
 -0.0152359 ]",11,,text,,,,,15,25,792.0,3,612.0,src/resources/pdf/diffusion.pdf,
"[204, 565, 407, 592]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",12,,text,,,,,15,25,792.0,3,612.0,src/resources/pdf/diffusion.pdf,
"[106, 594, 505, 639]","where  is a constant that does not depend on . So, we see that the most straightforward parameteri- zation of  is a model that predicts , the forward process posterior mean. However, we can expand Eq. (8) further by reparameterizing Eq. (4) as  for  and applying the forward process posterior formula (7):","[ 0.0602417   0.01698303  0.00236511 ...  0.01173401 -0.03573608
 -0.05023193]",13,,text,,,,,15,25,792.0,3,612.0,src/resources/pdf/diffusion.pdf,
"[111, 641, 498, 722]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",14,,text,,,,,15,25,792.0,3,612.0,src/resources/pdf/diffusion.pdf,
"[107, 84, 196, 96]",Algorithm 1 Training,"[ 0.01792908  0.03210449 -0.02191162 ... -0.0100708  -0.001091
  0.00392914]",0,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[307, 84, 401, 96]",Algorithm 2 Sampling,"[-0.00434875 -0.00210762 -0.01976013 ...  0.027771    0.01196289
 -0.02815247]",1,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[109, 99, 284, 172]","1: repeat 2: x0 ∼ q(x0) 3: 4: 5: Take gradient descent step on 6: until converged 1:  2: for  do 3:  if , else 4: 5: end for 6: return x0","[ 0.01225281  0.00978088 -0.0016346  ... -0.021698   -0.00301361
  0.01661682]",2,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[308, 102, 493, 171]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",3,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[106, 182, 504, 211]","Equation (10) reveals that  must predict given . Since  is available as input to the model, we may choose the parameterization","[-0.01036835  0.01953125 -0.02139282 ...  0.01541901 -0.02301025
 -0.00489044]",4,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[123, 212, 471, 240]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",5,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[106, 241, 505, 293]","where  is a function approximator intended to predict  from . To sample  is to compute , where . The complete sampling procedure, Algorithm 2, resembles Langevin dynamics with  as a learned gradient of the data density. Furthermore, with the parameterization (11), Eq. (10) simplifies to:","[ 0.00553513  0.02359009  0.01302338 ...  0.00332451 -0.00323486
 -0.02165222]",6,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[186, 294, 425, 321]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",7,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[107, 322, 505, 366]","which resembles denoising score matching over multiple noise scales indexed by  [55]. As Eq. (12) is equal to (one term of) the variational bound for the Langevin-like reverse process (11), we see that optimizing an objective resembling denoising score matching is equivalent to using variational inference to fit the finite-time marginal of a sampling chain resembling Langevin dynamics.","[-0.01113129  0.01966858  0.04315186 ...  0.0026741  -0.01535034
 -0.05325317]",8,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[106, 371, 505, 448]","To summarize, we can train the reverse process mean function approximator  to predict , or by modifying its parameterization, we can train it to predict . (There is also the possibility of predicting , but we found this to lead to worse sample quality early in our experiments.) We have shown that the -prediction parameterization both resembles Langevin dynamics and simplifies the diffusion model’s variational bound to an objective that resembles denoising score matching. Nonetheless, it is just another parameterization of  , so we verify its effectiveness in Section 4 in an ablation where we compare predicting   aga−inst predicting .","[ 0.01232147  0.00583267 -0.00568008 ...  0.0275116  -0.01850891
 -0.02905273]",9,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[107, 460, 323, 472]","3.3 Data scaling, reverse process decoder, and","[ 0.01832581  0.03839111 -0.02264404 ...  0.03359985  0.01908875
 -0.04351807]",10,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[108, 479, 505, 524]","We assume that image data consists of integers in  scaled linearly to . This ensures that the neural network reverse process operates on consistently scaled inputs starting from the standard normal prior  . To obtain discrete log likelihoods, we set the last term of the reverse process to an independent discrete decoder derived from the Gaussian :","[-0.01299286  0.02056885 -0.00562668 ...  0.01846313 -0.04483032
  0.01509857]",11,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[159, 526, 361, 559]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",12,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[176, 562, 450, 590]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",13,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[106, 591, 505, 668]","where  is the data dimensionality and the  superscript indicates extraction of one coordinate. (It would be straightforward to instead incorporate a more powerful decoder like a conditional autoregressive model, but we leave that to future work.) Similar to the discretized continuous distributions used in VAE decoders and autoregressive models [34, 52], our choice here ensures that the variational bound is a lossless codelength of discrete data, without need of adding noise to the data or incorporating the Jacobian of the scaling operation into the log likelihood. At the end of sampling, we display   noiselessly.","[ 0.00014293  0.00501633  0.01921082 ...  0.00146294 -0.00328827
 -0.03851318]",14,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[108, 679, 251, 691]",3.4 Simplified training objective,"[ 0.0282135   0.03149414  0.00081825 ...  0.02481079  0.01332855
 -0.02980042]",15,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[107, 699, 503, 722]","With the reverse process and decoder defined above, the variational bound, consisting of terms derived from Eqs. (12) and (13), is clearly differentiable with respect to   and is ready to be employed for","[-0.00392151  0.01169586  0.01281738 ... -0.0028038  -0.00634384
 -0.02749634]",16,,text,,,,,17,25,792.0,4,612.0,src/resources/pdf/diffusion.pdf,
"[106, 82, 341, 261]",,"[-0.01577759 -0.01393127 -0.02098083 ...  0.00871277 -0.02033997
 -0.03173828]",0,,table,,Table 1: CIFAR10 results. NLL measured in bits/dim.,,src/resources/pdf/diffusion/auto/images/276fd69bf3e026da2c42480029fa84d856eea1bdc460b5bf58f96d282b7455df.jpg,11,25,792.0,5,612.0,src/resources/pdf/diffusion.pdf,
"[341, 141, 505, 242]",,"[ 0.02217102  0.03059387  0.03164673 ...  0.02349854 -0.0085144
 -0.00115681]",1,,table,,Table 2: Unconditional CIFAR10 reverse process parameterization and training objec- tive ablation. Blank entries were unstable to train and generated poor samples with out-of- range scores.,,src/resources/pdf/diffusion/auto/images/6733bd7444d8c614a00b4a498bf0150aafcdb724403769ef7f59ff5840648378.jpg,11,25,792.0,5,612.0,src/resources/pdf/diffusion.pdf,
"[105, 272, 504, 295]","training. However, we found it beneficial to sample quality (and simpler to implement) to train on the following variant of the variational bound:","[-0.00236702  0.0056572   0.00377274 ...  0.01284027  0.00091028
 -0.03564453]",2,,text,,,,,11,25,792.0,5,612.0,src/resources/pdf/diffusion.pdf,
"[187, 300, 424, 322]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",3,,text,,,,,11,25,792.0,5,612.0,src/resources/pdf/diffusion.pdf,
"[106, 327, 505, 394]","where  is uniform between 1 and . The  case corresponds to   with the integral in the discrete decoder definition (13) approximated by the Gaussian probability density function times the bin width, ignoring  and edge effects. The  cases correspond to an unweighted version of Eq. (12), analogous to the loss weighting used by the NCSN denoising score matching model [55]. 1  does not appear because the forward process variances  are fixed.) Algorithm 1 displays the complete training procedure with this simplified objective.","[ 0.00345612  0.01791382  0.00830841 ... -0.01164246  0.00206375
 -0.03207397]",4,,text,,,,,11,25,792.0,5,612.0,src/resources/pdf/diffusion.pdf,
"[106, 398, 505, 476]","Since our simplified objective (14) discards the weighting in Eq. (12), it is a weighted variational bound that emphasizes different aspects of reconstruction compared to the standard variational bound [18, 22]. In particular, our diffusion process setup in Section 4 causes the simplified objective to down-weight loss terms corresponding to small . These terms train the network to denoise data with very small amounts of noise, so it is beneficial to down-weight them so that the network can focus on more difficult denoising tasks at larger  terms. We will see in our experiments that this reweighting leads to better sample quality.","[ 0.00655746  0.00806427  0.01196289 ...  0.02813721 -0.02403259
 -0.0241394 ]",5,,text,,,,,11,25,792.0,5,612.0,src/resources/pdf/diffusion.pdf,
"[107, 492, 191, 506]",4 Experiments,"[-0.02713013  0.05657959 -0.03756714 ... -0.0177002  -0.0276947
 -0.00764084]",6,,text,,,,,11,25,792.0,5,612.0,src/resources/pdf/diffusion.pdf,
"[107, 517, 505, 584]","We set  for all experiments so that the number of neural network evaluations needed during sampling matches previous work [53, 55]. We set the forward process variances to constants increasing linearly from   to  . These constants were chosen to be small relative to data scaled to  , ensuring that reverse and forward processes have approximately the same functional form while keeping the signal-to-noise ratio at as small as possible (  bits per dimension in our experiments).","[ 0.02552795  0.01953125  0.03347778 ...  0.03851318 -0.02626038
 -0.00333023]",7,,text,,,,,11,25,792.0,5,612.0,src/resources/pdf/diffusion.pdf,
"[107, 588, 505, 633]","To represent the reverse process, we use a U-Net backbone similar to an unmasked Pixel  [52, 48] with group normalization throughout [66]. Parameters are shared across time, which is specified to the network using the Transformer sinusoidal position embedding [60]. We use self-attention at the  feature map resolution [63, 60]. Details are in Appendix B.","[-0.02476501  0.01485443 -0.00891876 ...  0.03677368 -0.01000214
  0.02963257]",8,,text,,,,,11,25,792.0,5,612.0,src/resources/pdf/diffusion.pdf,
"[107, 646, 194, 658]",4.1 Sample quality,"[-0.03120422  0.03863525 -0.03695679 ...  0.01228333 -0.00802612
 -0.05181885]",9,,text,,,,,11,25,792.0,5,612.0,src/resources/pdf/diffusion.pdf,
"[107, 667, 505, 722]","Table 1 shows Inception scores, FID scores, and negative log likelihoods (lossless codelengths) on CIFAR10. With our FID score of 3.17, our unconditional model achieves better sample quality than most models in the literature, including class conditional models. Our FID score is computed with respect to the training set, as is standard practice; when we compute it with respect to the test set, the score is 5.24, which is still better than many of the training set FID scores in the literature.","[-0.01739502  0.02253723 -0.01600647 ...  0.01690674 -0.0198822
 -0.01617432]",10,,text,,,,,11,25,792.0,5,612.0,src/resources/pdf/diffusion.pdf,
"[107, 71, 302, 201]",,"[-0.05743408 -0.03662109  0.02378845 ...  0.00279808 -0.01278687
  0.00577927]",0,,image,,Figure 3: LSUN Church samples.,,src/resources/pdf/diffusion/auto/images/15c23af5cfcb4c598ce83accf952931bb3231265c7ff5e3276bb2ef499af5c80.jpg,10,25,792.0,6,612.0,src/resources/pdf/diffusion.pdf,
"[307, 71, 501, 201]",,"[-0.03118896 -0.0758667  -0.00590897 ...  0.02671814 -0.01794434
 -0.01046753]",1,,image,,Figure 4: LSUN Bedroom samples.,,src/resources/pdf/diffusion/auto/images/8ce70bdb001cd692a2a72c3dbaa6ca83dbf520756b56b3935971e28af9a9065d.jpg,10,25,792.0,6,612.0,src/resources/pdf/diffusion.pdf,
"[107, 232, 507, 304]",,"[ 0.04342651 -0.01242828 -0.00096178 ... -0.03192139 -0.05609131
 -0.02519226]",2,,table,,,,src/resources/pdf/diffusion/auto/images/6956eb791b81ac7ee46be7d6e8c6b63cd08668c441aef94e33e4ed4e324fa59d.jpg,10,25,792.0,6,612.0,src/resources/pdf/diffusion.pdf,
"[106, 317, 505, 361]","We find that training our models on the true variational bound yields better codelengths than training on the simplified objective, as expected, but the latter yields the best sample quality. See Fig. 1 for CIFAR10 and CelebA-HQ  samples, Fig. 3 and Fig. 4 for LSUN  samples [71], and Appendix D for more.","[-0.03111267  0.03765869 -0.01064301 ...  0.02613831 -0.00479507
 -0.02130127]",3,,text,,,,,10,25,792.0,6,612.0,src/resources/pdf/diffusion.pdf,
"[107, 375, 407, 387]",4.2 Reverse process parameterization and training objective ablation,"[ 0.03936768  0.05761719  0.02261353 ...  0.0385437  -0.00971222
 -0.02308655]",4,,text,,,,,10,25,792.0,6,612.0,src/resources/pdf/diffusion.pdf,
"[107, 395, 505, 483]","In Table 2, we show the sample quality effects of reverse process parameterizations and training objectives (Section 3.2). We find that the baseline option of predicting  works well only when trained on the true variational bound instead of unweighted mean squared error, a simplified objective akin to Eq. (14). We also see that learning reverse process variances (by incorporating a parameterized diagonal   into the variational bound) leads to unstable training and poorer sample quality compared to fixed variances. Predicting , as we proposed, performs approximately as well as predicting  when trained on the variational bound with fixed variances, but much better when trained with our simplified objective.","[-0.00090599  0.02346802  0.01530457 ...  0.02836609 -0.01676941
 -0.02194214]",5,,text,,,,,10,25,792.0,6,612.0,src/resources/pdf/diffusion.pdf,
"[107, 497, 210, 509]",4.3 Progressive coding,"[ 0.01319122  0.02891541 -0.01025391 ...  0.00173092  0.0319519
  0.01428986]",6,,text,,,,,10,25,792.0,6,612.0,src/resources/pdf/diffusion.pdf,
"[107, 517, 505, 584]","Table 1 also shows the codelengths of our CIFAR10 models. The gap between train and test is at most 0.03 bits per dimension, which is comparable to the gaps reported with other likelihood-based models and indicates that our diffusion model is not overfitting (see Appendix D for nearest neighbor visualizations). Still, while our lossless codelengths are better than the large estimates reported for energy based models and score matching using annealed importance sampling [11], they are not competitive with other types of likelihood-based generative models [7].","[-0.02574158  0.02719116  0.00133801 ...  0.04248047 -0.00983429
 -0.00854492]",7,,text,,,,,10,25,792.0,6,612.0,src/resources/pdf/diffusion.pdf,
"[107, 588, 505, 644]","Since our samples are nonetheless of high quality, we conclude that diffusion models have an inductive bias that makes them excellent lossy compressors. Treating the variational bound terms as rate and  as distortion, our CIFAR10 model with the highest quality samples has a rate of 1.78 bits/dim and a distortion of 1.97 bits/dim, which amounts to a root mean squared error of 0.95 on a scale from 0 to 255. More than half of the lossless codelength describes imperceptible distortions.","[-0.00568008  0.06860352 -0.01006317 ...  0.02670288 -0.00065231
  0.00745773]",8,,text,,,,,10,25,792.0,6,612.0,src/resources/pdf/diffusion.pdf,
"[107, 656, 505, 722]","Progressive lossy compression We can probe further into the rate-distortion behavior of our model by introducing a progressive lossy code that mirrors the form of Eq. (5): see Algorithms 3 and 4, which assume access to a procedure, such as minimal random coding [19, 20], that can transmit a sample  using approximately  bits on average for any distributions  and , for which only  is available to the receiver beforehand. When applied to , Algorithms 3 and 4 transmit  in sequence using a total expected codelength equal to Eq. (5). The receiver, at any time , has the partial information  fully available and can progressively estimate:","[-0.00843048  0.01340485  0.00494003 ... -0.008461    0.00813293
 -0.02919006]",9,,text,,,,,10,25,792.0,6,612.0,src/resources/pdf/diffusion.pdf,
"[108, 188, 501, 282]",,"[ 0.02441406  0.00259018 -0.00669479 ...  0.01311493  0.02285767
 -0.02552795]",0,,image,,"Figure 5: Unconditional CIFAR10 test set rate-distortion vs. time. Distortion is measured in root mean squared error on a [0, 255] scale. See Table 4 for details.",,src/resources/pdf/diffusion/auto/images/fe6bb8288119f7f0abb457b9cdaca487100cf8af05e854000093a983c43a1943.jpg,10,25,792.0,7,612.0,src/resources/pdf/diffusion.pdf,
"[126, 407, 485, 480]",,"[-0.02903748  0.00933838  0.00836182 ...  0.01690674 -0.02388
  0.00384521]",1,,image,,"Figure 6: Unconditional CIFAR10 progressive generation   over time, from left to right). Extended samples and sample quality metrics over time in the appendix (Figs. 10 and 14).",,src/resources/pdf/diffusion/auto/images/ca035afd3363566145bdbdeb1cc3f9bf3d38144520e21fe6bb2b7e9ac93af7b8.jpg,10,25,792.0,7,612.0,src/resources/pdf/diffusion.pdf,
"[145, 515, 465, 585]",,"[-0.03024292 -0.00883484  0.01533508 ...  0.0105896  -0.02502441
 -0.02742004]",2,,image,,"Figure 7: When conditioned on the same latent, CelebA-HQ  samples share high-level attributes. Bottom-right quadrants are , and other quadrants are samples from .",,src/resources/pdf/diffusion/auto/images/5afabf59277a1d51113de0cb4633e3a3e4e4f8d4b2f4cfed2145fd90ec7136c8.jpg,10,25,792.0,7,612.0,src/resources/pdf/diffusion.pdf,
"[104, 72, 468, 84]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",3,,text,,,,,10,25,792.0,7,612.0,src/resources/pdf/diffusion.pdf,
"[221, 90, 390, 105]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",4,,text,,,,,10,25,792.0,7,612.0,src/resources/pdf/diffusion.pdf,
"[106, 111, 506, 179]","due to Eq. (4). (A stochastic reconstruction  is also valid, but we do not consider it here because it makes distortion more difficult to evaluate.) Figure 5 shows the resulting rate- distortion plot on the CIFAR10 test set. At each time , the distortion is calculated as the root mean squared error , and the rate is calculated as the cumulative number of bits received so far at time . The distortion decreases steeply in the low-rate region of the rate-distortion plot, indicating that the majority of the bits are indeed allocated to imperceptible distortions.","[-0.01029968  0.01681519  0.00811005 ... -0.03170776 -0.03720093
 -0.02088928]",5,,text,,,,,10,25,792.0,7,612.0,src/resources/pdf/diffusion.pdf,
"[106, 320, 505, 397]","Progressive generation We also run a progressive unconditional generation process given by progressive decompression from random bits. In other words, we predict the result of the reverse process, , while sampling from the reverse process using Algorithm 2. Figures 6 and 10 show the resulting sample quality of  over the course of the reverse process. Large scale image features appear first and details appear last. Figure 7 shows stochastic predictions  with frozen for various . When  is small, all but fine details are preserved, and when  is large, only large scale features are preserved. Perhaps these are hints of conceptual compression [18].","[-0.02511597  0.00891876 -0.00571442 ...  0.01468658 -0.00486755
 -0.04144287]",6,,text,,,,,10,25,792.0,7,612.0,src/resources/pdf/diffusion.pdf,
"[108, 621, 498, 633]",Connection to autoregressive decoding Note that the variational bound (5) can be rewritten as:,"[-0.02363586  0.03723145  0.02780151 ... -0.01412201  0.03790283
 -0.03335571]",7,,text,,,,,10,25,792.0,7,612.0,src/resources/pdf/diffusion.pdf,
"[129, 637, 465, 673]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",8,,text,,,,,10,25,792.0,7,612.0,src/resources/pdf/diffusion.pdf,
"[107, 677, 504, 723]","(See Appendix A for a derivation.) Now consider setting the diffusion process length   to the dimensionality of the data, defining the forward process so that  places all probability mass on  with the first  coordinates masked out (i.e.   masks out the  coordinate), setting  to place all mass on a blank image, and, for the sake of argument, taking   to be a fully expressive conditional distribution. With these choices, , and minimizing  trains  to copy coordinates  unchanged and to predict the  coordinate given . Thus, training  with this particular diffusion is training an autoregressive model.","[-0.00114727  0.0340271  -0.01345062 ...  0.0154953   0.0196991
 -0.00956726]",9,,text,,,,,10,25,792.0,7,612.0,src/resources/pdf/diffusion.pdf,
"[106, 70, 505, 146]",,"[-0.03094482  0.01147461 -0.01148224 ... -0.01887512 -0.01277161
 -0.04818726]",0,,image,,Figure 8: Interpolations of CelebA-HQ 256x256 images with 500 timesteps of diffusio,,src/resources/pdf/diffusion/auto/images/668c787f7112e4aa60d86a8cf72f6f2d0e061025f229f0a4611ea4592982849a.jpg,8,25,792.0,8,612.0,src/resources/pdf/diffusion.pdf,
"[107, 182, 505, 226]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",1,,text,,,,,8,25,792.0,8,612.0,src/resources/pdf/diffusion.pdf,
"[107, 232, 506, 320]","We can therefore interpret the Gaussian diffusion model (2) as a kind of autoregressive model with a generalized bit ordering that cannot be expressed by reordering data coordinates. Prior work has shown that such reorderings introduce inductive biases that have an impact on sample quality [38], so we speculate that the Gaussian diffusion serves a similar purpose, perhaps to greater effect since Gaussian noise might be more natural to add to images compared to masking noise. Moreover, the Gaussian diffusion length is not restricted to equal the data dimension; for instance, we use , which is less than the dimension of the  or  images in our experiments. Gaussian diffusions can be made shorter for fast sampling or longer for model expressiveness.","[-0.02154541 -0.006073   -0.0234375  ...  0.01099396 -0.00371361
 -0.00302887]",2,,text,,,,,8,25,792.0,8,612.0,src/resources/pdf/diffusion.pdf,
"[107, 342, 187, 353]",4.4 Interpolation,"[-0.0181427   0.01873779  0.0176239  ... -0.01693726  0.0178833
  0.00077581]",3,,text,,,,,8,25,792.0,8,612.0,src/resources/pdf/diffusion.pdf,
"[107, 365, 506, 465]","We can interpolate source images  in latent space using  as a stochastic encoder, , then decoding the linearly interpolated latent  into image space by the reverse process,  . In effect, we use the reverse process to remove artifacts from linearly interpolating corrupted versions of the source images, as depicted in Fig. 8 (left). We fixed the noise for different values of  so  and  remain the same. Fig. 8 (right) shows interpolations and reconstructions of original CelebA-HQ  images  ). The reverse process produces high-quality reconstructions, and plausible interpolations that smoothly vary attributes such as pose, skin tone, hairstyle, expression and background, but not eyewear. Larger  results in coarser and more varied interpolations, with novel samples at  (Appendix Fig. 9).","[-0.03204346  0.03387451  0.00917816 ... -0.00270271 -0.00935364
 -0.03869629]",4,,text,,,,,8,25,792.0,8,612.0,src/resources/pdf/diffusion.pdf,
"[107, 489, 197, 502]",5 Related Work,"[ 0.00798035  0.00577927 -0.01087189 ... -0.03182983  0.01849365
  0.01280212]",5,,text,,,,,8,25,792.0,8,612.0,src/resources/pdf/diffusion.pdf,
"[107, 519, 505, 640]","While diffusion models might resemble flows [9, 46, 10, 32, 5, 16, 23] and VAEs [33, 47, 37], diffusion models are designed so that  has no parameters and the top-level latent  has nearly zero mutual information with the data . Our -prediction reverse process parameterization establishes a connection between diffusion models and denoising score matching over multiple noise levels with annealed Langevin dynamics for sampling [55, 56]. Diffusion models, however, admit straightforward log likelihood evaluation, and the training procedure explicitly trains the Langevin dynamics sampler using variational inference (see Appendix C for details). The connection also has the reverse implication that a certain weighted form of denoising score matching is the same as variational inference to train a Langevin-like sampler. Other methods for learning transition operators of Markov chains include infusion training [2], variational walkback [15], generative stochastic networks [1], and others [50, 54, 36, 42, 35, 65].","[ 0.00653839  0.01560974  0.01698303 ...  0.02668762 -0.00860596
 -0.03240967]",6,,text,,,,,8,25,792.0,8,612.0,src/resources/pdf/diffusion.pdf,
"[107, 645, 505, 722]","By the known connection between score matching and energy-based modeling, our work could have implications for other recent work on energy-based models [67–69, 12, 70, 13, 11, 41, 17, 8]. Our rate-distortion curves are computed over time in one evaluation of the variational bound, reminiscent of how rate-distortion curves can be computed over distortion penalties in one run of annealed importance sampling [24]. Our progressive decoding argument can be seen in convolutional DRAW and related models [18, 40] and may also lead to more general designs for subscale orderings or sampling strategies for autoregressive models [38, 64].","[-0.00242615 -0.00546646  0.02857971 ...  0.01899719  0.00624847
 -0.02313232]",7,,text,,,,,8,25,792.0,8,612.0,src/resources/pdf/diffusion.pdf,
"[107, 71, 183, 84]",6 Conclusion,"[ 0.00703812  0.05209351 -0.02365112 ... -0.00234413  0.00170612
 -0.01504517]",0,,text,,,,,10,25,792.0,9,612.0,src/resources/pdf/diffusion.pdf,
"[107, 96, 505, 162]","We have presented high quality image samples using diffusion models, and we have found connections among diffusion models and variational inference for training Markov chains, denoising score matching and annealed Langevin dynamics (and energy-based models by extension), autoregressive models, and progressive lossy compression. Since diffusion models seem to have excellent inductive biases for image data, we look forward to investigating their utility in other data modalities and as components in other types of generative models and machine learning systems.","[-0.02015686  0.03329468 -0.01315308 ...  0.0252533   0.01881409
  0.01335144]",1,,text,,,,,10,25,792.0,9,612.0,src/resources/pdf/diffusion.pdf,
"[107, 178, 190, 192]",Broader Impact,"[-0.03619385  0.0335083  -0.0054512  ... -0.00123119 -0.02102661
  0.00759506]",2,,text,,,,,10,25,792.0,9,612.0,src/resources/pdf/diffusion.pdf,
"[107, 204, 505, 259]","Our work on diffusion models takes on a similar scope as existing work on other types of deep generative models, such as efforts to improve the sample quality of GANs, flows, autoregressive models, and so forth. Our paper represents progress in making diffusion models a generally useful tool in this family of techniques, so it may serve to amplify any impacts that generative models have had (and will have) on the broader world.","[-0.01277161 -0.00346184 -0.0241394  ...  0.01617432  0.00776672
 -0.02171326]",3,,text,,,,,10,25,792.0,9,612.0,src/resources/pdf/diffusion.pdf,
"[107, 264, 505, 373]","Unfortunately, there are numerous well-known malicious uses of generative models. Sample gen- eration techniques can be employed to produce fake images and videos of high profile figures for political purposes. While fake images were manually created long before software tools were avail- able, generative models such as ours make the process easier. Fortunately, CNN-generated images currently have subtle flaws that allow detection [62], but improvements in generative models may make this more difficult. Generative models also reflect the biases in the datasets on which they are trained. As many large datasets are collected from the internet by automated systems, it can be difficult to remove these biases, especially when the images are unlabeled. If samples from generative models trained on these datasets proliferate throughout the internet, then these biases will only be reinforced further.","[-0.03970337  0.01902771 -0.02662659 ...  0.01663208 -0.00361443
 -0.02384949]",4,,text,,,,,10,25,792.0,9,612.0,src/resources/pdf/diffusion.pdf,
"[107, 379, 505, 434]","On the other hand, diffusion models may be useful for data compression, which, as data becomes higher resolution and as global internet traffic increases, might be crucial to ensure accessibility of the internet to wide audiences. Our work might contribute to representation learning on unlabeled raw data for a large range of downstream tasks, from image classification to reinforcement learning, and diffusion models might also become viable for creative uses in art, photography, and music.","[ 0.00076342  0.02565002 -0.04464722 ... -0.00089645  0.00345993
  0.00886536]",5,,text,,,,,10,25,792.0,9,612.0,src/resources/pdf/diffusion.pdf,
"[107, 450, 339, 464]",Acknowledgments and Disclosure of Funding,"[ 0.03125     0.0680542  -0.0030365  ... -0.02807617  0.02615356
 -0.02227783]",6,,text,,,,,10,25,792.0,9,612.0,src/resources/pdf/diffusion.pdf,
"[107, 475, 504, 498]",This work was supported by ONR PECASE and the NSF Graduate Research Fellowship under grant number DGE-1752814. Google’s TensorFlow Research Cloud (TFRC) provided Cloud TPUs.,"[ 0.04089355  0.05691528  0.01881409 ...  0.01600647  0.04089355
 -0.00635529]",7,,text,,,,,10,25,792.0,9,612.0,src/resources/pdf/diffusion.pdf,
"[107, 514, 163, 526]",References,"[-0.00434494  0.04223633 -0.02407837 ... -0.01483154  0.02075195
  0.00366783]",8,,text,,,,,10,25,792.0,9,612.0,src/resources/pdf/diffusion.pdf,
"[110, 531, 506, 722]","[1] Guillaume Alain, Yoshua Bengio, Li Yao, Jason Yosinski, Eric Thibodeau-Laufer, Saizheng Zhang, and Pascal Vincent. GSNs: generative stochastic networks. Information and Inference: A Journal of the IMA, 5(2):210–249, 2016. [2] Florian Bordes, Sina Honari, and Pascal Vincent. Learning to generate samples from noise through infusion training. In International Conference on Learning Representations, 2017. [3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2019. [4] Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and Yoshua Bengio. Your GAN is secretly an energy-based model and you should use discriminator driven latent sampling. arXiv preprint arXiv:2003.06060, 2020. [5] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In Advances in Neural Information Processing Systems, pages 6571–6583, 2018. [6] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An improved autoregres- sive generative model. In International Conference on Machine Learning, pages 863–871, 2018. [7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [8] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc’Aurelio Ranzato. Residual energy-based models for text generation. arXiv preprint arXiv:2004.11714, 2020. [9] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014. [10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv preprint arXiv:1605.08803, 2016. [11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In Advances in Neural Information Processing Systems, pages 3603–3613, 2019. [12] Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian Wu. Learning generative ConvNets via multi-grid modeling and sampling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9155–9164, 2018. [13] Ruiqi Gao, Erik Nijkamp, Diederik P Kingma, Zhen Xu, Andrew M Dai, and Ying Nian Wu. Flow contrastive estimation of energy-based models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7518–7528, 2020. [14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672–2680, 2014. [15] Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback: Learning a transition operator as a stochastic recurrent net. In Advances in Neural Information Processing Systems, pages 4392–4402, 2017. [16] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. FFJORD: Free-form continuous dynamics for scalable reversible generative models. In International Conference on Learning Representations, 2019. [17] Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. In International Conference on Learning Representations, 2020. [18] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. In Advances In Neural Information Processing Systems, pages 3549–3557, 2016. [19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity (CCC’07), pages 10–23. IEEE, 2007. [20] Marton Havasi, Robert Peharz, and José Miguel Hernández-Lobato. Minimal random code learning: Getting bits back from compressed model parameters. In International Conference on Learning Represen- tations, 2019. [21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems, pages 6626–6637, 2017. [22] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mo- hamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017. [23] Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow : Improving flow-based generative models with variational dequantization and architecture design. In International Conference on Machine Learning, 2019. [24] Sicong Huang, Alireza Makhzani, Yanshuai Cao, and Roger Grosse. Evaluating lossy compression rates of deep generative models. In International Conference on Machine Learning, 2020. [25] Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Video pixel networks. In International Conference on Machine Learning, pages 1771–1779, 2017. [26] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural audio synthesis. In International Conference on Machine Learning, pages 2410–2419, 2018. [27] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018. [28] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4401–4410, 2019. [29] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. arXiv preprint arXiv:2006.06676v1, 2020. [30] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8110–8119, 2020. [31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. [32] Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In Advances in Neural Information Processing Systems, pages 10215–10224, 2018. [33] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013. [34] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In Advances in Neural Information Processing Systems, pages 4743–4751, 2016. [35] John Lawson, George Tucker, Bo Dai, and Rajesh Ranganath. Energy-inspired models: Learning with sampler-induced distributions. In Advances in Neural Information Processing Systems, pages 8501–8513, 2019. [36] Daniel Levy, Matt D. Hoffman, and Jascha Sohl-Dickstein. Generalizing Hamiltonian Monte Carlo with neural networks. In International Conference on Learning Representations, 2018. [37] Lars Maaløe, Marco Fraccaro, Valentin Liévin, and Ole Winther. BIVA: A very deep hierarchy of latent variables for generative modeling. In Advances in Neural Information Processing Systems, pages 6548–6558, 2019. [38] Jacob Menick and Nal Kalchbrenner. Generating high fidelity images with subscale pixel networks and multidimensional upscaling. In International Conference on Learning Representations, 2019. [39] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018. [40] Alex Nichol. VQ-DRAW: A sequential discrete VAE. arXiv preprint arXiv:2003.01599, 2020. [41] Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy of MCMC-based maximum likelihood learning of energy-based models. arXiv preprint arXiv:1903.12370, 2019. [42] Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-persistent short-run MCMC toward energy-based model. In Advances in Neural Information Processing Systems, pages 5233–5243, 2019. [43] Georg Ostrovski, Will Dabney, and Remi Munos. Autoregressive quantile networks for generative modeling. In International Conference on Machine Learning, pages 3936–3945, 2018. [44] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. WaveGlow: A flow-based generative network for speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3617–3621. IEEE, 2019. [45] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ- VAE-2. In Advances in Neural Information Processing Systems, pages 14837–14847, 2019. [46] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International Conference on Machine Learning, pages 1530–1538, 2015. [47] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approx- imate inference in deep generative models. In International Conference on Machine Learning, pages 1278–1286, 2014. [48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 234–241. Springer, 2015. [49] Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, pages 901–909, 2016. [50] Tim Salimans, Diederik Kingma, and Max Welling. Markov Chain Monte Carlo and variational inference: Bridging the gap. In International Conference on Machine Learning, pages 1218–1226, 2015. [51] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pages 2234–2242, 2016. [52] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN : Improving the PixelCNN with discretized logistic mixture likelihood and other modifications. In International Conference on Learning Representations, 2017. [53] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256–2265, 2015. [54] Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-NICE-MC: Adversarial training for MCMC. In Advances in Neural Information Processing Systems, pages 5140–5150, 2017. [55] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, pages 11895–11907, 2019. [56] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. arXiv preprint arXiv:2006.09011, 2020. [57] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016. [58] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. International Conference on Machine Learning, 2016. [59] Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional image generation with PixelCNN decoders. In Advances in Neural Information Processing Systems, pages 4790–4798, 2016. [60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998–6008, 2017. [61] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23(7):1661–1674, 2011. [62] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated images are surprisingly easy to spot...for now. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020. [63] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7794–7803, 2018. [64] Auke J Wiggers and Emiel Hoogeboom. Predictive sampling with forecasting autoregressive models. arXiv preprint arXiv:2002.09928, 2020. [65] Hao Wu, Jonas Köhler, and Frank Noé. Stochastic normalizing flows. arXiv preprint arXiv:2002.06707, 2020. [66] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer Vision (ECCV), pages 3–19, 2018. [67] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In International Conference on Machine Learning, pages 2635–2644, 2016. [68] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Synthesizing dynamic patterns by spatial-temporal generative convnet. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7093–7101, 2017. [69] Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying Nian Wu. Learning descriptor networks for 3d shape synthesis and analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8629–8638, 2018. [70] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Learning energy-based spatial-temporal generative convnets for dynamic patterns. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019. [71] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. [72] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.","[-0.00862122  0.00413513 -0.016922   ... -0.01520538  0.01000977
  0.03004456]",9,,text,,,,,10,25,792.0,9,612.0,src/resources/pdf/diffusion.pdf,
"[105, 44, 507, 728]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,1,25,792.0,10,612.0,src/resources/pdf/diffusion.pdf,
"[105, 40, 508, 716]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,1,25,792.0,11,612.0,src/resources/pdf/diffusion.pdf,
"[105, 54, 507, 719]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,1,25,792.0,12,612.0,src/resources/pdf/diffusion.pdf,
"[159, 152, 452, 236]",,"[-0.01673889 -0.06155396 -0.02737427 ...  0.03451538 -0.04226685
 -0.03933716]",0,,table,,Table 3: FID scores for LSUN  datasets,,src/resources/pdf/diffusion/auto/images/8a55f29096e40ffdc551f43217935bfbaa82e5f1db8fbc84bfc12eb70f16eb37.jpg,8,25,792.0,13,612.0,src/resources/pdf/diffusion.pdf,
"[135, 337, 476, 469]",,"[ 0.00090313 -0.00917053 -0.00104523 ...  0.00316238  0.00810242
 -0.01792908]",1,,table,,Table 4: Unconditional CIFAR10 test set rate-distortion values (accompanies Fig. 5),,src/resources/pdf/diffusion/auto/images/7bd3bceb9ca8c15e99a94491771ee55d19dfbfeba66db5b0caf8509ddb75f52d.jpg,8,25,792.0,13,612.0,src/resources/pdf/diffusion.pdf,
"[107, 71, 201, 84]",Extra information,"[-0.01745605 -0.01596069  0.02804565 ... -0.01194    -0.01860046
 -0.03118896]",2,,text,,,,,8,25,792.0,13,612.0,src/resources/pdf/diffusion.pdf,
"[105, 97, 505, 120]","LSUN FID scores for LSUN datasets are included in Table 3. Scores marked with ∗ are reported by StyleGAN2 as baselines, and other scores are reported by their respective authors.","[ 0.00407028 -0.04144287 -0.01341248 ...  0.01070404 -0.02279663
 -0.02127075]",3,,text,,,,,8,25,792.0,13,612.0,src/resources/pdf/diffusion.pdf,
"[106, 259, 506, 304]","Progressive compression Our lossy compression argument in Section 4.3 is only a proof of concept, because Algorithms 3 and 4 depend on a procedure such as minimal random coding [20], which is not tractable for high dimensional data. These algorithms serve as a compression interpretation of the variational bound (5) of Sohl-Dickstein et al. [53], not yet as a practical compression system.","[-0.01084137  0.01661682 -0.01062012 ...  0.00522995 -0.00092411
  0.02246094]",4,,text,,,,,8,25,792.0,13,612.0,src/resources/pdf/diffusion.pdf,
"[107, 497, 237, 510]",A Extended derivations,"[-0.02308655  0.02262878  0.00293159 ...  0.00379372  0.02297974
  0.02313232]",5,,text,,,,,8,25,792.0,13,612.0,src/resources/pdf/diffusion.pdf,
"[107, 523, 504, 547]","Below is a derivation of Eq. (5), the reduced variance variational bound for diffusion models. This material is from Sohl-Dickstein et al. [53]; we include it here only for completeness.","[-0.03271484  0.01345825  0.02487183 ... -0.01435089 -0.021698
 -0.00310326]",6,,text,,,,,8,25,792.0,13,612.0,src/resources/pdf/diffusion.pdf,
"[109, 554, 436, 727]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",7,,text,,,,,8,25,792.0,13,612.0,src/resources/pdf/diffusion.pdf,
"[119, 69, 499, 103]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,8,25,792.0,14,612.0,src/resources/pdf/diffusion.pdf,
"[108, 119, 505, 142]","The following is an alternate version of  . It is not tractable to estimate, but it is useful for our discussion in Section 4.3.","[ 0.00301743 -0.01248169 -0.00328827 ...  0.00154972  0.00967407
 -0.03031921]",1,,text,,,,,8,25,792.0,14,612.0,src/resources/pdf/diffusion.pdf,
"[129, 145, 464, 305]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",2,,text,,,,,8,25,792.0,14,612.0,src/resources/pdf/diffusion.pdf,
"[106, 315, 234, 329]",B Experimental details,"[-0.02400208  0.05187988  0.00242424 ... -0.00686646 -0.03076172
  0.00513077]",3,,text,,,,,8,25,792.0,14,612.0,src/resources/pdf/diffusion.pdf,
"[106, 340, 505, 440]","Our neural network architecture follows the backbone of   [52], which is a U-Net [48] based on a Wide ResNet [72]. We replaced weight normalization [49] with group normalization [66] to make the implementation simpler. Our  models use four feature map resolutions  to ), and our  models use six. All models have two convolutional residual blocks per resolution level and self-attention blocks at the  resolution between the convolutional blocks [6]. Diffusion time  is specified by adding the Transformer sinusoidal position embedding [60] into each residual block. Our CIFAR10 model has 35.7 million parameters, and our LSUN and CelebA-HQ models have 114 million parameters. We also trained a larger variant of the LSUN Bedroom model with approximately 256 million parameters by increasing filter count.","[-0.02565002 -0.01785278 -0.02438354 ...  0.03210449 -0.02510071
 -0.00948334]",4,,text,,,,,8,25,792.0,14,612.0,src/resources/pdf/diffusion.pdf,
"[107, 443, 505, 510]","We used TPU v3-8 (similar to 8 V100 GPUs) for all experiments. Our CIFAR model trains at 21 steps per second at batch size 128 (10.6 hours to train to completion at  steps), and sampling a batch of 256 images takes 17 seconds. Our CelebA-HQ/LSUN ) models train at 2.2 steps per second at batch size 64, and sampling a batch of 128 images takes 300 seconds. We trained on CelebA-HQ for  steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN Church for 1.2M steps. The larger LSUN Bedroom model was trained for 1.15M steps.","[ 0.00865173  0.00240135 -0.01442719 ...  0.0092926  -0.0043602
 -0.00577164]",5,,text,,,,,8,25,792.0,14,612.0,src/resources/pdf/diffusion.pdf,
"[107, 515, 505, 548]","Apart from an initial choice of hyperparameters early on to make network size fit within memory constraints, we performed the majority of our hyperparameter search to optimize for CIFAR10 sample quality, then transferred the resulting settings over to the other datasets:","[ 0.00975037  0.0016346  -0.02203369 ...  0.00522614 -0.00110435
 -0.00497437]",6,,text,,,,,8,25,792.0,14,612.0,src/resources/pdf/diffusion.pdf,
"[133, 557, 505, 722]","• We chose the   schedule from a set of constant, linear, and quadratic schedules, all constrained so that  . We set  without a sweep, and we chose a linear schedule from  to . • We set the dropout rate on CIFAR10 to 0.1 by sweeping over the values . Without dropout on CIFAR10, we obtained poorer samples reminiscent of the overfitting artifacts in an unregularized Pixe  [52]. We set dropout rate on the other datasets to zero without sweeping. • We used random horizontal flips during training for CIFAR10; we tried training both with and without flips, and found flips to improve sample quality slightly. We also used random horizontal flips for all other datasets except LSUN Bedroom. • We tried Adam [31] and RMSProp early on in our experimentation process and chose the former. We left the hyperparameters to their standard values. We set the learning rate to  without any sweeping, and we lowered it to  for the  images, which seemed unstable to train with the larger learning rate. • We set the batch size to 128 for CIFAR10 and 64 for larger images. We did not sweep over these values. • We used EMA on model parameters with a decay factor of 0.9999. We did not sweep over this value.","[ 0.00535202 -0.03936768  0.01811218 ...  0.02026367  0.00709915
 -0.01382446]",7,,text,,,,,8,25,792.0,14,612.0,src/resources/pdf/diffusion.pdf,
"[132, 72, 505, 120]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",0,,text,,,,,9,25,792.0,15,612.0,src/resources/pdf/diffusion.pdf,
"[107, 129, 506, 227]","Final experiments were trained once and evaluated throughout training for sample quality. Sample quality scores and log likelihood are reported on the minimum FID value over the course of training. On CIFAR10, we calculated Inception and FID scores on 50000 samples using the original code from the OpenAI [51] and TTUR [21] repositories, respectively. On LSUN, we calculated FID scores on 50000 samples using code from the StyleGAN2 [30] repository. CIFAR10 and CelebA-HQ were loaded as provided by TensorFlow Datasets (https://www.tensorflow.org/datasets), and LSUN was prepared using code from StyleGAN. Dataset splits (or lack thereof) are standard from the papers that introduced their usage in a generative modeling context. All details can be found in the source code release.","[ 0.00060129 -0.00255775 -0.02174377 ... -0.00175762 -0.02149963
 -0.02832031]",1,,text,,,,,9,25,792.0,15,612.0,src/resources/pdf/diffusion.pdf,
"[108, 242, 265, 256]",C Discussion on related work,"[ 0.01337433  0.00569916 -0.04696655 ...  0.00566864  0.00813293
 -0.01173401]",2,,text,,,,,9,25,792.0,15,612.0,src/resources/pdf/diffusion.pdf,
"[108, 267, 505, 300]","Our model architecture, forward process definition, and prior differ from NCSN [55, 56] in subtle but important ways that improve sample quality, and, notably, we directly train our sampler as a latent variable model rather than adding it after training post-hoc. In greater detail:","[ 0.01054382  0.00820923 -0.00202751 ... -0.01747131 -0.02206421
 -0.00769424]",3,,text,,,,,9,25,792.0,15,612.0,src/resources/pdf/diffusion.pdf,
"[129, 308, 505, 495]","1. We use a U-Net with self-attention; NCSN uses a RefineNet with dilated convolutions. We condition all layers on  by adding in the Transformer sinusoidal position embedding, rather than only in normalization layers (NCSNv1) or only at the output (v2). 2. Diffusion models scale down the data with each forward process step (by a  factor) so that variance does not grow when adding noise, thus providing consistently scaled inputs to the neural net reverse process. NCSN omits this scaling factor. 3. Unlike NCSN, our forward process destroys signal  , ensur- ing a close match between the prior and aggregate posterior of . Also unlike NCSN, our  are very small, which ensures that the forward process is reversible by a Markov chain with conditional Gaussians. Both of these factors prevent distribution shift when sampling. 4. Our Langevin-like sampler has coefficients (learning rate, noise scale, etc.) derived rig- orously from  in the forward process. Thus, our training procedure directly trains our sampler to match the data distribution after  steps: it trains the sampler as a latent variable model using variational inference. In contrast, NCSN’s sampler coefficients are set by hand post-hoc, and their training procedure is not guaranteed to directly optimize a quality metric of their sampler.","[ 0.00611496  0.02656555 -0.01096344 ...  0.03253174 -0.00943756
  0.01823425]",4,,text,,,,,9,25,792.0,15,612.0,src/resources/pdf/diffusion.pdf,
"[107, 510, 171, 523]",D Samples,"[-0.03036499  0.04962158 -0.03820801 ...  0.01488495 -0.02452087
 -0.03497314]",5,,text,,,,,9,25,792.0,15,612.0,src/resources/pdf/diffusion.pdf,
"[108, 534, 504, 556]","Additional samples Figure 11, 13, 16, 17, 18, and 19 show uncurated samples from the diffusion models trained on CelebA-HQ, CIFAR10 and LSUN datasets.","[-0.05105591 -0.01675415 -0.05264282 ...  0.00788116 -0.0063591
 -0.03341675]",6,,text,,,,,9,25,792.0,15,612.0,src/resources/pdf/diffusion.pdf,
"[106, 568, 505, 677]","Latent structure and reverse process stochasticity During sampling, both the prior   and Langevin dynamics are stochastic. To understand the significance of the second source of noise, we sampled multiple images conditioned on the same intermediate latent for the CelebA  dataset. Figure 7 shows multiple draws from the reverse process  that share the latent  for . To accomplish this, we run a single reverse chain from an initial draw from the prior. At the intermediate timesteps, the chain is split to sample multiple images. When the chain is split after the prior draw at , the samples differ significantly. However, when the chain is split after more steps, samples share high-level attributes like gender, hair color, eyewear, saturation, pose and facial expression. This indicates that intermediate latents like  encode these attributes, despite their imperceptibility.","[-0.03948975 -0.00312233  0.01577759 ...  0.00156689 -0.03421021
 -0.01271057]",7,,text,,,,,9,25,792.0,15,612.0,src/resources/pdf/diffusion.pdf,
"[107, 689, 505, 722]","Coarse-to-fine interpolation Figure 9 shows interpolations between a pair of source CelebA  images as we vary the number of diffusion steps prior to latent space interpolation. Increasing the number of diffusion steps destroys more structure in the source images, which the model completes during the reverse process. This allows us to interpolate at both fine granularities and coarse granularities. In the limiting case of 0 diffusion steps, the interpolation mixes source images in pixel space. On the other hand, after 1000 diffusion steps, source information is lost and interpolations are novel samples.","[-0.04464722  0.01147461 -0.00306511 ... -0.01625061 -0.02961731
 -0.01107788]",8,,text,,,,,9,25,792.0,15,612.0,src/resources/pdf/diffusion.pdf,
"[107, 127, 504, 383]",,"[-0.03240967 -0.00984192 -0.00701523 ... -0.03192139 -0.0227356
 -0.04434204]",0,,image,,Figure 9: Coarse-to-fine interpolations that vary the number of diffusion steps prior to latent mixing.,,src/resources/pdf/diffusion/auto/images/4924fbfc005ebfcc6018688fa9d35c970ee62c83d8cbba9391021f1a3da9e9ba.jpg,3,25,792.0,16,612.0,src/resources/pdf/diffusion.pdf,
"[131, 572, 481, 700]",,"[ 0.00920868  0.02575684  0.01409912 ... -0.00043058 -0.01531982
 -0.05819702]",1,,image,,Figure 10: Unconditional CIFAR10 progressive sampling quality over time,,src/resources/pdf/diffusion/auto/images/0e8a23c1295358924846a9160f8c86917d569e9bfdf3b3ed07ca498cfd6b6d8a.jpg,3,25,792.0,16,612.0,src/resources/pdf/diffusion.pdf,
"[106, 72, 505, 117]",,"[ 0.01444244  0.0531311   0.0014658  ... -0.02137756  0.04043579
 -0.00286293]",2,,text,,,,,3,25,792.0,16,612.0,src/resources/pdf/diffusion.pdf,
"[106, 186, 504, 585]",,"[-0.06433105 -0.0328064   0.00454712 ... -0.01901245 -0.03317261
 -0.04251099]",0,,image,,Figure 11: CelebA-HQ  generated samples,,src/resources/pdf/diffusion/auto/images/c22a4cd56551631bec877fed3854f299b27070433f06c09567244c5601392fa7.jpg,1,25,792.0,17,612.0,src/resources/pdf/diffusion.pdf,
"[106, 159, 506, 586]",,"[-0.05368042 -0.04446411 -0.04428101 ... -0.0115509   0.01476288
 -0.01593018]",0,,image,,(b) Inception feature space nearest neighbors,,src/resources/pdf/diffusion/auto/images/a8f085b03f73f18d17f6cf96ebcdaad8a8dc5e416a9009a2f4de7ad3f2c70b2d.jpg,1,25,792.0,18,612.0,src/resources/pdf/diffusion.pdf,
"[106, 186, 505, 586]",,"[-0.06015015  0.01644897 -0.00681305 ...  0.00559998 -0.02603149
 -0.0279541 ]",0,,image,,Figure 13: Unconditional CIFAR10 generated samples,,src/resources/pdf/diffusion/auto/images/07e985672f84b100bfdb0ef04e2a3741338e9e849b26a04aa8132c9b3ac91169.jpg,1,25,792.0,19,612.0,src/resources/pdf/diffusion.pdf,
"[106, 186, 505, 586]",,"[-0.06188965 -0.00814819  0.01966858 ...  0.0094223  -0.03268433
 -0.02189636]",0,,image,,Figure 14: Unconditional CIFAR10 progressive generation,,src/resources/pdf/diffusion/auto/images/25b7d67c464bb17e99d421b5a18a4c223967a5e4ff916ce56cdbd021ffa64819.jpg,1,25,792.0,20,612.0,src/resources/pdf/diffusion.pdf,
"[126, 181, 486, 577]",,"[-0.04501343 -0.02070618 -0.03341675 ...  0.00635529  0.02778625
  0.00063562]",0,,image,,"Figure 15: Unconditional CIFAR10 nearest neighbors. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.",,src/resources/pdf/diffusion/auto/images/9b7786abb344011b190fffcc0341fb96d3419cc56be3fd22eb3c0a95d08281c6.jpg,1,25,792.0,21,612.0,src/resources/pdf/diffusion.pdf,
"[107, 126, 504, 649]",,"[-0.07531738 -0.03967285  0.01461792 ...  0.01976013 -0.01099396
 -0.00138474]",0,,image,,Figure 16: LSUN Church generated samples.,,src/resources/pdf/diffusion/auto/images/29748b61e8d0b2e6d882725f34855daffdd365f27abbfae2ee0e3c0f2aab6de1.jpg,1,25,792.0,22,612.0,src/resources/pdf/diffusion.pdf,
"[107, 129, 504, 648]",,"[-0.04418945 -0.06658936 -0.01939392 ...  0.01834106 -0.01628113
 -0.02998352]",0,,image,,"Figure 17: LSUN Bedroom generated samples, large model.",,src/resources/pdf/diffusion/auto/images/ea63278321dbe05c281d9cb14cdbda053917604e27893d1d039ac7c3ce7b0ef4.jpg,1,25,792.0,23,612.0,src/resources/pdf/diffusion.pdf,
"[107, 128, 504, 649]",,"[-0.04873657 -0.07037354 -0.00201225 ...  0.00881195 -0.0091095
 -0.04391479]",0,,image,,"Figure 18: LSUN Bedroom generated samples, small model.",,src/resources/pdf/diffusion/auto/images/9fbc4498842d1390856b50dfe992551b06ae41fea180a20b410f2e3f3ea4c80c.jpg,1,25,792.0,24,612.0,src/resources/pdf/diffusion.pdf,
"[107, 126, 504, 648]",,"[-0.05297852 -0.04910278 -0.00127602 ...  0.01052856 -0.01374817
  0.01177216]",0,,image,,Figure 19: LSUN Cat generated samples. FID,,src/resources/pdf/diffusion/auto/images/a8028026923ef76a4350ef4da23370fdc25fb73dcffeb8b57533612333691e00.jpg,1,25,792.0,25,612.0,src/resources/pdf/diffusion.pdf,
